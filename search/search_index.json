{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AnkaFlow - Run Data Pipelines Anywhere","text":"<p>From REST APIs to SQL, from Local Python to Browser Execution</p>"},{"location":"#what-is-ankaflow","title":"What is AnkaFlow?","text":"<p>AnkaFlow is a YAML-driven, SQL-powered data pipeline framework designed for both local Python and in-browser (Pyodide) execution. It enables seamless extraction, transformation, and joining of data across REST APIs, cloud storage, and databases, all without writing custom Python code.</p> <p>Write your pipeline once, run it anywhere.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Dual Execution Modes: Run pipelines locally or fully in-browser with Pyodide.</li> <li>DuckDB In-Memory SQL Engine: Fast, scalable analytics with SQL.</li> <li>Dynamic Templating: Full support for variable injection, header and query templating.</li> <li>REST &amp; GraphQL Support: Production-ready REST and GraphQL connectors with error handling and polling.</li> <li>Joins Across REST and SQL: Native support for combining API responses with SQL datasets.</li> <li>Python Transform Stage: Execute custom Python logic inline within your pipeline.</li> <li>DeltaLake, BigQuery, S3, MSSQL, Oracle: Seamlessly connect to enterprise data sources.</li> <li>YAML Anchors &amp; References: DRY pipeline definitions with reusable components.</li> <li>Async Ready and Future-Proof: Designed for scalable and parallel execution.</li> </ul>"},{"location":"#example-use-cases","title":"Example Use Cases","text":"<ul> <li>Data Enrichment Pipelines: Join Shopify orders (REST), DeltaLake financials, BigQuery users, and real-time weather data.</li> <li>Browser-Based Data Apps: Execute pipelines directly in the browser, preserving data privacy.</li> <li>ML Feature Engineering: Combine SQL and Python transform steps for complex feature generation.</li> <li>SaaS Product Integrations: Embed pipelines into dashboards, trigger REST calls, and process responses.</li> <li>Ad-hoc Analysis and Reporting: Dynamic pipelines for analysts and consultants, no Python code required.</li> </ul>"},{"location":"#why-choose-ankaflow","title":"Why Choose AnkaFlow?","text":""},{"location":"#ankaflow-vs-other-pipeline-frameworks","title":"AnkaFlow vs Other Pipeline Frameworks","text":"Feature AnkaFlow Airflow Dagster Bonobo Luigi DLT In-Browser Execution (Pyodide) \u2705 Yes \u274c \u274c \u274c \u274c \u274c Dynamic Templating \u2705 Yes \ud83d\udd36 Partial (Jinja) \ud83d\udd36 Partial \ud83d\udd36 Basic \ud83d\udd36 Basic \ud83d\udd36 via Python REST + SQL Join \u2705 Native \ud83d\udd36 Plugin-based \ud83d\udd36 Possible \ud83d\udd36 Indirect \ud83d\udd36 Indirect \ud83d\udd36 via SQLMesh Python Transform \u2705 Yes \ud83d\udd36 Plugin-based \u2705 Yes \u2705 Yes \u2705 Yes \u2705 Yes Pure SQL Transforms \u2705 Native (DuckDB SQL) \ud83d\udd36 via Plugins \ud83d\udd36 Limited SQL Nodes \u274c \u274c \u2705 via Destinations **BigQuery / Delta / S3 ** \u2705 Native Support \ud83d\udd36 via Plugins \u2705 Integrations \ud83d\udd36 User-managed \ud83d\udd36 User-managed \u2705 Native Recursive YAML / Anchors \u2705 Yes \ud83d\udd36 via Jinja \ud83d\udd36 Partial \u274c \u274c \u274c External System Requirements \u2705 None \u2014 self-contained \u274c Requires DB &amp; Scheduler \ud83d\udd36 Optional Metadata DB \u2705 Lightweight \u2014 no deps \u274c Requires Scheduler \u2705 No built-in orchestration Configuration-First Design \u2705 Declarative \u2014 code optional \ud83d\udd36 Code-first with DAGs \ud83d\udd36 Hybrid \u2014 config &amp; code \ud83d\udd36 Mostly code-based \ud83d\udd36 Code-centric \u274c Code is required (Python)"},{"location":"#roadmap-highlights","title":"Roadmap Highlights","text":"<ul> <li>\u2705 Fully battle-tested REST and GraphQL support</li> <li>\u2705 Python transform stage shipped</li> <li>\u2705 IndexedDB caching</li> <li>\ud83d\udfe0 Built-in data lineage tracking</li> <li>\ud83d\udfe0 Parallel execution in local runtime</li> </ul>"},{"location":"#get-started-today","title":"Get Started Today","text":"<p>Write once, run anywhere \u2014 from your laptop to the browser. AnkaFlow pipelines adapt to your workflow, combining flexibility, power, and portability.</p> <p>LIVE DEMO</p>"},{"location":"#documentation","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>Technical Summary</li> <li>Pipeline Specification</li> <li>AnkaFlow API</li> </ul>"},{"location":"#built-with","title":"Built with","text":"<p>DuckDB | YAML | Jinja</p>"},{"location":"browser-readme/","title":"\ud83e\uddea AnkaFlow in the Browser (Pyodide)","text":"<p>This guide shows how to run AnkaFlow pipelines fully in-browser, using Pyodide (Python in WebAssembly). No server, no install \u2014 pipelines run client-side using the same YAML-based definitions.</p>"},{"location":"browser-readme/#demo-options","title":"\ud83d\ude80 Demo Options","text":"Method Link \ud83e\uddea JupyterLite Notebook Launch Demo Notebook \ud83c\udf10 HTML SPA Demo Try YAML Upload Demo"},{"location":"browser-readme/#how-it-works","title":"\ud83d\udce6 How It Works","text":"<ul> <li>AnkaFlow is compatible with Pyodide (via <code>micropip</code>)</li> <li>Remote files (e.g., S3, GCS) are fetched using <code>pyodide.http.pyfetch</code> or custom implementation (e.g. <code>axios</code>)</li> <li>The SQL engine is DuckDB (running in WASM)</li> <li>Everything is local to your browser</li> </ul>"},{"location":"browser-readme/#example-jupyterlite-or-pyodide","title":"\ud83e\uddf0 Example (JupyterLite or Pyodide)","text":"<pre><code>import micropip\nawait micropip.install(\"ankaflow\")\n\nfrom ankaflow import Flow\nyaml = '''\n- name: Load\n  kind: source\n  connection:\n    kind: Parquet\n    locator: data.parquet\n\n- name: View\n  kind: transform\n  query: select * from Load\n'''\n\nFlow().run(yaml)\n</code></pre>"},{"location":"browser-readme/#notes","title":"\ud83e\udde0 Notes","text":"<ul> <li>Only packages available in the Pyodide index can be used</li> <li>Some connectors (e.g., BigQuery, ClickHouse) are not available in the browser</li> <li>All pipelines must avoid server-only dependencies when targeting Pyodide</li> </ul>"},{"location":"browser-readme/#for-developers","title":"\ud83d\udee0 For Developers","text":"<p>If you're embedding AnkaFlow in a browser app:</p> <ol> <li>Load Pyodide:</li> </ol> <pre><code>&lt;script src=\"https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js\"&gt;&lt;/script&gt;\n</code></pre> <ol> <li>Bootstrap and run:</li> </ol> <pre><code>const pyodide = await loadPyodide();\nawait pyodide.loadPackage([\"micropip\"]);\nawait pyodide.runPythonAsync(`\n    import micropip\n    await micropip.install(\"ankaflow\")\n    from ankaflow import Flow\n    Flow().run(...)\n`);\n</code></pre>"},{"location":"browser-readme/#browser-specific-modules","title":"\ud83d\udcc1 Browser-Specific Modules","text":"<p>The following modules help with Pyodide execution:</p> <ul> <li><code>connections/rest/browser.py</code>: Fetches data via <code>pyfetch</code></li> <li><code>LocalFileSystem</code>: Writes to Pyodide's <code>/tmp</code></li> <li><code>ObjectDownloader</code>: Manages cloud-to-browser downloads</li> </ul>"},{"location":"browser-readme/#works-great-in","title":"\u2705 Works Great In","text":"<ul> <li>[x] Chrome</li> <li>[x] Firefox</li> <li>[x] JupyterLite</li> <li>[x] VS Code WebView</li> <li>[x] GitHub Pages</li> </ul>"},{"location":"custom-connection/","title":"CustomConnection User Guide","text":"<p>Welcome to the CustomConnection user guide. This document will walk you through:</p> <ul> <li>What <code>CustomConnection</code> is and when to use it</li> <li>The available configuration fields</li> <li>How to reference a <code>CustomConnection</code> in your pipeline YAML</li> <li>Example usage and best practices</li> </ul>"},{"location":"custom-connection/#1-introduction","title":"1. Introduction","text":"<p><code>CustomConnection</code> allows you to plug in your own connection logic into the AnkaFlow pipeline. Your custom connection class must implement the base <code>Connection</code> interface (or derive from it), and provide the following methods:</p> <ul> <li><code>tap()</code>: Extract data from the source</li> <li><code>sink()</code>: Write data to the destination</li> <li><code>sql()</code>: Execute or generate SQL statements</li> <li><code>show_schema()</code>: Display or return the table/schema structure</li> </ul> <p>Even if your class does not need all of these for its logic, it must expose them (they can be no-ops).</p>"},{"location":"custom-connection/#2-customconnection-fields","title":"2. CustomConnection Fields","text":"<p>Below is a description of each field in the <code>CustomConnection</code> model.</p> Field Type Description <code>kind</code> <code>Literal[\"CustomConnection\"]</code> Must always be set to <code>CustomConnection</code> to select this provider. <code>module</code> <code>str</code> Python module path containing your custom connection class (e.g. <code>myapp.connectors.database</code>). <code>classname</code> <code>str</code> The name of the class to load from the specified module. <code>params</code> <code>dict</code>(default <code>{}</code>) Arbitrary parameters passed into your connection\u2019s constructor. <code>config</code> <code>ConnectionConfiguration \\| None</code> (Optional) Pre-built configuration object injected by BaseConnection super-class. <code>fields</code> <code>List[Field] \\| None</code> (Optional) Schema fields, auto-populated or used by the base implementation if needed. <code>locator</code> <code>str \\| None</code> (Optional) Name or identifier used by <code>Connection.locate()</code> for dynamic discovery."},{"location":"custom-connection/#3-defining-a-customconnection-in-your-pipeline","title":"3. Defining a CustomConnection in Your Pipeline","text":"<p>In your pipeline YAML, under a <code>sink</code> or <code>tap</code> stage, you reference a custom connection like this:</p> <pre><code>- name: MySqlWriter\n  kind: sink\n  connection:\n    kind: CustomConnection\n    module: myapp.connectors.database\n    classname: MySQL\n    params:\n      port: 5555\n</code></pre> <p>Explanation:</p> <ol> <li><code>name:</code> Logical name for the pipeline stage (<code>MySqlWriter</code>).</li> <li><code>kind: sink</code> Specifies that this stage writes data (a \"sink\" stage).</li> <li><code>connection:</code> Block configures how to connect to the target system:<ul> <li><code>kind</code>: Must be <code>CustomConnection</code>.</li> <li><code>module</code>: Python import path where your class lives.</li> <li><code>classname</code>: Actual class name inside that module.</li> <li><code>params</code>: Any keyword arguments your class expects (e.g. table name, credentials key).</li> </ul> </li> <li><code>show: 1</code> Enables schema introspection after connecting (Debug mode).</li> </ol>"},{"location":"custom-connection/#4-implementing-your-custom-class","title":"4. Implementing Your Custom Class","text":"<p>In <code>myapp/connectors/database.py</code>:</p> <pre><code>from ankaflow.connection import Connection\n\nclass MySQL(Connection):\n    def init(self):\n        # This method is provided by base class as convenience to\n        # avoid mucking with super().__init__()\n        # It is called in the end of base class __init__ hence\n        # self.config and other attributes are already populated.\n        pass\n\n    async def tap(self, query: str|None = None):\n        # Extract data from external storage and cache to pipeline\n        df = get_dataframe_from_mysql(query)\n        await self.c.register(\"tmp_{self.name}\", df)\n        await self.c.sql(f'CREATE TABLE \"{self.name}\" AS SELECT * FROM data')\n        # You can use CREATE OR REPLACE or CREATE IF NOT EXISTS\n        # CREATE OR REPLACE Will overwrite existing data (does not enforce stage name uniqueness)\n        # CREATE IF NOT EXISTS will append data if table already exists.\n        await self.c.unregister(\"tmp_{self.name}\")\n\n    async def sink(self, from_name: str):\n        # Write data to external storage\n        # from_name is always previous stage name (presumably tap or transform)\n        relation = await self.c.sql(f'SELECT * FROM \"{from_name}\"')\n        df = await relation.df() # of fetchall()\n        send_df_to_mysql(df)\n\n    async def sql(self, query: str):\n        # Execute SQL logic on target\n        execute_sql_on_mysql(query)\n\n    async def show_schema(self):\n        # Return table schema, e.g. list of Field objects\n        return []\n</code></pre> <p>Note: All methods must be async.</p> <p>Note: Your class must derive from the base <code>Connection</code> to inherit common logic and have access to <code>self.conn.cfg</code>, <code>self.conn.locator()</code>, etc.</p>"},{"location":"custom-connection/#5-best-practices-troubleshooting","title":"5. Best Practices &amp; Troubleshooting","text":"<ul> <li>Validation: Always validate your <code>params</code> against what your class expects. Mismatches will not raise errors at <code>Stages.load()</code> time but at runtime.</li> <li>Module Path: Ensure your project\u2019s root is on <code>PYTHONPATH</code> so that <code>import module</code> succeeds at runtime.</li> <li>Debugging: Use <code>show: 1</code> in your YAML to print the output of tap().</li> <li>Error Handling: Wrap your I/O in try/except blocks and surface meaningful messages; Ankalow will propagate exceptions upstream.</li> </ul> <p>Happy piping!</p>"},{"location":"getting-started/","title":"Getting Started with AnkaFlow","text":"<p>AnkaFlow is a powerful data pipeline framework that allows you to define, manage, and execute complex workflows involving data sourcing, transformation, and storage. This guide walks you through the basic steps to get started with AnkaFlow, including setting up the pipeline, installing dependencies, and running your first pipeline.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you get started, make sure you have the following installed on your machine:</p> <ol> <li>Python 3.12 or later</li> <li><code>pip</code> (Python's package installer)</li> </ol>"},{"location":"getting-started/#a-install-ankaflow","title":"A. Install AnkaFlow","text":"<p>To begin using AnkaFlow, install the required packages by running:</p> <pre><code>pip install AnkaFlow\n</code></pre> <p>In server environment you may want to include additional connectors</p> <pre><code>pip install AnkaFlow[server]\n</code></pre> <p>This will install AnkaFlow and its necessary dependencies, including <code>duckdb</code>, <code>yaml</code>, and <code>pandas</code>.</p>"},{"location":"getting-started/#b-environment-variables","title":"B. Environment variables","text":"<p>There are few environment variables that can be used to confgure AnkaFlow behaviour (usage is optional):</p> <ul> <li>Load extensions from local disk (disable network load)</li> </ul> <pre><code>export DUCKDB_EXTENSION_DIR=/my/dir\n</code></pre> <ul> <li>Disable local filesystem access</li> </ul> <pre><code>export DUCKDB_DISABLE_LOCALFS=1\n</code></pre> <ul> <li>Lock DuckDB configuration and prevent changing it in pipeline</li> </ul> <pre><code>export DUCKDB_LOCK_CONFIG=1\n</code></pre> <p>In Pyodide environment these settings are not available.</p>"},{"location":"getting-started/#1-imports","title":"1. Imports","text":"<p>To begin using AnkaFlow, you'll first need to import the necessary libraries:</p> <pre><code>from ankaflow import Flow, FlowContext, ConnectionConfiguration\nimport logging\n</code></pre>"},{"location":"getting-started/#2-create-a-logger","title":"2. Create a Logger","text":"<p>It's always good practice to create a logger for your pipeline to track its execution. The logger will help capture events, errors, and other relevant information during the pipeline's execution.</p> <pre><code># Create a logger to log pipeline events\nmy_logger = logging.getLogger('mylogger')\nmy_logger.setLevel(logging.DEBUG)\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel(logging.DEBUG)\nmy_logger.addHandler(console_handler)\n</code></pre>"},{"location":"getting-started/#3-load-yaml-from-file","title":"3. Load YAML from File","text":"<p>The configuration for your pipeline is often defined in a YAML file. This file will contain the stages of the pipeline, their configurations, and other necessary details.</p> <pre><code>from ankaflow import Stages\n\npipeline_config = Stages.load('pipeline_config.yaml')\n</code></pre> <p>This assumes you have a <code>pipeline_config.yaml</code> file in the same directory. Here's an example of what the YAML file might look like:</p> <pre><code>stages:\n  - kind: tap\n    name: source_data\n    connection:\n      kind: BigQuery\n      project_id: \"my_project\"\n      dataset: \"my_dataset\"\n  - kind: transform\n    name: process_data\n    query: \"SELECT * FROM source_data WHERE condition\"\n  - kind: sink\n    name: output_data\n    connection:\n      kind: File\n      file_path: \"output/data.csv\"\n</code></pre>"},{"location":"getting-started/#4-create-connectionconfiguration-and-flowcontext","title":"4. Create <code>ConnectionConfiguration</code> and <code>FlowContext</code>","text":"<p>The <code>ConnectionConfiguration</code> and <code>FlowContext</code> are essential for configuring the pipeline and providing context for variables and connections.</p> <pre><code># Create a ConnectionConfiguration with necessary details\nconn_config = ConnectionConfiguration(\n    kind='BigQuery', \n    project_id='my_project',\n    dataset='my_dataset'\n)\n\n# Create a FlowContext, passing any relevant configuration parameters\nflow_context = FlowContext(\n    context_variable='some_value',  # Example variable\n    connection=conn_config\n)\n</code></pre>"},{"location":"getting-started/#6-create-the-pipeline-flow","title":"6. Create the Pipeline (<code>Flow</code>)","text":"<p>Now that you have everything set up (logger, configuration, stages), you can create the <code>Flow</code> object and start running your pipeline.</p> <pre><code># Create the Flow instance with the pipeline stages, context, and configuration\nflow = Flow(\n    defs=stages, \n    context=flow_context, \n    default_connection=conn_config, \n    logger=my_logger\n)\n\n# Run the pipeline\nflow.run()\n</code></pre>"},{"location":"getting-started/#wrapping-up","title":"Wrapping Up","text":"<ul> <li>AnkaFlow allows you to create flexible and modular data pipelines by defining stages for sourcing, transforming, and storing data.</li> <li>You can manage your pipeline configuration using YAML files and easily set up connections to different data sources (BigQuery, databases, files).</li> <li>The logger provides essential insight into your pipeline\u2019s execution, helping you debug and track issues.</li> </ul> <p>Now that you've set up your first pipeline, you can customize it further by adding more stages, adjusting configurations, and experimenting with different data sources.</p>"},{"location":"getting-started/#additional-resources","title":"Additional Resources","text":"<ul> <li>AnkaFlow Documentation: For more advanced usage and API references, check out the AnkaFlow documentation.</li> <li>Community Support: If you encounter any issues, join the AnkaFlow community for support and troubleshooting.</li> </ul>"},{"location":"intro/","title":"AnkaFlow","text":"<p>Write Once, Run Anywhere \u2014 SQL-powered, YAML-defined data pipelines that work in Python or the Browser.</p> <p>Run data workflows that combine Parquet, REST APIs, and SQL transforms with no infrastructure, using DuckDB under the hood. Supports local Python, Pyodide (browser), and Chrome/Firefox Extensions.</p>"},{"location":"intro/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>\u2705 Run pipelines in Python or in-browser (Pyodide/WebAssembly)</li> <li>\u2705 Supports Parquet, REST, GraphQL, DeltaLake, BigQuery, ClickHouse, and more</li> <li>\u2705 No-code pipelines via YAML: source \u2192 transform \u2192 sink</li> <li>\u2705 Modular: browser/server backends, cloud optional</li> <li>\u2705 Built on DuckDB with SQL and optional Python transforms</li> <li>\u2705 Compatible with Pyodide, Chrome Extensions, and GitHub Pages</li> </ul>"},{"location":"intro/#quickstart","title":"\ud83d\udee0 Quickstart","text":""},{"location":"intro/#python-server","title":"\ud83d\udc0d Python (server)","text":"<pre><code>uv pip install -e .[server]\nankaflow pipeline.yaml\n</code></pre>"},{"location":"intro/#browser-pyodide","title":"\ud83c\udf10 Browser (Pyodide)","text":"<ol> <li>Open demo</li> <li>Upload a <code>pipeline.yaml</code> file</li> <li>View SQL output live in your browser</li> </ol>"},{"location":"intro/#dev-environment","title":"\ud83e\uddea Dev Environment","text":"<pre><code>uv pip install -e .[dev,server]\n</code></pre>"},{"location":"intro/#installation","title":"\ud83d\udce6 Installation","text":"<p>Install only the core (minimal setup with Parquet, JSON S3 support for Pyodide or remote embedding; does not include databases):</p> <pre><code>uv pip install ankaflow\n</code></pre> <p>Install with full server capabilities (BigQuery, ClickHouse, Delta write):</p> <pre><code>uv pip install ankaflow[server]\n</code></pre> <p>Install for development:</p> <pre><code>uv pip install -e .[dev,server]\n</code></pre>"},{"location":"intro/#project-layout","title":"\ud83d\udcc2 Project Layout","text":"<ul> <li><code>ankaflow/</code> \u2013 Python engine and core logic</li> <li><code>docs/</code> \u2013 Markdown and API docs (generated via pdoc)</li> </ul> <p>Upload a <code>pipeline.yaml</code> like:</p> <pre><code>- name: Load\n  kind: source\n  connection: { kind: Parquet, locator: data$*.parquet }\n\n- name: View\n  kind: transform\n  query: select * from Load\n</code></pre>"},{"location":"intro/#roadmap","title":"\ud83e\udde0 Roadmap","text":"<ul> <li>[x] Pyodide-compatible core</li> <li>[x] Remote file support (S3, GCS)</li> <li>[x] Chrome/Firefox extension support</li> <li>[ ] IndexedDB caching</li> <li>[ ] REST-to-SQL join templating</li> <li>[ ] JupyterLite integration</li> </ul>"},{"location":"intro/#contributing","title":"\ud83d\ude4c Contributing","text":"<p>PRs welcome.</p>"},{"location":"intro/#license","title":"\ud83d\udcc4 License","text":"<p>MIT License</p>"},{"location":"llm-readme/","title":"Leveraging LLMs in Data Pipelines with AnkaFlow","text":"<p>AnkaFlow allows you to integrate powerful language models like OpenAI's GPT into a SQL-based ETL workflow \u2014 not for generating text, but for generating structured SQL queries dynamically from user questions. This turns natural language into executable logic as part of a data pipeline. Here's how to structure such a pipeline, what the components do, and how to generalize it to your own use case.</p>"},{"location":"llm-readme/#what-are-we-building","title":"\ud83e\udde0 What Are We Building?","text":"<p>A pipeline that: 1. Loads structured data (e.g., Parquet files). 2. Extracts schema information. 3. Sends metadata + user question to an LLM (e.g., GPT). 4. Receives a SQL query as output. 5. Executes the query and optionally returns results.</p>"},{"location":"llm-readme/#pipeline-structure-overview","title":"\ud83e\uddf1 Pipeline Structure Overview","text":"<p>AnkaFlow pipelines are defined in YAML and executed via an in-memory DuckDB engine. A pipeline consists of:</p> <ul> <li><code>source</code> steps to load structured data or call external services (like an LLM) and generate queries</li> <li><code>transform</code> steps to apply logic</li> <li><code>sink</code> steps to persist results or make them available for later reuse</li> </ul>"},{"location":"llm-readme/#llm-integration-step-by-step","title":"\ud83e\udde9 LLM Integration: Step-by-Step","text":""},{"location":"llm-readme/#1-load-your-data","title":"1. Load Your Data","text":"<p>Use <code>source</code> steps to ingest data from Parquet, SQL, or REST APIs:</p> <pre><code>- name: LoadSales\n  kind: source\n  connection:\n    kind: Parquet\n    locator: data/sales_data*.parquet\n</code></pre>"},{"location":"llm-readme/#2-discover-schema-dynamically","title":"2. Discover Schema Dynamically","text":"<p>Use a SQL query to inspect the tables you've loaded:</p> <pre><code>- name: Describe\n  kind: self\n  query: &gt;\n    SELECT table_name, column_name, data_type\n    FROM information_schema.columns\n    WHERE table_schema = 'main'\n</code></pre>"},{"location":"llm-readme/#3-save-schema-as-a-variable","title":"3. Save Schema as a Variable","text":"<p>Store the schema result for reuse in the LLM call:</p> <pre><code>- name: SetSchema\n  kind: sink\n  connection:\n    kind: Variable\n    locator: DiscoveredSchema\n</code></pre>"},{"location":"llm-readme/#4-prompt-template-header-block","title":"4. Prompt Template (Header Block)","text":"<p>Define a reusable LLM prompt using Jinja2 templating:</p> <pre><code>- name: Header\n  kind: header\n  prompt: &amp;Prompt |\n    You are a SQL query generator.\n    Schema: &lt;&lt; schema_json &gt;&gt;\n    &lt;% if relations_json %&gt;Relations: &lt;&lt; relations_json &gt;&gt;&lt;% endif %&gt;\n    Question: &lt;&lt; user_prompt &gt;&gt;\n</code></pre>"},{"location":"llm-readme/#5-generate-sql-via-llm","title":"5. Generate SQL via LLM","text":"<p>Use a <code>tap</code> step with an LLM backend:</p> <pre><code>- name: GenerateSQL\n  kind: tap\n  connection:\n    kind: SQLGen\n    config:\n      kind: openai\n    variables:\n    schema_json: &lt;&lt;API.look('DiscoveredSchema', variables) | tojson&gt;&gt;\n    user_prompt: &lt;&lt;API.look('UserPrompt', variables)&gt;&gt;\n  query: *Prompt\n</code></pre> <p>Here, the LLM is used to translate a natural language prompt into executable SQL using the live schema as context. The generated query will be injected to the pipeline generating new output.</p> <p>The output can be examined by setting <code>show: 1</code>.</p>"},{"location":"llm-readme/#design-considerations","title":"\ud83d\udd04 Design Considerations","text":"<ul> <li>Idempotent Execution: Each step is deterministic. Even LLM steps are cacheable and inspectable.</li> <li>Traceability: LLM input and output are transparent; prompt + schema + response are all visible.</li> <li>Declarative Logic: SQL output is injected back into the pipeline like any other transform.</li> <li>Custom Connectors: Can integrate other backends besides OpenAI (e.g., local models or proxies).</li> </ul>"},{"location":"llm-readme/#use-cases","title":"\ud83d\udd27 Use Cases","text":"<ul> <li>Ad Hoc Analytics: Analysts can phrase questions in plain language.</li> <li>Self-Service Dashboards: Business users ask \"How many orders were delayed last month?\" and get results.</li> <li>Semantic SQL Layer: Front-end apps dynamically generate SQL from user queries without manual query writing.</li> </ul>"},{"location":"llm-readme/#generalization-tips","title":"\ud83d\udee0 Generalization Tips","text":"<ul> <li>Template your LLM prompts. Store them under <code>- kind: header</code> and inject metadata via <code>variables</code>.</li> <li>Always pass schema as JSON \u2014 ideally normalized via an inspection query.</li> <li>Store LLM results in <code>Variable</code> sinks to reuse, log, or audit them.</li> <li>Use environment-specific <code>backend</code> config for OpenAI, local, or proxy usage.</li> </ul>"},{"location":"llm-readme/#running-the-pipeline","title":"\ud83d\ude80 Running the Pipeline","text":"<p>Once defined, pipelines can be run locally:</p> <pre><code>ankaflow pipeline.yaml\n</code></pre> <p>Or embedded into browser apps via Pyodide.</p>"},{"location":"llm-readme/#summary","title":"\ud83d\udccc Summary","text":"Feature Benefit Schema-aware prompts More accurate SQL generation Variable injection Reuse outputs across steps YAML-first architecture Version-controlled, auditable flow OpenAI backend (or other) Plug in any LLM provider"},{"location":"motherduck/","title":"Motherduck Integration Guide","text":"<p>This guide explains how to integrate and manipulate Motherduck data in your AnkaFlow pipelines. Since Motherduck requires native network access, all pipeline runs using Motherduck must execute on a server or environment with internet connectivity.</p>"},{"location":"motherduck/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>AnkaFlow installed and configured.</li> <li>Access to a Motherduck account and dataset.</li> <li>Valid Motherduck API token set as an environment variable.</li> </ul> <pre><code>export motherduck_token=your_real_token_here\n</code></pre>"},{"location":"motherduck/#2-pipeline-yaml-example","title":"2. Pipeline YAML Example","text":"<p>Below is a minimal pipeline demonstrating how to attach a Motherduck endpoint, query system tables, and retrieve sample data.</p> <pre><code>- name: First step\n  kind: self\n  query: select 42 as meaning\n\n- name: Attach Motherduck\n  kind: self\n  query: &gt;\n    attach 'md:';\n    select table_catalog, table_schema, table_name\n      from INFORMATION_SCHEMA.tables;\n  show: 5\n\n- name: Show sample data\n  kind: self\n  query: &gt;\n    select *\n      from sample_data.nyc.taxi\n      limit 10;\n  show: 5\n</code></pre>"},{"location":"motherduck/#explanation","title":"Explanation","text":"<ol> <li>API Token: AnkaFlow reads <code>motherduck_token</code> from the environment.  </li> <li><code>attach 'md:'</code>: Establishes the connection to Motherduck using the DSN prefix <code>md:</code>.  </li> <li>System Catalog Query: Lists available tables in <code>INFORMATION_SCHEMA.tables</code>.  </li> <li>Data Query: Pulls the first 10 rows from <code>sample_data.nyc.taxi</code>.</li> </ol>"},{"location":"motherduck/#3-running-the-pipeline","title":"3. Running the Pipeline","text":"<p>Execute your pipeline on a server with network access:</p> <pre><code>ankaflow pipeline.yaml\n</code></pre> <p>You should see output sections like:</p> <pre><code>---- Attach Motherduck\n  table_catalog | table_schema | table_name\n  \u2026 (5 rows shown) \u2026\n\n---- Show sample data\n  VendorID | tpep_pickup_datetime | \u2026 \n  \u2026 (10 rows shown) \u2026\n</code></pre>"},{"location":"motherduck/#4-writing-data-to-motherduck","title":"4. Writing Data to Motherduck","text":"<p>To create or overwrite tables in Motherduck, use SQL DDL statements:</p> <pre><code>- name: Create reporting table\n  kind: self\n  query: &gt;\n    attach 'md:';\n    create or replace table analytics.daily_counts as\n      select date_trunc('day', tpep_pickup_datetime) as day,\n             count(*) as rides\n        from sample_data.nyc.taxi\n       group by day;\n</code></pre> <p>Tip: Confirm write permissions and dataset names before running DDL.</p>"},{"location":"motherduck/#5-advanced-data-manipulations","title":"5. Advanced Data Manipulations","text":""},{"location":"motherduck/#51-inserting-additional-rows","title":"5.1. Inserting Additional Rows","text":"<pre><code>- name: Append special events\n  kind: self\n  query: &gt;\n    attach 'md:';\n    insert into analytics.daily_counts(day, rides)\n      values('2023-12-31', 1234);\n</code></pre>"},{"location":"motherduck/#52-updating-records","title":"5.2. Updating Records","text":"<pre><code>- name: Correct ride count\n  kind: self\n  query: &gt;\n    attach 'md:';\n    update analytics.daily_counts\n       set rides = rides + 10\n     where day = '2023-12-31';\n</code></pre>"},{"location":"motherduck/#53-cleaning-up","title":"5.3. Cleaning Up","text":"<pre><code>- name: Drop temp table\n  kind: self\n  query: &gt;\n    attach 'md:';\n    drop table if exists analytics.daily_counts_temp;\n</code></pre>"},{"location":"motherduck/#6-best-practices","title":"6. Best Practices","text":"<ul> <li>Chunk large queries to avoid timeouts.  </li> <li>Use <code>show: N</code> to preview results without overwhelming the logs.  </li> <li>Modularize repeated <code>attach 'md:'</code> calls by creating a top-level stage for connection.  </li> <li>Secure your token: do not commit <code>motherduck_token</code> to source control.</li> </ul>"},{"location":"motherduck/#7-troubleshooting","title":"7. Troubleshooting","text":"<ul> <li>Authentication errors \u2192 Verify <code>motherduck_token</code> is correct and exported.  </li> <li>Network failures \u2192 Ensure outbound connectivity to Motherduck endpoints.  </li> <li>Permission denied \u2192 Check your account\u2019s dataset ACLs.</li> </ul> <p>Happy querying with Motherduck in AnkaFlow!</p>"},{"location":"overview/","title":"Technical Introduction Document: Setting Up and Configuring a Data Pipeline in AnkaFlow","text":"<p>This document provides a step-by-step guide for setting up and configuring a data pipeline using AnkaFlow. The pipeline framework facilitates seamless data transformations and efficient management of stages such as data sourcing (tap), transformations, and sink operations.</p>"},{"location":"overview/#1-overview-of-the-pipeline-system","title":"1. Overview of the Pipeline System","text":"<p>A pipeline in AnkaFlow defines the flow of data through various stages. Each stage corresponds to a specific data operation such as:</p> <ul> <li>Tap Stage: Sources data from a remote or local system.</li> <li>Transform Stage: Processes or transforms data, typically using SQL queries or other computational logic.</li> <li>Sink Stage: Stores the processed data into a target system (e.g., a database, file system, or cloud storage).</li> </ul> <p>These stages are executed sequentially, with each stage building on the data produced by the previous one.</p>"},{"location":"overview/#2-pipeline-components","title":"2. Pipeline Components","text":"<ul> <li>Flow: The primary object that controls the execution of the pipeline. It defines the order of stages, manages connections, and handles error flow control.</li> </ul> <p>Example Initialization:</p> <p><code>python   flow = Flow(       defs=stages,        context=flowuct_context,        default_connection=conn_config,        logger=my_logger   )</code></p> <ul> <li><code>defs</code>: A list of stages (e.g., <code>TapStage</code>, <code>TransformStage</code>, <code>SinkStage</code>).</li> <li><code>context</code>: The dynamic context used in query templates.</li> <li><code>default_connection</code>: Connection configuration passed to the underlying systems.</li> <li> <p><code>logger</code>: Optional logger for logging pipeline activities.</p> </li> <li> <p>Datablock: Represents an executable piece of the pipeline. Each <code>Datablock</code> corresponds to a specific stage and includes the logic to execute that stage, e.g., reading data, transforming it, or storing it.</p> </li> </ul> <p>Example:</p> <p><code>python   datablock = Datablock(       conn=db_connection,       defs=datablock_def,       context=flow_context,       default_connection=conn_config   )</code></p> <ul> <li>Stage Handler: Each stage (tap, transform, sink) is associated with a handler that defines how the stage should be executed. The handler interacts with the data and performs the necessary operations.</li> </ul>"},{"location":"overview/#3-configuration-and-setup","title":"3. Configuration and Setup","text":"<p>To configure a pipeline, you must define the following:</p> <ul> <li>Stages: Define the sequence of operations your pipeline will execute. Each stage must specify its type (<code>tap</code>, <code>transform</code>, <code>sink</code>) and the corresponding logic (e.g., queries, data manipulations).</li> <li>Connection Configurations: Each stage typically connects to a data source or target system. Connection details, such as credentials, endpoint URLs, and database configurations, are passed into the stages.</li> </ul> <p>Example:</p> <p><code>yaml   stages:     - kind: tap       name: source_data       connection:         kind: BigQuery         project_id: \"my_project\"         dataset: \"my_dataset\"     - kind: transform       name: process_data       query: \"SELECT * FROM source_data WHERE condition\"     - kind: sink       name: output_data       connection:         kind: File         file_path: \"output/data.csv\"</code></p> <ul> <li>Variables: If your pipeline stages reference dynamic values, such as dates or keys, you can define these variables within the <code>context</code> to be injected into your queries at runtime.</li> </ul>"},{"location":"overview/#4-executing-the-pipeline","title":"4. Executing the Pipeline","text":"<p>Once the pipeline is defined, you can execute it using the <code>Flow</code> class. This will initiate the stages in sequence and handle the transformation of data across all stages.</p> <ul> <li>Run Pipeline: Execute the pipeline and get the final data.</li> </ul> <p><code>python   flow.run()</code></p> <ul> <li>Access Output Data: Once the pipeline runs, you can access the data produced by the final stage via the <code>df()</code> method.</li> </ul> <p><code>python   result_df = flow.df()</code></p>"},{"location":"overview/#5-error-handling-and-flow-control","title":"5. Error Handling and Flow Control","text":"<p>AnkaFlow provides robust error handling mechanisms, ensuring that errors are managed appropriately during pipeline execution. The <code>FlowControl</code> configuration allows you to define how errors should be handled (e.g., fail or warn).</p> <p>Example of flow control:</p> <pre><code>flow_control = FlowControl(on_error=\"fail\")\n</code></pre>"},{"location":"overview/#6-show-schema-for-stages","title":"6. Show Schema for Stages","text":"<p>Each stage may expose a schema that defines the structure of the data. This can be useful to inspect and verify the data format before moving to subsequent stages.</p> <pre><code>schema = flow.show_schema()\n</code></pre>"},{"location":"overview/#7-testing-and-debugging-pipelines","title":"7. Testing and Debugging Pipelines","text":"<p>For reliable pipeline execution, it is important to test and debug each stage. AnkaFlow includes utilities for mock testing, simulating remote connections, and verifying the correctness of SQL queries.</p> <ul> <li>Unit Tests: Each stage can be tested in isolation. For example, the <code>TapStageHandler</code> can be tested to ensure that it retrieves the correct data from the source.</li> </ul>"},{"location":"overview/#8-conclusion","title":"8. Conclusion","text":"<p>With these components and configurations, AnkaFlow allows you to define, execute, and manage data pipelines flexibly. It integrates various data sources and sinks, applies transformations, and enables efficient error handling, all while keeping the pipeline definitions clean and reusable.</p> <p>Next Steps:</p> <ul> <li>Customize your pipeline based on the specific sources, transformations, and sinks relevant to your use case.</li> <li>Explore advanced features like parallel pipeline execution and nested sub-pipelines for more complex workflows.</li> </ul> <p>This document serves as an introduction to configuring a basic pipeline setup in AnkaFlow. For more advanced configurations and features, refer to the detailed documentation on stages, handlers, and connections.</p>"},{"location":"api/","title":"Index","text":"<p>API areas</p>"},{"location":"api/ankaflow/","title":"AnkaFlow","text":""},{"location":"api/ankaflow/#ankaflow.__all__","title":"__all__","text":"<pre><code>__all__ = [\n    \"Flow\",\n    \"AsyncFlow\",\n    \"FlowControl\",\n    \"Stages\",\n    \"ConnectionConfiguration\",\n    \"Variables\",\n    \"FlowContext\",\n    \"FlowRunError\",\n    \"FlowError\",\n    \"S3Config\",\n    \"GSConfig\",\n    \"ClickhouseConfig\",\n    \"BigQueryConfig\",\n    \"BucketConfig\",\n]\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow","title":"AsyncFlow","text":"<pre><code>AsyncFlow(\n    defs: Stages,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Variables = None,\n    logger: Logger = None,\n    conn: DDB = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n)\n</code></pre> <p>Controls the flow of data based on the pipeline definition.</p> <p>The stages are run in the defined order and can either fetch data from a source (tap), transform via SQL, or store to a sink.</p> <p>Parameters:</p> Name Type Description Default <code>defs</code> <code>Stages</code> <p>List of stages to be processed.</p> required <code>context</code> <code>FlowContext</code> <p>Dynamic context for query templates.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Configuration passed to underlying storage or database.</p> required <code>variables</code> <code>Variables</code> <p>Variables for tap or prepare. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>Logger for show() requests. Defaults to None.</p> <code>None</code> <code>conn</code> <code>DDB</code> <p>Existing DuckDB connection. If not set, a new connection is created.</p> <code>None</code> <code>flow_control</code> <code>FlowControl</code> <p>Flow control configuration. Defaults to FlowControl().</p> <code>None</code> <code>log_context</code> <code>str</code> <p>Log context passed to each stage. Defaults to None.</p> <code>None</code> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(\n    self,\n    defs: m.Stages,\n    context: m.FlowContext,\n    default_connection: m.ConnectionConfiguration,\n    variables: m.Variables = None,\n    logger: logging.Logger = None,\n    conn: \"DDB\" = None,\n    flow_control: \"FlowControl\" = None,\n    log_context: str = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        defs (m.Stages): List of stages to be processed.\n        context (m.FlowContext): Dynamic context for query templates.\n        default_connection (m.ConnectionConfiguration): Configuration passed\n            to underlying storage or database.\n        variables (m.Variables, optional): Variables for tap or prepare.\n            Defaults to {}.\n        logger (logging.Logger, optional): Logger for show() requests.\n            Defaults to None.\n        conn (DDB, optional): Existing DuckDB connection. If not set, a new\n            connection is created.\n        flow_control (FlowControl, optional): Flow control configuration.\n            Defaults to FlowControl().\n        log_context (str, optional): Log context passed to each stage.\n            Defaults to None.\n    \"\"\"\n    self.defs = defs\n    self.vars = m.Variables() if variables is None else variables\n    self.lastname: t.Optional[str] = None\n    self.ctx: m.FlowContext = context\n    self.conn_opts: m.ConnectionConfiguration = default_connection\n    self.logger = logger\n    self.flow_control = flow_control or FlowControl()\n    self.log_context = log_context\n    self.idb: t.Optional[\"DDB\"] = conn\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.log","title":"log","text":"<pre><code>log: Logger\n</code></pre> <p>Returns the logger.</p>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.logs","title":"logs","text":"<pre><code>logs: List[str]\n</code></pre> <p>Returns the log buffer.</p>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.connect","title":"connect","text":"<pre><code>connect() -&gt; None\n</code></pre> <p>Connects to the underlying database if not already connected.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def connect(self) -&gt; None:\n    \"\"\"\n    Connects to the underlying database if not already connected.\n    \"\"\"\n    if self.idb:\n        return\n    else:\n        self.idb = await DDB.connect(self.conn_opts)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.df","title":"df","text":"<pre><code>df() -&gt; DataFrame\n</code></pre> <p>Returns a dataframe from the last stage.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from the last stage.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def df(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns a dataframe from the last stage.\n\n    Returns:\n        pd.DataFrame: Data from the last stage.\n    \"\"\"\n    if not self.lastname:\n        return pd.DataFrame()\n    coro = await self.idb.sql(f'SELECT * FROM \"{self.lastname}\"')\n    return await coro.df()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.pull_df","title":"pull_df","text":"<pre><code>pull_df() -&gt; DataFrame\n</code></pre> <p>Convenience method to pull the final dataframe.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from the last stage.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def pull_df(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Convenience method to pull the final dataframe.\n\n    Returns:\n        pd.DataFrame: Data from the last stage.\n    \"\"\"\n    await self.run()\n    return await self.df()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.run","title":"run","text":"<pre><code>run() -&gt; AsyncFlow\n</code></pre> <p>Runs the pipeline stages.</p> <p>Returns:</p> Name Type Description <code>AsyncFlow</code> <code>AsyncFlow</code> <p>Self instance after running the pipeline.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def run(self) -&gt; \"AsyncFlow\":\n    \"\"\"\n    Runs the pipeline stages.\n\n    Returns:\n        AsyncFlow: Self instance after running the pipeline.\n    \"\"\"\n    if not self.idb:\n        await self.connect()\n    start = arrow.get()\n\n    vars_info = {k: v for k, v in self.vars.items()}\n    self.log.info(f\"Pipeline called with variables:\\n{vars_info}\")\n    for step in self.defs.steps():\n        try:\n            if step.kind == \"header\":\n                continue\n            if step.log_level:\n                self.log.setLevel(step.log_level.value)\n            log_ctx = f\"[{self.log_context}]\" if self.log_context else \"\"\n            self.log.info(f\"{step.kind} '{step.name}' {log_ctx}\")\n            stage = StageBlock(\n                self.idb,  # type: ignore[assignment]\n                step,\n                self.ctx,\n                self.conn_opts,\n                variables=self.vars,\n                logger=self.log,\n                prevous_stage=self.lastname,  # type: ignore[assignment]\n                log_context=self.log_context,\n            )\n            self.lastname = await stage.do() or self.lastname\n        except Exception as ex:\n            msg = dd(f\"{ex.__class__.__name__} in {step.name}: {ex}\")\n            if step.on_error == FlowControl.ON_ERROR_FAIL:\n                if self.flow_control.on_error == FlowControl.ON_ERROR_FAIL:\n                    self.log.error(\n                        f\"Pipeline failed at '{step.name}':\\n{dd(msg)}\"\n                    )\n                    end = arrow.get()\n                    self.log.info(f\"Run duration: {end - start}\")\n                    raise e.FlowRunError(\n                        f\"Failed at '{step.name}':\\n{dd(msg)}\"\n                    )\n            else:\n                self.log.warning(f\"Failed '{step.name}':\\n{dd(msg)}\")\n        if step.throttle and step.throttle &gt; 0:\n            self.log.info(f\"Flow throttling {step.throttle}s\")\n            await sleep(step.throttle)\n    end = arrow.get()\n    self.log.setLevel(logging.INFO)\n    self.log.info(f\"Run duration: {end - start}\")\n    return self\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.AsyncFlow.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; List[SchemaItem]\n</code></pre> <p>Returns schemas for all supported stages.</p> <p>Returns:</p> Type Description <code>List[SchemaItem]</code> <p>t.List[m.models.SchemaItem]: List of schema items.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def show_schema(self) -&gt; t.List[m.core.SchemaItem]:\n    \"\"\"\n    Returns schemas for all supported stages.\n\n    Returns:\n        t.List[m.models.SchemaItem]: List of schema items.\n    \"\"\"\n    items: t.List[m.core.SchemaItem] = []\n    for step in self.defs:\n        try:\n            db = StageBlock(\n                self.idb,\n                step,\n                self.ctx,\n                self.conn_opts,\n                variables=self.vars,\n                logger=self.log,\n                prevous_stage=self.lastname,\n            )\n            st = await db.show_schema()\n            if st is None:\n                continue\n            elif isinstance(st, list):\n                items.extend(st)\n            elif isinstance(st, m.core.SchemaItem):\n                items.append(st)\n            else:\n                self.log.warning(\"Unexpected schema item type\")\n        except NotImplementedError:\n            continue\n        except Exception:\n            raise\n    return items\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.BigQueryConfig","title":"BigQueryConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for accessing BigQuery datasets.</p>"},{"location":"api/ankaflow/#ankaflow.BigQueryConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to service account JSON credentials for BigQuery access.</p>"},{"location":"api/ankaflow/#ankaflow.BigQueryConfig.dataset","title":"dataset","text":"<pre><code>dataset: Optional[str] = None\n</code></pre> <p>BigQuery dataset name.</p>"},{"location":"api/ankaflow/#ankaflow.BigQueryConfig.project","title":"project","text":"<pre><code>project: Optional[str] = None\n</code></pre> <p>GCP project ID containing the BigQuery dataset.</p>"},{"location":"api/ankaflow/#ankaflow.BigQueryConfig.region","title":"region","text":"<pre><code>region: Optional[str] = None\n</code></pre> <p>BigQuery region (e.g., \"us-central1\").</p>"},{"location":"api/ankaflow/#ankaflow.BucketConfig","title":"BucketConfig","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.BucketConfig.bucket","title":"bucket","text":"<pre><code>bucket: Optional[str] = None\n</code></pre> <p>Bucket name eg. <code>s3://my-bucket</code> or local absolute path eg. <code>/data/path</code></p>"},{"location":"api/ankaflow/#ankaflow.BucketConfig.data_prefix","title":"data_prefix","text":"<pre><code>data_prefix: Optional[str] = None\n</code></pre> <p>Prefix for data files: <code>s3://my-bucket/&lt;prefix&gt;</code> or <code>/data/path/&lt;prefix&gt;</code></p>"},{"location":"api/ankaflow/#ankaflow.BucketConfig.locator_wildcard","title":"locator_wildcard","text":"<pre><code>locator_wildcard: Optional[Tuple] = None\n</code></pre> <p>Regular expression and wildcard to modify <code>locator</code>. Useful in cases when you sink data to <code>data-YYYY.parquet</code> but want to read <code>data-*.parquet</code>. Provide tuple with patten as first element and replacement as second one. Example:</p> <pre><code>('-\\d{4}-', '*')\n</code></pre> <p>This will create wildcard for <code>data-YYYY-base.parquet</code> as <code>data*base.parquet</code>.</p> Wildcard Description <code>*</code> matches any number of any characters (including none) <code>**</code> matches any number of subdirectories (including none) <code>[abc]</code> matches one character given in the bracket <code>[a-z]</code> matches one character from the range given in the bracket <p>Wildcard is automatically applied in tap and show_schema operations.</p>"},{"location":"api/ankaflow/#ankaflow.BucketConfig.region","title":"region","text":"<pre><code>region: str | None = None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.ClickhouseConfig","title":"ClickhouseConfig","text":"<p>               Bases: <code>DatabaseConfig</code></p> <p>Configuration for ClickHouse database, extending generic SQL config.</p>"},{"location":"api/ankaflow/#ankaflow.ClickhouseConfig.blocksize","title":"blocksize","text":"<pre><code>blocksize: int = 50000\n</code></pre> <p>Number of rows to process per block for batch operations.</p>"},{"location":"api/ankaflow/#ankaflow.ConnectionConfiguration","title":"ConnectionConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level container for all connection configurations in a pipeline.</p> <p>Includes default configuration blocks for supported sources and sinks like: - Local file systems - S3 and GCS buckets - BigQuery datasets - ClickHouse databases</p> <p>These can be customized per pipeline or extended with new sources.</p> Example <p>To support MySQL, you can define a new model like this:</p> <pre><code>class MySQLConfig(DatabaseConfig):\n    pass  # You can add MySQL-specific fields here if needed\n\nclass ExtendedConnectionConfiguration(ConnectionConfiguration):\n    mysql: MySQLConfig = PydanticField(default_factory=MySQLConfig)\n</code></pre> <p>This will allow you to specify MySQL connection settings in your YAML:</p> <pre><code>connection:\n  kind: MySQL\n  config:\n    host: localhost\n    port: 3306\n    database: sales\n    username: admin\n    password: secret\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.ConnectionConfiguration.bigquery","title":"bigquery","text":"<pre><code>bigquery: BigQueryConfig = Field(\n    default_factory=BigQueryConfig\n)\n</code></pre> <p>BigQuery connection configuration.</p>"},{"location":"api/ankaflow/#ankaflow.ConnectionConfiguration.clickhouse","title":"clickhouse","text":"<pre><code>clickhouse: ClickhouseConfig = Field(\n    default_factory=ClickhouseConfig\n)\n</code></pre> <p>Language model configuration.</p>"},{"location":"api/ankaflow/#ankaflow.ConnectionConfiguration.gs","title":"gs","text":"<pre><code>gs: GSConfig = Field(default_factory=GSConfig)\n</code></pre> <p>Google Cloud Storage configuration.</p>"},{"location":"api/ankaflow/#ankaflow.ConnectionConfiguration.llm","title":"llm","text":"<pre><code>llm: LLMConfig = Field(default_factory=LLMConfig)\n</code></pre> <p>Language model configuration.</p>"},{"location":"api/ankaflow/#ankaflow.ConnectionConfiguration.local","title":"local","text":"<pre><code>local: BucketConfig = Field(default_factory=BucketConfig)\n</code></pre> <p>Local file system configuration.</p>"},{"location":"api/ankaflow/#ankaflow.ConnectionConfiguration.s3","title":"s3","text":"<pre><code>s3: S3Config = Field(default_factory=S3Config)\n</code></pre> <p>S3 cloud storage configuration.</p>"},{"location":"api/ankaflow/#ankaflow.Flow","title":"Flow","text":"<pre><code>Flow(\n    defs: Stages,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Variables = None,\n    logger: Logger = None,\n    conn: DuckDBPyConnection = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n)\n</code></pre> <p>Flow controls the flow of data based on the pipeline definition. The stages are run in the defined order and can either fetch data from source (tap), transform via SQL statement, or store to sink.</p> <p>This is the sync version and should be used in most cases.</p> <p>Parameters:</p> Name Type Description Default <code>defs</code> <code>Stages</code> <p>List of stages to be processed.</p> required <code>context</code> <code>FlowContext</code> <p>dynamic context that can be used in query templates.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Configuration passed to underlying storage or database (s3 bucket, database connection string &amp;c).</p> required <code>variables</code> <code>dict</code> <p>Any variables that can be referenced by a tap or prepare. Ony data structures that can be passed to pl.from_dicts() are supported in taps. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>If set then show() requests are logged. Defaults to None.</p> <code>None</code> <code>conn</code> <code>DuckDBPyConnection</code> <p>Existing DuckDB connection. If not set new connection will be created.</p> <code>None</code> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(\n    self,\n    defs: m.Stages,\n    context: m.FlowContext,\n    default_connection: m.ConnectionConfiguration,\n    variables: m.Variables = None,\n    logger: logging.Logger = None,\n    conn: duckdb.DuckDBPyConnection = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        defs (Stages): List of stages to be processed.\n        context (FlowContext): dynamic context that can be used\n            in query templates.\n        default_connection (ConnectionConfiguration): Configuration\n            passed to underlying storage or database\n            (s3 bucket, database connection string &amp;c).\n        variables (dict, optional): Any variables that can be referenced\n            by a tap or prepare. Ony data structures that can be passed\n            to pl.from_dicts() are supported in taps. Defaults to {}.\n        logger (Logger, optional): If set then show() requests are logged.\n            Defaults to None.\n        conn (DuckDBPyConnection): Existing DuckDB connection. If not set\n            new connection will be created.\n    \"\"\"\n    self.flow = AsyncFlow(\n        defs,\n        context,\n        default_connection,\n        variables=m.Variables() if variables is None else variables,\n        logger=logger,\n        conn=conn,\n        flow_control=flow_control or FlowControl(),\n        log_context=log_context,\n    )\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.Flow.df","title":"df","text":"<pre><code>df()\n</code></pre> <p>Returns dataframe from the last stage</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def df(self):\n    \"\"\"Returns dataframe from the last stage\"\"\"\n    return asyncio_run(self.flow.df())\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.Flow.pull_df","title":"pull_df","text":"<pre><code>pull_df() -&gt; DataFrame\n</code></pre> <p>Convenience method to pull final dataframe</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: data from last stage</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def pull_df(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Convenience method to pull final dataframe\n\n    Returns:\n        pl.DataFrame: data from last stage\n    \"\"\"\n    asyncio_run(self.flow.run())\n    return asyncio_run(self.flow.df())\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.Flow.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; List[SchemaItem]\n</code></pre> <p>Returns schemas of all supported stages</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def show_schema(self) -&gt; t.List[m.core.SchemaItem]:\n    \"\"\"\n    Returns schemas of all supported stages\n    \"\"\"\n    return asyncio_run(self.flow.show_schema())\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.FlowContext","title":"FlowContext","text":"<p>               Bases: <code>ImmutableMap</code></p> <p>Context dictionary can be used to supply arbitrary data to pipeline that can be referenced in templates much much like variables with the difference that they cannot be modified at runtime.</p> <p>Context can be initiated via dictionary:</p> <p><code>context = FlowContext(**{'foo':'Bar'}])</code></p> <p>Items can be reference using both bracket and dot notation:</p> <p><code>context.foo == context['foo'] == 'Bar'</code></p>"},{"location":"api/ankaflow/#ankaflow.FlowControl","title":"FlowControl","text":"<pre><code>FlowControl(on_error: str = 'fail')\n</code></pre> Source code in <code>ankaflow/core/policy.py</code> <pre><code>def __init__(self, on_error: str = \"fail\"):\n    self.on_error = on_error\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.FlowError","title":"FlowError","text":"<p>               Bases: <code>Exception</code></p>"},{"location":"api/ankaflow/#ankaflow.FlowRunError","title":"FlowRunError","text":"<p>               Bases: <code>FlowError</code></p>"},{"location":"api/ankaflow/#ankaflow.GSConfig","title":"GSConfig","text":"<p>               Bases: <code>BucketConfig</code></p> <p>Google Cloud Storage (GCS) configuration.</p> <ul> <li>HMAC credentials allow reading/writing Parquet, JSON, and CSV.</li> <li>Delta table writes require a full service account credential file.</li> </ul>"},{"location":"api/ankaflow/#ankaflow.GSConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to GCP service account credential file (JSON). Required for certain write operations (e.g., Delta tables).</p>"},{"location":"api/ankaflow/#ankaflow.GSConfig.hmac_key","title":"hmac_key","text":"<pre><code>hmac_key: Optional[str] = None\n</code></pre> <p>GCS HMAC key ID for authenticated access.</p>"},{"location":"api/ankaflow/#ankaflow.GSConfig.hmac_secret","title":"hmac_secret","text":"<pre><code>hmac_secret: Optional[str] = None\n</code></pre> <p>GCS HMAC secret for authenticated access.</p>"},{"location":"api/ankaflow/#ankaflow.S3Config","title":"S3Config","text":"<p>               Bases: <code>BucketConfig</code></p> <p>S3-specific bucket configuration including optional authentication.</p>"},{"location":"api/ankaflow/#ankaflow.S3Config.access_key_id","title":"access_key_id","text":"<pre><code>access_key_id: Optional[str] = None\n</code></pre> <p>AWS access key ID for private S3 access.</p>"},{"location":"api/ankaflow/#ankaflow.S3Config.secret_access_key","title":"secret_access_key","text":"<pre><code>secret_access_key: Optional[str] = None\n</code></pre> <p>AWS secret access key for private S3 access.</p>"},{"location":"api/ankaflow/#ankaflow.Stages","title":"Stages","text":"<p>               Bases: <code>RootModel[List[Stage]]</code></p> <p>A sequence of processing stages in a data pipeline.</p> <p>Each <code>Datablock</code> in <code>root</code> represents one discrete step or transformation in an end-to-end business workflow (e.g., tap, transform, sink, validate).</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>List[Datablock]</code> <p>Ordered list of pipeline stages to execute.</p>"},{"location":"api/ankaflow/#ankaflow.Stages.enumerate_steps","title":"enumerate_steps","text":"<pre><code>enumerate_steps() -&gt; Iterator[tuple[int, Stage]]\n</code></pre> <p>Yield each stage along with its 0-based position.</p> <p>Use this when you need both the stage and its index for logging, metrics, or conditional branching.</p> <p>Returns:</p> Type Description <code>Iterator[tuple[int, Stage]]</code> <p>Iterator[Tuple[int, Datablock]]: Pairs of (index, stage).</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>def enumerate_steps(self) -&gt; t.Iterator[tuple[int, Stage]]:\n    \"\"\"Yield each stage along with its 0-based position.\n\n    Use this when you need both the stage and its index for logging,\n    metrics, or conditional branching.\n\n    Returns:\n        Iterator[Tuple[int, Datablock]]: Pairs of (index, stage).\n    \"\"\"\n    return enumerate(self.root)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.Stages.load","title":"load","text":"<pre><code>load(source: Union[str, Path, IO[str], Loadable]) -&gt; Stages\n</code></pre> <p>Load a pipeline from YAML (path, YAML-string, file-like or Loadable).</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | Path | IO[str] | Loadable</code> <ul> <li>Path to a .yaml file</li> <li>Raw YAML content</li> <li>File-like object returning YAML</li> <li>Any object with a <code>.load()</code> method returning Python data</li> </ul> required <p>Returns:</p> Name Type Description <code>Stages</code> <code>Stages</code> <p>a validated <code>Stages</code> instance.</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    source: t.Union[str, Path, t.IO[str], Loadable],\n) -&gt; \"Stages\":\n    \"\"\"Load a pipeline from YAML (path, YAML-string, file-like or Loadable).\n\n    Args:\n        source (str | Path | IO[str] | Loadable):\n            - Path to a .yaml file\n            - Raw YAML content\n            - File-like object returning YAML\n            - Any object with a `.load()` method returning Python data\n\n    Returns:\n        Stages: a validated `Stages` instance.\n    \"\"\"\n    # 1) If it\u2019s a loader-object, pull Python data directly\n    if isinstance(source, Loadable):\n        data = source.load()\n\n    else:\n        # 2) Read text for YAML parsing:\n        if hasattr(source, \"read\"):\n            text = t.cast(t.IO[str], source).read()\n        else:\n            text = str(source)\n\n        # 3) First, try parsing as raw YAML\n        try:\n            data = yaml.safe_load(text)\n        except yaml.YAMLError:\n            data = None\n\n        # 4) Only if that parse returned a `str` we treat it as a filename\n        if isinstance(data, str):\n            try:\n                text = Path(data).read_text()\n                data = yaml.safe_load(text)\n            except (OSError, yaml.YAMLError) as e:\n                raise ValueError(\n                    f\"Could not interpret {data!r} as YAML or file path\"\n                ) from e\n\n    # 5) Validate final shape\n    if not isinstance(data, list):\n        raise ValueError(\n            f\"Expected a list of pipeline stages, got {type(data).__name__}\"\n        )\n\n    # 5) Finally, validate into our model\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.Stages.steps","title":"steps","text":"<pre><code>steps() -&gt; Iterator[Stage]\n</code></pre> <p>Yield each stage in execution order.</p> <p>Returns:</p> Type Description <code>Iterator[Stage]</code> <p>Iterator[Datablock]: An iterator over the stages,</p> <code>Iterator[Stage]</code> <p>from first to last.</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>def steps(self) -&gt; t.Iterator[Stage]:\n    \"\"\"Yield each stage in execution order.\n\n    Returns:\n        Iterator[Datablock]: An iterator over the stages,\n        from first to last.\n    \"\"\"\n    return iter(self.root)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.Variables","title":"Variables","text":"<p>               Bases: <code>dict</code></p> <p>Variables is a <code>dict</code>-based collection storing arbitrary data under keys. Variable can be populated via:</p> <ul> <li>Dictionary passed to pipeline:   <code>Flow(defs, context, variables={'foo': 'bar'})</code></li> <li>Sink operation:<pre><code>- kind: sink\n\n    name: my_sink\n\n    connection:\n\n        kind: Variable\n\n        locator: variable_name\n</code></pre> </li> </ul> <p>This step will place a list of records from preceding step in   variables as <code>{'variable_name': t.List[dict]}</code></p> <p>In most cases sainking to variable is not needed as all preceding stages in a pipeline can be referenced via <code>name</code>. Sinking to variable is useful when you need to share data with subpipeline.</p> <p>Special variable is <code>loop_control</code> which is populated dynamically from previous step and current stage type is pipeline. In case previous step generates multiple records then the new pipeline is called for each record, and control variable holds a <code>dict</code> representing the current record.</p>"},{"location":"api/ankaflow/#ankaflow.api","title":"api","text":""},{"location":"api/ankaflow/#ankaflow.api.API","title":"API","text":""},{"location":"api/ankaflow/#ankaflow.api.API.dt","title":"dt","text":"<pre><code>dt(\n    datelike: Union[\n        str, int, float, date, datetime, Arrow, None\n    ] = None,\n    tz: str | None = None,\n    format: str | None = None,\n    default: str | None = None,\n)\n</code></pre> <p>Attempts to convert input object into Arrow.</p> <p>Parameters:</p> Name Type Description Default <code>tz</code> <code>str</code> <p>IANA timezone string If . Defaults to None.</p> <code>None</code> <code>format</code> <code>str</code> <p>If set use this format to parse input. Defaults to None.</p> <code>None</code> <code>default</code> <code>str</code> <p>Use default timestamp if first attempt fails. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Arrow</code> <p>Arrow object</p> Source code in <code>ankaflow/api.py</code> <pre><code>@staticmethod\ndef dt(\n    datelike: t.Union[\n        str, \"int\", float, date, datetime, arrow.Arrow, None\n    ] = None,\n    tz: str | None = None,\n    format: str | None = None,\n    default: str | None = None,\n):\n    \"\"\"\n    Attempts to convert input object into Arrow.\n\n    Args:\n        datelike (t.Union[str, int, float, date, datetime, arrow.Arrow],\n            optional): Object to be evaluated to Arrow. Defaults to None.\n        tz (str, optional): IANA timezone string If . Defaults to None.\n        format (str, optional): If set use this format to parse input.\n            Defaults to None.\n        default (str, optional): Use default timestamp if first attempt\n            fails. Defaults to None.\n\n    Returns:\n        Arrow: Arrow object\n    \"\"\"\n    # Handle various return types:\n    # nan - when table exists but not data\n    # nanosecond timestamp in bigquery\n    if pd.isna(datelike):  # type: ignore\n        datelike = default\n    if isinstance(datelike, (int)):\n        if datelike &gt; 9999999999:\n            return arrow.get(pd.to_datetime(datelike))\n    try:\n        if datelike is None:\n            a = arrow.get()\n        elif format:\n            a = arrow.get(datelike, format)  # type: ignore\n        else:\n            a = arrow.get(datelike)\n    except Exception:\n        if default:\n            a = arrow.get(default)\n        else:\n            raise\n    if tz:\n        return a.replace(tzinfo=tz)\n    return a\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.api.API.error","title":"error","text":"<pre><code>error(expression: Any, message: str) -&gt; Any\n</code></pre> <p>Raises an exception if input expression evaluates to True.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>Any</code> <p>Expression to be evaluetd</p> required <code>message</code> <code>str</code> <p>Message to include in exception</p> required <p>Raises:</p> Type Description <code>UserGeneratedError</code> <p>description</p> <p>Returns:</p> Type Description <code>Any</code> <p>t.Any: Expression</p> Source code in <code>ankaflow/api.py</code> <pre><code>@staticmethod\ndef error(expression: t.Any, message: str) -&gt; t.Any:\n    \"\"\"\n    Raises an exception if input expression evaluates\n    to True.\n\n    Args:\n        expression (t.Any): Expression to be evaluetd\n        message (str): Message to include in exception\n\n    Raises:\n        UserGeneratedError: _description_\n\n    Returns:\n        t.Any: Expression\n    \"\"\"\n    if expression:\n        raise UserGeneratedError(message)\n    return expression\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.api.API.look","title":"look","text":"<pre><code>look(\n    lookup: str, data: Any, default: Any = None\n) -&gt; Union[str, None]\n</code></pre> <p>Extracts value from given structure, or default value if requested value not found.</p> <p>Parameters:</p> Name Type Description Default <code>lookup</code> <code>str</code> <p>Lookup query</p> required <code>data</code> <code>Any</code> <p>Iterable object</p> required <code>default</code> <code>Any</code> <p>Default value to return. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: description</p> Source code in <code>ankaflow/api.py</code> <pre><code>@staticmethod\ndef look(\n    lookup: str, data: t.Any, default: t.Any = None\n) -&gt; t.Union[str, None]:\n    \"\"\"\n    Extracts value from given structure, or default value\n    if requested value not found.\n\n    Args:\n        lookup (str): Lookup query\n        data (t.Any): Iterable object\n        default (t.Any, optional): Default value to return.\n            Defaults to None.\n\n    Returns:\n        t.Union[str, None]: _description_\n    \"\"\"\n    found = jmespath.search(lookup, data)\n    if found is None:\n        return default\n    return found\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.api.API.peek","title":"peek","text":"<pre><code>peek(value: Any) -&gt; str\n</code></pre> <p>Returns information about value type: module.Class Useful for debugging data obtained from remote sources.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Any value</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>description</p> Source code in <code>ankaflow/api.py</code> <pre><code>@staticmethod\ndef peek(value: t.Any) -&gt; str:\n    \"\"\"\n    Returns information about value type:\n    module.Class\n    Useful for debugging data obtained from\n    remote sources.\n\n    Args:\n        value (Any): Any value\n\n    Returns:\n        str: _description_\n    \"\"\"\n    m = value.__class__.__module__\n    n = value.__class__.__name__\n    return f\"{m}.{n}\"\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.api.API.setvariable","title":"setvariable","text":"<pre><code>setvariable(collection: dict, key: str, value: Any) -&gt; Any\n</code></pre> <p>Attempts to assign value to a dictionary under the given key.</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <code>dict</code> <p>Dictionary to use e.g. variables</p> required <code>key</code> <code>str</code> <p>Key to store thalue to be storese value under</p> required <code>value</code> <code>Any</code> <p>V</p> required <p>Returns:</p> Type Description <code>Any</code> <p>t.Any: Original value</p> Source code in <code>ankaflow/api.py</code> <pre><code>@staticmethod\ndef setvariable(collection: dict, key: str, value: t.Any) -&gt; t.Any:\n    \"\"\"\n    Attempts to assign value to a dictionary under the given key.\n\n    Args:\n        collection (dict): Dictionary to use e.g. variables\n        key (str): Key to store thalue to be storese value under\n        value (t.Any): V\n\n    Returns:\n        t.Any: Original value\n    \"\"\"\n    collection[key] = value\n    return value\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.api.API.sqltuple","title":"sqltuple","text":"<pre><code>sqltuple(iterable: Iterable, mode: str = 'string')\n</code></pre> <p>Returns SQL tuple literal from given iterable.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable</code> <p>Any iterable.</p> required <code>mode</code> <code>str</code> <p>\"string\"|\"number\". Defaults to \"string\".</p> <code>'string'</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>When unsupported mode is used</p> <p>Returns:</p> Name Type Description <code>str</code> <p>SQL tuple literal</p> Source code in <code>ankaflow/api.py</code> <pre><code>@staticmethod\ndef sqltuple(iterable: t.Iterable, mode: str = \"string\"):\n    \"\"\"\n    Returns SQL tuple literal from given iterable.\n\n    Args:\n        iterable (Iterable): Any iterable.\n        mode (str, optional): \"string\"|\"number\". Defaults to \"string\".\n\n    Raises:\n        NotImplementedError: When unsupported mode is used\n\n    Returns:\n        str: SQL tuple literal\n    \"\"\"\n    items = [str(it) for it in iterable]\n    if mode == \"string\":\n        result = \"','\".join(items)\n        return f\"('{result}')\"\n    elif mode == \"number\":\n        result = \"','\".join(items)\n        return f\"('{result}')\"\n    else:\n        raise NotImplementedError(f\"Invalid mode: {mode}\")\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.cli","title":"cli","text":""},{"location":"api/ankaflow/#ankaflow.cli.resolve_yaml_path","title":"resolve_yaml_path","text":"<pre><code>resolve_yaml_path(path_arg: str) -&gt; Path\n</code></pre> <p>Resolve YAML path, remapping 'DEMO' to a relative demo file path.</p> Source code in <code>ankaflow/cli.py</code> <pre><code>def resolve_yaml_path(path_arg: str) -&gt; Path:\n    \"\"\"Resolve YAML path, remapping 'DEMO' to a relative demo file path.\"\"\"\n    if path_arg.upper() == \"DEMO\":\n        return Path(__file__).parent / \"yaml\" / \"example.yaml\"\n    return Path(path_arg).resolve()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections","title":"connections","text":""},{"location":"api/ankaflow/#ankaflow.connections.load_connection","title":"load_connection","text":"<pre><code>load_connection(\n    cls: Type[\n        Union[\n            Connection,\n            RestConnection,\n            CustomConnection,\n            Dimension,\n        ]\n    ],\n) -&gt; Type[Connection]\n</code></pre> <p>Load built-in connection or custom connection from module</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Connection name</p> required <code>module</code> <code>str</code> <p>If set then try to load from given module. Defaults to None i.e. built-in connection.</p> required Source code in <code>ankaflow/connections/__init__.py</code> <pre><code>def load_connection(\n    cls: t.Type[\n        t.Union[m.Connection, m.RestConnection, m.CustomConnection, m.Dimension]\n    ],\n) -&gt; t.Type[Connection]:\n    \"\"\"\n    Load built-in connection or custom connection from module\n\n    Args:\n        name (str): Connection name\n        module (str, optional): If set then try to load from\n            given module. Defaults to None i.e. built-in connection.\n    \"\"\"\n    mth = getattr(cls, \"load\", None)\n\n    try:\n        if callable(mth):\n            loaded = cls.load()  # type: ignore\n            if not issubclass(loaded, Connection):\n                raise TypeError(\n                    f\"{cls.__name__} is not a subclass of Connection\"\n                )\n            return loaded\n\n        return getattr(current_module, cls.kind)\n\n    except (ImportError, AttributeError) as e:\n        raise NoConnectionError(\n            f\"Connection '{cls.kind}' unavailable: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery","title":"bigquery","text":""},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery","title":"BigQuery","text":"<pre><code>BigQuery(\n    duck: DDB,\n    name: str,\n    connection: UnionConnection,\n    context: FlowContext,\n    variables: Variables,\n    logger: Logger = None,\n)\n</code></pre> <p>               Bases: <code>Connection</code></p> Source code in <code>ankaflow/connections/connection.py</code> <pre><code>def __init__(\n    self,\n    duck: DDB,\n    name: str,\n    connection: UnionConnection,\n    context: m.FlowContext,\n    variables: m.Variables,\n    logger: logging.Logger = None,  # type: ignore[assignment]\n) -&gt; None:\n    self.c = duck\n    self.name = name\n    self.limit = 0\n    self._fields = connection.fields\n    self.cfg: m.ConnectionConfiguration = t.cast(\n        m.ConnectionConfiguration, connection.config\n    )\n    self.conn = connection\n    self.ctx = context\n    self.vars = variables\n    self.log = logger\n    self.locator = Locator(self.cfg)  # type: ignore[assignment]\n    self.schema_ = Schema(self.c)\n    if not logger:\n        self.log = logging.getLogger()\n        self.log.addHandler(logging.NullHandler())\n    self.init()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset()\n</code></pre> <p>Creates dataset if it does not exist.</p> Source code in <code>ankaflow/connections/bigquery.py</code> <pre><code>def create_dataset(self):\n    \"\"\"Creates dataset if it does not exist.\"\"\"\n    client = self._get_client()\n    dataset_id = f\"{self.cfg.bigquery.project}.{self.cfg.bigquery.dataset}\"\n    location = self.cfg.bigquery.region\n\n    try:\n        dataset = Dataset(dataset_id)\n        if location:\n            dataset.location = location\n\n        client.create_dataset(dataset)\n        log.info(\n            f\"Created dataset `{dataset_id}` in `{location or 'default'}` region.\"  # noqa:E501\n        )\n    except gex.Conflict:\n        log.info(f\" Dataset `{dataset_id}` already exists.\")\n    except Exception as e:\n        raise Exception(f\"Failed to create dataset `{dataset_id}`: {e}\")\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.init","title":"init","text":"<pre><code>init()\n</code></pre> <p>Initializes internal configs and client.</p> Source code in <code>ankaflow/connections/bigquery.py</code> <pre><code>def init(self):\n    \"\"\"Initializes internal configs and client.\"\"\"\n    self.conn = t.cast(BigQueryConnection, self.conn)\n    self.queryconfig = QueryJobConfig()\n    self.loadconfig = LoadJobConfig()\n    self._client = None\n    self._setup_configs()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.locate","title":"locate","text":"<pre><code>locate(\n    name: Optional[str] = None, use_wildcard: bool = False\n) -&gt; str\n</code></pre> <p>Returns the fully qualified BigQuery table name using dot notation.</p> Source code in <code>ankaflow/connections/bigquery.py</code> <pre><code>def locate(\n    self, name: t.Optional[str] = None, use_wildcard: bool = False\n) -&gt; str:\n    \"\"\"Returns the fully qualified BigQuery table name using dot notation.\"\"\"\n    name = name or self.conn.locator\n\n    # Normalize identifiers (remove quotes/backticks)\n    name = self._normalize_bq_identifier(name)\n    dataset = (\n        self._normalize_bq_identifier(self.cfg.bigquery.dataset)\n        if self.cfg.bigquery.dataset\n        else None\n    )\n\n    if dataset:\n        return f\"{dataset}.{name}\"\n    return name\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; Columns\n</code></pre> <p>Infers runtime schema from BigQuery data (via DuckDB) and returns as Fields.</p> Source code in <code>ankaflow/connections/bigquery.py</code> <pre><code>async def show_schema(self) -&gt; m.Columns:\n    \"\"\"Infers runtime schema from BigQuery data (via DuckDB) and returns as Fields.\"\"\"  # noqa:E501\n    try:\n        return await self.schema_.show(self.name)\n    except CatalogException:\n        pass\n\n    # go to source\n    tmp_name = f\"schema_df_{self.name}\"\n\n    try:\n        # Step 1: Fetch 1 row from BigQuery\n        query = parse_one(f\"SELECT * FROM {self.locate()} LIMIT 1\").sql(\n            dialect=self.dialect\n        )\n        df = self._execute_query_to_dataframe(query)\n\n        # Step 2: Register in DuckDB\n        await self.c.register(tmp_name, df)\n\n        # Step 3: Return inferred schema as m.Fields\n        return await self.schema_.show(tmp_name)\n\n    except Exception as err:\n        self.log.exception(f\"Failed to infer schema: {err}\")\n        return self.schema_.error(err)\n\n    finally:\n        # Step 4: Always unregister temp table\n        await self.c.unregister(tmp_name)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.sink","title":"sink","text":"<pre><code>sink(from_name: str)\n</code></pre> <p>Public entrypoint for writing DuckDB data to BigQuery.</p> Source code in <code>ankaflow/connections/bigquery.py</code> <pre><code>async def sink(self, from_name: str):\n    \"\"\"Public entrypoint for writing DuckDB data to BigQuery.\"\"\"\n    config = self._build_load_config_from_modes()\n    try:\n        return await self._sink_impl(from_name, config=config)\n    except Exception as ex:\n        raise Exception(ex)\n    finally:\n        if self._client:\n            self._client.close()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.sql","title":"sql","text":"<pre><code>sql(statement: str) -&gt; Any\n</code></pre> <p>Executes a BigQuery SQL statement and logs the result.</p> Source code in <code>ankaflow/connections/bigquery.py</code> <pre><code>async def sql(self, statement: str) -&gt; t.Any:\n    \"\"\"Executes a BigQuery SQL statement and logs the result.\"\"\"\n    try:\n        df: DataFrame = self._execute_query_to_dataframe(statement)\n        self.log.info(f\"SQL result:\\n{print_df(df, all_rows=False)}\")\n    except Exception as err:\n        raise Exception(f\"SQL execution failed: {err}\")\n    finally:\n        if self._client:\n            self._client.close()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.bigquery.BigQuery.tap","title":"tap","text":"<pre><code>tap(query: Optional[str] = None, limit: int = 0)\n</code></pre> <p>Executes a BigQuery query and loads result into DuckDB.</p> Source code in <code>ankaflow/connections/bigquery.py</code> <pre><code>async def tap(self, query: t.Optional[str] = None, limit: int = 0):\n    \"\"\"Executes a BigQuery query and loads result into DuckDB.\"\"\"\n    if not query:\n        raise ValueError(\n            \"BigQuery requires an explicit query \u2014 none was provided.\"\n        )\n\n    # TODO: may cause issues when running paraller\n    temp_name = \"bigdf\"\n    try:\n        ranked_query = self._build_query_with_ranking(query)\n        df = self._execute_query_to_dataframe(ranked_query)\n\n        await self._register_dataframe_to_duckdb(df, temp_name)\n\n        return await self.c.sql(f\"\"\"\n            CREATE TABLE \"{self.name}\" AS\n            SELECT * FROM \"{temp_name}\"\n        \"\"\")\n    except Exception as ex:\n        raise Exception(f\"tap failed: {ex}\")\n    finally:\n        if self._client:\n            self._client.close()\n        try:\n            await self.c.unregister(temp_name)\n        except Exception as cleanup_ex:\n            log.warning(\n                f\"Failed to unregister temp table '{temp_name}': {cleanup_ex}\"\n            )\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse","title":"clickhouse","text":""},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.Clickhouse","title":"Clickhouse","text":"<pre><code>Clickhouse(\n    duck: DDB,\n    name: str,\n    connection: UnionConnection,\n    context: FlowContext,\n    variables: Variables,\n    logger: Logger = None,\n)\n</code></pre> <p>               Bases: <code>Connection</code></p> Source code in <code>ankaflow/connections/connection.py</code> <pre><code>def __init__(\n    self,\n    duck: DDB,\n    name: str,\n    connection: UnionConnection,\n    context: m.FlowContext,\n    variables: m.Variables,\n    logger: logging.Logger = None,  # type: ignore[assignment]\n) -&gt; None:\n    self.c = duck\n    self.name = name\n    self.limit = 0\n    self._fields = connection.fields\n    self.cfg: m.ConnectionConfiguration = t.cast(\n        m.ConnectionConfiguration, connection.config\n    )\n    self.conn = connection\n    self.ctx = context\n    self.vars = variables\n    self.log = logger\n    self.locator = Locator(self.cfg)  # type: ignore[assignment]\n    self.schema_ = Schema(self.c)\n    if not logger:\n        self.log = logging.getLogger()\n        self.log.addHandler(logging.NullHandler())\n    self.init()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.Clickhouse.init","title":"init","text":"<pre><code>init() -&gt; None\n</code></pre> <p>Initializes connection-specific defaults.</p> Source code in <code>ankaflow/connections/clickhouse.py</code> <pre><code>def init(self) -&gt; None:\n    \"\"\"Initializes connection-specific defaults.\"\"\"\n    # Created by decorator\n    self.client: t.Optional[ClickhouseClient] = None\n    self.ch: t.Optional[Client] = None\n    self._client = ClickhouseClient(self.cfg)\n    self._blocksize: int = self.cfg.clickhouse.blocksize or DEFAULT_STREAM_BLOCK\n    self.progress = ProgressLogger(None, self.log)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.Clickhouse.locate","title":"locate","text":"<pre><code>locate(\n    name: Optional[str] = None, use_wildcard: bool = False\n) -&gt; str\n</code></pre> <p>Returns the fully-qualified table reference with validation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Table name override (unused).</p> <code>None</code> <code>use_wildcard</code> <code>bool</code> <p>Placeholder flag (unused).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Fully-qualified table name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If locator format is invalid.</p> Source code in <code>ankaflow/connections/clickhouse.py</code> <pre><code>def locate(self, name: t.Optional[str] = None, use_wildcard: bool = False) -&gt; str:\n    \"\"\"Returns the fully-qualified table reference with validation.\n\n    Args:\n        name (Optional[str]): Table name override (unused).\n        use_wildcard (bool): Placeholder flag (unused).\n\n    Returns:\n        str: Fully-qualified table name.\n\n    Raises:\n        ValueError: If locator format is invalid.\n    \"\"\"\n    locator = self.conn.locator\n    if \".\" in locator:\n        parts = locator.split(\".\")\n        if len(parts) != 2:\n            raise ValueError(f\"Invalid locator format: {locator}\")\n        if self.cfg.clickhouse.database:\n            raise ValueError(\n                f\"Locator '{locator}' must not include a database prefix when 'database' is set.\"  # noqa:E501\n            )\n        database, table = parts\n    else:\n        if not self.cfg.clickhouse.database:\n            raise ValueError(\n                f\"Locator '{locator}' must include a database prefix when 'database' is not set.\"  # noqa:E501\n            )\n        database, table = self.cfg.clickhouse.database, locator\n    return f'\"{database}\".\"{table}\"'\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.ClickhouseClient","title":"ClickhouseClient","text":"<pre><code>ClickhouseClient(cfg: ConnectionConfiguration)\n</code></pre> <p>Initializes a Clickhouse client wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>ConnectionConfiguration</code> <p>Connection configuration.</p> required Source code in <code>ankaflow/connections/clickhouse.py</code> <pre><code>def __init__(self, cfg: \"m.ConnectionConfiguration\"):\n    \"\"\"Initializes a Clickhouse client wrapper.\n\n    Args:\n        cfg (m.ConnectionConfiguration): Connection configuration.\n    \"\"\"\n    self.cfg = cfg\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.ClickhouseClient.connect","title":"connect","text":"<pre><code>connect() -&gt; Iterator[Client]\n</code></pre> <p>Context-managed connection that disconnects cleanly.</p> Source code in <code>ankaflow/connections/clickhouse.py</code> <pre><code>@contextlib.contextmanager\ndef connect(self) -&gt; t.Iterator[Client]:\n    \"\"\"Context-managed connection that disconnects cleanly.\"\"\"\n    client = Client(\n        host=self.cfg.clickhouse.host,\n        port=self.cfg.clickhouse.port,\n        user=self.cfg.clickhouse.username,\n        password=self.cfg.clickhouse.password,\n        database=self.cfg.clickhouse.database,\n        send_receive_timeout=1000,\n    )\n    try:\n        yield client\n    finally:\n        client.disconnect()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.ClickhouseClient.insert_dataframe","title":"insert_dataframe","text":"<pre><code>insert_dataframe(\n    client: Client, query: str, df: DataFrame\n) -&gt; Any\n</code></pre> <p>Inserts a DataFrame into Clickhouse using the given query.</p> Source code in <code>ankaflow/connections/clickhouse.py</code> <pre><code>def insert_dataframe(self, client: Client, query: str, df: pd.DataFrame) -&gt; t.Any:\n    \"\"\"Inserts a DataFrame into Clickhouse using the given query.\"\"\"\n    return client.insert_dataframe(query, df, settings={\"use_numpy\": True})\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.ClickhouseClient.query_df","title":"query_df","text":"<pre><code>query_df(client: Client, query: str) -&gt; DataFrame\n</code></pre> <p>Executes a SELECT query and returns a DataFrame.</p> Source code in <code>ankaflow/connections/clickhouse.py</code> <pre><code>def query_df(self, client: Client, query: str) -&gt; pd.DataFrame:\n    \"\"\"Executes a SELECT query and returns a DataFrame.\"\"\"\n    return client.query_dataframe(query, settings={\"use_numpy\": True})\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.clickhouse.ClickhouseClient.stream_query","title":"stream_query","text":"<pre><code>stream_query(\n    client: Client,\n    query: str,\n    block_size: int | None = None,\n) -&gt; Generator\n</code></pre> <p>Streams ClickHouse query results.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>A live Clickhouse connection.</p> required <code>query</code> <code>str</code> <p>SQL query.</p> required <code>block_size</code> <code>int</code> <p>Optional server-side row block size.</p> <code>None</code> <p>Yields:</p> Type Description <code>Generator</code> <p>row blocks from ClickHouse.</p> Source code in <code>ankaflow/connections/clickhouse.py</code> <pre><code>def stream_query(\n    self, client: Client, query: str, block_size: int | None = None\n) -&gt; t.Generator:\n    \"\"\"Streams ClickHouse query results.\n\n    Args:\n        client (Client): A live Clickhouse connection.\n        query (str): SQL query.\n        block_size (int): Optional server-side row block size.\n\n    Yields:\n        row blocks from ClickHouse.\n    \"\"\"\n    settings = {}\n    if block_size:\n        settings[\"max_block_size\"] = block_size\n    for row in client.execute_iter(\n        query, with_column_types=True, settings=settings\n    ):\n        yield row\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.connection","title":"connection","text":""},{"location":"api/ankaflow/#ankaflow.connections.connection.Connection","title":"Connection","text":"<pre><code>Connection(\n    duck: DDB,\n    name: str,\n    connection: UnionConnection,\n    context: FlowContext,\n    variables: Variables,\n    logger: Logger = None,\n)\n</code></pre> <p>Base class for connections such as Deltatable or parquet.</p> Source code in <code>ankaflow/connections/connection.py</code> <pre><code>def __init__(\n    self,\n    duck: DDB,\n    name: str,\n    connection: UnionConnection,\n    context: m.FlowContext,\n    variables: m.Variables,\n    logger: logging.Logger = None,  # type: ignore[assignment]\n) -&gt; None:\n    self.c = duck\n    self.name = name\n    self.limit = 0\n    self._fields = connection.fields\n    self.cfg: m.ConnectionConfiguration = t.cast(\n        m.ConnectionConfiguration, connection.config\n    )\n    self.conn = connection\n    self.ctx = context\n    self.vars = variables\n    self.log = logger\n    self.locator = Locator(self.cfg)  # type: ignore[assignment]\n    self.schema_ = Schema(self.c)\n    if not logger:\n        self.log = logging.getLogger()\n        self.log.addHandler(logging.NullHandler())\n    self.init()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Connection.init","title":"init","text":"<pre><code>init()\n</code></pre> <p>Additional initialization called from within init.</p> Source code in <code>ankaflow/connections/connection.py</code> <pre><code>def init(self):\n    \"\"\"\n    Additional initialization called from within\n    __init__.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Connection.ranking","title":"ranking","text":"<pre><code>ranking(\n    selectable: str, query: str, validate_simple=False\n) -&gt; tuple[str, str]\n</code></pre> <p>Injects row ranking logic to deduplicate records based on versioning.</p> <p>If the connection defines a <code>version</code> and <code>key</code>, modifies the query to include a <code>ROW_NUMBER() OVER (...) AS __rank__</code> column and wraps it in a subquery that can be filtered using <code>WHERE __rank__ = 1</code>.</p> <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]: Transformed SQL query and the WHERE clause if applicable.</p> Source code in <code>ankaflow/connections/connection.py</code> <pre><code>def ranking(\n    self, selectable: str, query: str, validate_simple=False\n) -&gt; tuple[str, str]:\n    \"\"\"\n    Injects row ranking logic to deduplicate records based on versioning.\n\n    If the connection defines a `version` and `key`, modifies the query\n    to include a `ROW_NUMBER() OVER (...) AS __rank__` column and wraps it\n    in a subquery that can be filtered using `WHERE __rank__ = 1`.\n\n    Returns:\n        tuple[str, str]: Transformed SQL query and the WHERE clause if applicable.\n    \"\"\"  # noqa: E501\n    if not isinstance(self.conn, VersionedConnection):\n        return query, \"\"\n    apply_ranking = bool(self.conn.version and self.conn.key)\n\n    # Validate before building\n    validate_simple_query(query, apply_ranking)\n    return build_ranked_query(\n        query=query,\n        selectable=selectable,\n        version=self.conn.version,\n        keys=self.conn.key,\n        dialect=self.dialect,\n    )\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Connection.sink","title":"sink","text":"<pre><code>sink(from_name: str)\n</code></pre> <p>Sink or store data from given <code>name</code> (typically previous stage)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>string</code> <p>table name to use as source</p> required Source code in <code>ankaflow/connections/connection.py</code> <pre><code>async def sink(self, from_name: str):\n    \"\"\"\n    Sink or store data from given `name` (typically previous stage)\n\n    Args:\n        name (string): table name to use as source\n    \"\"\"\n    raise NotImplementedError(\"Store object must be implemented\")\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Connection.sql","title":"sql","text":"<pre><code>sql(statement: str) -&gt; Any\n</code></pre> <p>Execute raw sql using the specified connection (if supported).</p> Source code in <code>ankaflow/connections/connection.py</code> <pre><code>async def sql(self, statement: str) -&gt; t.Any:\n    \"\"\"\n    Execute raw sql using the specified connection (if supported).\n    \"\"\"\n    raise NotImplementedError(\"SQL execution not available\")\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Connection.tap","title":"tap","text":"<pre><code>tap(query: Optional[str] = None, limit: int = 0)\n</code></pre> <p>Implements loading from source storage.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Reduce the number of rows returned. Defaults to 0.</p> <code>0</code> Source code in <code>ankaflow/connections/connection.py</code> <pre><code>async def tap(self, query: t.Optional[str] = None, limit: int = 0):\n    \"\"\"\n    Implements loading from source storage.\n\n    Args:\n        limit (int, optional): Reduce the number of rows returned.\n            Defaults to 0.\n    \"\"\"\n    raise NotImplementedError(\"Store object must be implemented\")\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Locator","title":"Locator","text":"<pre><code>Locator(config: ConnectionConfiguration)\n</code></pre> <p>Initializes the Locator with connection configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConnectionConfiguration</code> <p>Config with bucket, prefix, etc.</p> required Source code in <code>ankaflow/connections/connection.py</code> <pre><code>def __init__(self, config: ConnectionConfiguration) -&gt; None:\n    \"\"\"\n    Initializes the Locator with connection configuration.\n\n    Args:\n        config (ConnectionConfiguration): Config with bucket, prefix, etc.\n    \"\"\"\n    self.cfg = config\n    self.resolver = ConfigResolver(config)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Locator.locate","title":"locate","text":"<pre><code>locate(name: str, use_wildcard: bool = False) -&gt; CommonPath\n</code></pre> <p>Resolves a full path using bucket, prefix, and optional wildcard substitution.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Relative or absolute path string.</p> required <code>use_wildcard</code> <code>bool</code> <p>Whether to apply wildcard regex substitution.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>CommonPath</code> <code>CommonPath</code> <p>A fully resolved path.</p> Source code in <code>ankaflow/connections/connection.py</code> <pre><code>def locate(self, name: str, use_wildcard: bool = False) -&gt; CommonPath:\n    \"\"\"\n    Resolves a full path using bucket, prefix, and optional wildcard substitution.\n\n    Args:\n        name (str): Relative or absolute path string.\n        use_wildcard (bool): Whether to apply wildcard regex substitution.\n\n    Returns:\n        CommonPath: A fully resolved path.\n    \"\"\"  # noqa: E501\n    if use_wildcard and self.wildcard:\n        pattern = re.compile(self.wildcard[0])\n        name = re.sub(pattern, self.wildcard[1], name)\n\n    name_path = PathFactory.make(name)\n\n    # Rule: Absolute remote path must remain as is\n    if isinstance(name_path, RemotePath):\n        return name_path\n\n    # Rule: root must exist and be absolute\n    if not self.bucket:\n        raise ValueError(\"Root not specified\")\n    root_path = PathFactory.make(self.bucket)\n    if not root_path.is_absolute():\n        raise ValueError(\"Root must be absolute\")\n\n    # Rule: absolute name path directly under root\n    if name_path.is_absolute():\n        name_path = PathFactory.make(name_path.path.lstrip(\"/\"))\n        return root_path.joinpath(*name_path.parts)\n\n    # Rule: If name path is relative, it must be under bucket/prefix/.\n    # Normalize the prefix by stripping any leading slashes.\n    prefix_str = (self.prefix or \"\").lstrip(\"/\")\n\n    prefix_path_obj = None\n    if prefix_str:\n        prefix_path_obj = PathFactory.make(prefix_str)\n        # Guard 1: Ensure prefix is relative\n        if prefix_path_obj.is_absolute():\n            raise ValueError(f\"Configured data_prefix '{self.prefix}' must be a relative path.\")  # noqa: E501\n        # Guard 2: Prevent prefix from 'sneaking out' of root (via '..')\n        # This is a basic check.A more comprehensive check might\n        # involve full path resolution and comparison, but typically,\n        # preventing '..' at the prefix level is sufficient\n        # if the Path object's joinpath handles it correctly.\n        if any(part == \"..\" for part in prefix_path_obj.parts):\n            raise ValueError(f\"Configured data_prefix '{self.prefix}' cannot contain '..' segments.\")  # noqa: E501\n\n    # Build the full prefix path: root_path + prefix_str parts\n    # If no prefix_str, prefix_path will just be root_path.\n    prefix_path = root_path\n    if prefix_path_obj:\n        prefix_path = root_path.joinpath(*prefix_path_obj.parts)\n\n    # Join with prefix (already includes root)\n    return prefix_path.joinpath(*name_path.parts)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Schema","title":"Schema","text":"<pre><code>Schema(duck: DDB)\n</code></pre> <p>Class for working with table schemas</p> Source code in <code>ankaflow/connections/connection.py</code> <pre><code>def __init__(self, duck: DDB):\n    self.c = duck\n    self.schema_ = None  # type: ignore[assignment]\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Schema.generate","title":"generate","text":"<pre><code>generate(\n    table: str, fields: Columns, exists_ok: bool = False\n) -&gt; str\n</code></pre> <p>Generates CREATE TABLE statement from schema.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Table name to create</p> required <code>schema</code> <code>Fields</code> <p>List of fields (names and dtypes)</p> required <code>exists_ok</code> <code>bool</code> <p>If true then create statement includes <code>IF NOT EXISTS</code>. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>CREATE statement</p> Source code in <code>ankaflow/connections/connection.py</code> <pre><code>def generate(\n    self, table: str, fields: m.Columns, exists_ok: bool = False\n) -&gt; str:\n    \"\"\"\n    Generates CREATE TABLE statement from schema.\n\n    Args:\n        name (str): Table name to create\n        schema (m.Fields): List of fields (names and dtypes)\n        exists_ok (bool, optional): If true then create statement\n            includes `IF NOT EXISTS`. Defaults to False.\n\n    Returns:\n        str: CREATE statement\n    \"\"\"\n    cols = []\n    for f in fields:\n        cols.append(Column(f.name, f.type))  # type: ignore[assignment]\n    creator = PostgreSQLQuery().create_table(table)\n    if exists_ok:\n        creator = creator.if_not_exists()\n    creator = creator.columns(*cols)\n    return creator.get_sql()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.connection.Schema.show","title":"show","text":"<pre><code>show(table: str, query: str | None = None) -&gt; Columns\n</code></pre> <p>Returns the schema of a table as a validated Fields model.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>The name of the DuckDB table/view.</p> required <p>Returns:</p> Type Description <code>Columns</code> <p>m.Fields: List of validated fields.</p> Source code in <code>ankaflow/connections/connection.py</code> <pre><code>async def show(self, table: str, query: str | None = None) -&gt; m.Columns:\n    # OK, refactored. Do not touch\n    \"\"\"\n    Returns the schema of a table as a validated Fields model.\n\n    Args:\n        table (str): The name of the DuckDB table/view.\n\n    Returns:\n        m.Fields: List of validated fields.\n    \"\"\"\n    if query:\n        qry = f\"DESCRIBE {query}\"\n    else:\n        qry = f'DESCRIBE \"{table}\"'\n    rel = await self.c.sql(qry)\n    df = await rel.df()\n    df = df.rename(columns={\"column_name\": \"name\", \"column_type\": \"type\"})\n    items = [\n        m.Column.model_validate(it) for it in df.to_dict(orient=\"records\")\n    ]\n    return m.Columns.model_validate(items)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.delta","title":"delta","text":""},{"location":"api/ankaflow/#ankaflow.connections.delta.Deltatable","title":"Deltatable","text":"<pre><code>Deltatable(\n    duck: DDB,\n    name: str,\n    connection: UnionConnection,\n    context: FlowContext,\n    variables: Variables,\n    logger: Logger = None,\n)\n</code></pre> <p>               Bases: <code>Connection</code></p> Source code in <code>ankaflow/connections/connection.py</code> <pre><code>def __init__(\n    self,\n    duck: DDB,\n    name: str,\n    connection: UnionConnection,\n    context: m.FlowContext,\n    variables: m.Variables,\n    logger: logging.Logger = None,  # type: ignore[assignment]\n) -&gt; None:\n    self.c = duck\n    self.name = name\n    self.limit = 0\n    self._fields = connection.fields\n    self.cfg: m.ConnectionConfiguration = t.cast(\n        m.ConnectionConfiguration, connection.config\n    )\n    self.conn = connection\n    self.ctx = context\n    self.vars = variables\n    self.log = logger\n    self.locator = Locator(self.cfg)  # type: ignore[assignment]\n    self.schema_ = Schema(self.c)\n    if not logger:\n        self.log = logging.getLogger()\n        self.log.addHandler(logging.NullHandler())\n    self.init()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.delta.Deltatable.sink","title":"sink","text":"<pre><code>sink(from_name: str)\n</code></pre> <p>DeltaTable sink behavior is driven by available schema and data:</p>"},{"location":"api/ankaflow/#ankaflow.connections.delta.Deltatable.sink--strategy-matrix","title":"Strategy Matrix:","text":"Schema (conn.fields) Data Available Resulting Strategy No No SKIP \u2192 no action Yes No CREATE \u2192 define schema Yes Yes WRITE \u2192 create &amp; write No Yes WRITE \u2192 infer &amp; write -------------------------------------------------------------------- <p>When table already exists (is_deltatable): - CREATE \u2192 skip creation, write only if data present - WRITE  \u2192 directly append/merge data to table</p> <p>Optimizations (vacuum/compact) applied if self.conn.optimize is set.</p> Source code in <code>ankaflow/connections/delta.py</code> <pre><code>async def sink(self, from_name: str):\n    \"\"\"\n    DeltaTable sink behavior is driven by available schema and data:\n\n    Strategy Matrix:\n    --------------------------------------------------------------------\n    | Schema (conn.fields) | Data Available | Resulting Strategy       |\n    |----------------------|----------------|--------------------------|\n    | No                   | No             | SKIP \u2192 no action         |\n    | Yes                  | No             | CREATE \u2192 define schema   |\n    | Yes                  | Yes            | WRITE \u2192 create &amp; write   |\n    | No                   | Yes            | WRITE \u2192 infer &amp; write    |\n    --------------------------------------------------------------------\n\n    When table already exists (is_deltatable):\n    - CREATE \u2192 skip creation, write only if data present\n    - WRITE  \u2192 directly append/merge data to table\n\n    Optimizations (vacuum/compact) applied if self.conn.optimize is set.\n    \"\"\"\n    uri = self.locate()\n    view = await self._create_temp_view(from_name)\n    rows = await self._count_rows(view)  # NEW\n    data = await self._read_view(view)  # Arrow\n    await self._drop_view(view)\n\n    strategy = self._create_strategy(rows)\n    if strategy == SinkStrategy.SKIP:\n        self.log.info(f\"{self.name}: No schema or data to insert.\")\n        return\n\n    schema = await self._infer_schema(\n        data if strategy == SinkStrategy.WRITE else None\n    )\n    arrow_out = await self._to_arrow(data, schema)\n    arrow_out = self._cast_dict_to_string(arrow_out)\n\n    is_delta = dl.DeltaTable.is_deltatable(\n        uri, storage_options=self.delta_opts\n    )\n\n    if strategy == SinkStrategy.CREATE:\n        if not is_delta and self.conn.fields:\n            await self._create_deltatable(uri)\n        if rows &gt; 0:\n            await self._write_deltatable(uri, arrow_out)\n    elif strategy == SinkStrategy.WRITE:\n        await self._write_deltatable(\n            uri, arrow_out, create_flag=not is_delta\n        )\n\n    self.log.debug(\n        f\"Written {rows} records to {'existing' if is_delta else 'new'} Delta table\"  # noqa:E501\n    )\n\n    if self.conn.optimize is not None:\n        try:\n            await self._maybe_optimize(uri)\n        except DeltaError as ex:\n            raise e.ConnectionException(f\"Optimize failed: {ex}\") from None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.delta.Deltatable.sql","title":"sql","text":"<pre><code>sql(statement: str)\n</code></pre> <p>Executes a limited subset of SQL-like operations on a Delta table.</p> <p>Supported commands: - DROP Deltatable - TRUNCATE Deltatable - OPTIMIZE Deltatable [COMPACT][VACUUM] [AGE=[d|h]][DRY_RUN] [CLEANUP] <p>All other statements raise an error.</p> Source code in <code>ankaflow/connections/delta.py</code> <pre><code>async def sql(self, statement: str):\n    \"\"\"\n    Executes a limited subset of SQL-like operations on a Delta table.\n\n    Supported commands:\n    - DROP Deltatable\n    - TRUNCATE Deltatable\n    - OPTIMIZE Deltatable [COMPACT] [VACUUM] [AGE=&lt;int&gt;[d|h]] [DRY_RUN] [CLEANUP]\n\n    All other statements raise an error.\n    \"\"\"  # noqa: E501\n    tokens: t.List[str] = [t.casefold() for t in shlex.split(statement)]\n    if not tokens:\n        raise ValueError(f\"Invalid Delta SQL command: {statement}\")\n\n    if tokens == [\"drop\", \"deltatable\"]:\n        return self._drop_deltatable()\n    elif tokens == [\"truncate\", \"deltatable\"]:\n        return await self._truncate_deltatable()\n    elif (\n        len(tokens) &gt;= 2\n        and tokens[0] == \"optimize\"\n        and tokens[1] == \"deltatable\"\n    ):\n        return await self._sql_optimize(\n            statement\n        )  # pass original for parsing\n    else:\n        raise ValueError(f\"Invalid Delta SQL command: {statement}\")\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.connections.errors","title":"errors","text":""},{"location":"api/ankaflow/#ankaflow.connections.errors.ConnectionException","title":"ConnectionException","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for connection errors with optional message redaction.</p>"},{"location":"api/ankaflow/#ankaflow.const","title":"const","text":""},{"location":"api/ankaflow/#ankaflow.core","title":"core","text":""},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow","title":"AsyncFlow","text":"<pre><code>AsyncFlow(\n    defs: Stages,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Variables = None,\n    logger: Logger = None,\n    conn: DDB = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n)\n</code></pre> <p>Controls the flow of data based on the pipeline definition.</p> <p>The stages are run in the defined order and can either fetch data from a source (tap), transform via SQL, or store to a sink.</p> <p>Parameters:</p> Name Type Description Default <code>defs</code> <code>Stages</code> <p>List of stages to be processed.</p> required <code>context</code> <code>FlowContext</code> <p>Dynamic context for query templates.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Configuration passed to underlying storage or database.</p> required <code>variables</code> <code>Variables</code> <p>Variables for tap or prepare. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>Logger for show() requests. Defaults to None.</p> <code>None</code> <code>conn</code> <code>DDB</code> <p>Existing DuckDB connection. If not set, a new connection is created.</p> <code>None</code> <code>flow_control</code> <code>FlowControl</code> <p>Flow control configuration. Defaults to FlowControl().</p> <code>None</code> <code>log_context</code> <code>str</code> <p>Log context passed to each stage. Defaults to None.</p> <code>None</code> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(\n    self,\n    defs: m.Stages,\n    context: m.FlowContext,\n    default_connection: m.ConnectionConfiguration,\n    variables: m.Variables = None,\n    logger: logging.Logger = None,\n    conn: \"DDB\" = None,\n    flow_control: \"FlowControl\" = None,\n    log_context: str = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        defs (m.Stages): List of stages to be processed.\n        context (m.FlowContext): Dynamic context for query templates.\n        default_connection (m.ConnectionConfiguration): Configuration passed\n            to underlying storage or database.\n        variables (m.Variables, optional): Variables for tap or prepare.\n            Defaults to {}.\n        logger (logging.Logger, optional): Logger for show() requests.\n            Defaults to None.\n        conn (DDB, optional): Existing DuckDB connection. If not set, a new\n            connection is created.\n        flow_control (FlowControl, optional): Flow control configuration.\n            Defaults to FlowControl().\n        log_context (str, optional): Log context passed to each stage.\n            Defaults to None.\n    \"\"\"\n    self.defs = defs\n    self.vars = m.Variables() if variables is None else variables\n    self.lastname: t.Optional[str] = None\n    self.ctx: m.FlowContext = context\n    self.conn_opts: m.ConnectionConfiguration = default_connection\n    self.logger = logger\n    self.flow_control = flow_control or FlowControl()\n    self.log_context = log_context\n    self.idb: t.Optional[\"DDB\"] = conn\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.log","title":"log","text":"<pre><code>log: Logger\n</code></pre> <p>Returns the logger.</p>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.logs","title":"logs","text":"<pre><code>logs: List[str]\n</code></pre> <p>Returns the log buffer.</p>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.connect","title":"connect","text":"<pre><code>connect() -&gt; None\n</code></pre> <p>Connects to the underlying database if not already connected.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def connect(self) -&gt; None:\n    \"\"\"\n    Connects to the underlying database if not already connected.\n    \"\"\"\n    if self.idb:\n        return\n    else:\n        self.idb = await DDB.connect(self.conn_opts)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.df","title":"df","text":"<pre><code>df() -&gt; DataFrame\n</code></pre> <p>Returns a dataframe from the last stage.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from the last stage.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def df(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns a dataframe from the last stage.\n\n    Returns:\n        pd.DataFrame: Data from the last stage.\n    \"\"\"\n    if not self.lastname:\n        return pd.DataFrame()\n    coro = await self.idb.sql(f'SELECT * FROM \"{self.lastname}\"')\n    return await coro.df()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.pull_df","title":"pull_df","text":"<pre><code>pull_df() -&gt; DataFrame\n</code></pre> <p>Convenience method to pull the final dataframe.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from the last stage.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def pull_df(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Convenience method to pull the final dataframe.\n\n    Returns:\n        pd.DataFrame: Data from the last stage.\n    \"\"\"\n    await self.run()\n    return await self.df()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.run","title":"run","text":"<pre><code>run() -&gt; AsyncFlow\n</code></pre> <p>Runs the pipeline stages.</p> <p>Returns:</p> Name Type Description <code>AsyncFlow</code> <code>AsyncFlow</code> <p>Self instance after running the pipeline.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def run(self) -&gt; \"AsyncFlow\":\n    \"\"\"\n    Runs the pipeline stages.\n\n    Returns:\n        AsyncFlow: Self instance after running the pipeline.\n    \"\"\"\n    if not self.idb:\n        await self.connect()\n    start = arrow.get()\n\n    vars_info = {k: v for k, v in self.vars.items()}\n    self.log.info(f\"Pipeline called with variables:\\n{vars_info}\")\n    for step in self.defs.steps():\n        try:\n            if step.kind == \"header\":\n                continue\n            if step.log_level:\n                self.log.setLevel(step.log_level.value)\n            log_ctx = f\"[{self.log_context}]\" if self.log_context else \"\"\n            self.log.info(f\"{step.kind} '{step.name}' {log_ctx}\")\n            stage = StageBlock(\n                self.idb,  # type: ignore[assignment]\n                step,\n                self.ctx,\n                self.conn_opts,\n                variables=self.vars,\n                logger=self.log,\n                prevous_stage=self.lastname,  # type: ignore[assignment]\n                log_context=self.log_context,\n            )\n            self.lastname = await stage.do() or self.lastname\n        except Exception as ex:\n            msg = dd(f\"{ex.__class__.__name__} in {step.name}: {ex}\")\n            if step.on_error == FlowControl.ON_ERROR_FAIL:\n                if self.flow_control.on_error == FlowControl.ON_ERROR_FAIL:\n                    self.log.error(\n                        f\"Pipeline failed at '{step.name}':\\n{dd(msg)}\"\n                    )\n                    end = arrow.get()\n                    self.log.info(f\"Run duration: {end - start}\")\n                    raise e.FlowRunError(\n                        f\"Failed at '{step.name}':\\n{dd(msg)}\"\n                    )\n            else:\n                self.log.warning(f\"Failed '{step.name}':\\n{dd(msg)}\")\n        if step.throttle and step.throttle &gt; 0:\n            self.log.info(f\"Flow throttling {step.throttle}s\")\n            await sleep(step.throttle)\n    end = arrow.get()\n    self.log.setLevel(logging.INFO)\n    self.log.info(f\"Run duration: {end - start}\")\n    return self\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.AsyncFlow.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; List[SchemaItem]\n</code></pre> <p>Returns schemas for all supported stages.</p> <p>Returns:</p> Type Description <code>List[SchemaItem]</code> <p>t.List[m.models.SchemaItem]: List of schema items.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def show_schema(self) -&gt; t.List[m.core.SchemaItem]:\n    \"\"\"\n    Returns schemas for all supported stages.\n\n    Returns:\n        t.List[m.models.SchemaItem]: List of schema items.\n    \"\"\"\n    items: t.List[m.core.SchemaItem] = []\n    for step in self.defs:\n        try:\n            db = StageBlock(\n                self.idb,\n                step,\n                self.ctx,\n                self.conn_opts,\n                variables=self.vars,\n                logger=self.log,\n                prevous_stage=self.lastname,\n            )\n            st = await db.show_schema()\n            if st is None:\n                continue\n            elif isinstance(st, list):\n                items.extend(st)\n            elif isinstance(st, m.core.SchemaItem):\n                items.append(st)\n            else:\n                self.log.warning(\"Unexpected schema item type\")\n        except NotImplementedError:\n            continue\n        except Exception:\n            raise\n    return items\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.Flow","title":"Flow","text":"<pre><code>Flow(\n    defs: Stages,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Variables = None,\n    logger: Logger = None,\n    conn: DuckDBPyConnection = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n)\n</code></pre> <p>Flow controls the flow of data based on the pipeline definition. The stages are run in the defined order and can either fetch data from source (tap), transform via SQL statement, or store to sink.</p> <p>This is the sync version and should be used in most cases.</p> <p>Parameters:</p> Name Type Description Default <code>defs</code> <code>Stages</code> <p>List of stages to be processed.</p> required <code>context</code> <code>FlowContext</code> <p>dynamic context that can be used in query templates.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Configuration passed to underlying storage or database (s3 bucket, database connection string &amp;c).</p> required <code>variables</code> <code>dict</code> <p>Any variables that can be referenced by a tap or prepare. Ony data structures that can be passed to pl.from_dicts() are supported in taps. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>If set then show() requests are logged. Defaults to None.</p> <code>None</code> <code>conn</code> <code>DuckDBPyConnection</code> <p>Existing DuckDB connection. If not set new connection will be created.</p> <code>None</code> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(\n    self,\n    defs: m.Stages,\n    context: m.FlowContext,\n    default_connection: m.ConnectionConfiguration,\n    variables: m.Variables = None,\n    logger: logging.Logger = None,\n    conn: duckdb.DuckDBPyConnection = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        defs (Stages): List of stages to be processed.\n        context (FlowContext): dynamic context that can be used\n            in query templates.\n        default_connection (ConnectionConfiguration): Configuration\n            passed to underlying storage or database\n            (s3 bucket, database connection string &amp;c).\n        variables (dict, optional): Any variables that can be referenced\n            by a tap or prepare. Ony data structures that can be passed\n            to pl.from_dicts() are supported in taps. Defaults to {}.\n        logger (Logger, optional): If set then show() requests are logged.\n            Defaults to None.\n        conn (DuckDBPyConnection): Existing DuckDB connection. If not set\n            new connection will be created.\n    \"\"\"\n    self.flow = AsyncFlow(\n        defs,\n        context,\n        default_connection,\n        variables=m.Variables() if variables is None else variables,\n        logger=logger,\n        conn=conn,\n        flow_control=flow_control or FlowControl(),\n        log_context=log_context,\n    )\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.Flow.df","title":"df","text":"<pre><code>df()\n</code></pre> <p>Returns dataframe from the last stage</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def df(self):\n    \"\"\"Returns dataframe from the last stage\"\"\"\n    return asyncio_run(self.flow.df())\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.Flow.pull_df","title":"pull_df","text":"<pre><code>pull_df() -&gt; DataFrame\n</code></pre> <p>Convenience method to pull final dataframe</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: data from last stage</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def pull_df(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Convenience method to pull final dataframe\n\n    Returns:\n        pl.DataFrame: data from last stage\n    \"\"\"\n    asyncio_run(self.flow.run())\n    return asyncio_run(self.flow.df())\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.Flow.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; List[SchemaItem]\n</code></pre> <p>Returns schemas of all supported stages</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def show_schema(self) -&gt; t.List[m.core.SchemaItem]:\n    \"\"\"\n    Returns schemas of all supported stages\n    \"\"\"\n    return asyncio_run(self.flow.show_schema())\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow","title":"flow","text":""},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow","title":"AsyncFlow","text":"<pre><code>AsyncFlow(\n    defs: Stages,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Variables = None,\n    logger: Logger = None,\n    conn: DDB = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n)\n</code></pre> <p>Controls the flow of data based on the pipeline definition.</p> <p>The stages are run in the defined order and can either fetch data from a source (tap), transform via SQL, or store to a sink.</p> <p>Parameters:</p> Name Type Description Default <code>defs</code> <code>Stages</code> <p>List of stages to be processed.</p> required <code>context</code> <code>FlowContext</code> <p>Dynamic context for query templates.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Configuration passed to underlying storage or database.</p> required <code>variables</code> <code>Variables</code> <p>Variables for tap or prepare. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>Logger for show() requests. Defaults to None.</p> <code>None</code> <code>conn</code> <code>DDB</code> <p>Existing DuckDB connection. If not set, a new connection is created.</p> <code>None</code> <code>flow_control</code> <code>FlowControl</code> <p>Flow control configuration. Defaults to FlowControl().</p> <code>None</code> <code>log_context</code> <code>str</code> <p>Log context passed to each stage. Defaults to None.</p> <code>None</code> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(\n    self,\n    defs: m.Stages,\n    context: m.FlowContext,\n    default_connection: m.ConnectionConfiguration,\n    variables: m.Variables = None,\n    logger: logging.Logger = None,\n    conn: \"DDB\" = None,\n    flow_control: \"FlowControl\" = None,\n    log_context: str = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        defs (m.Stages): List of stages to be processed.\n        context (m.FlowContext): Dynamic context for query templates.\n        default_connection (m.ConnectionConfiguration): Configuration passed\n            to underlying storage or database.\n        variables (m.Variables, optional): Variables for tap or prepare.\n            Defaults to {}.\n        logger (logging.Logger, optional): Logger for show() requests.\n            Defaults to None.\n        conn (DDB, optional): Existing DuckDB connection. If not set, a new\n            connection is created.\n        flow_control (FlowControl, optional): Flow control configuration.\n            Defaults to FlowControl().\n        log_context (str, optional): Log context passed to each stage.\n            Defaults to None.\n    \"\"\"\n    self.defs = defs\n    self.vars = m.Variables() if variables is None else variables\n    self.lastname: t.Optional[str] = None\n    self.ctx: m.FlowContext = context\n    self.conn_opts: m.ConnectionConfiguration = default_connection\n    self.logger = logger\n    self.flow_control = flow_control or FlowControl()\n    self.log_context = log_context\n    self.idb: t.Optional[\"DDB\"] = conn\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.log","title":"log","text":"<pre><code>log: Logger\n</code></pre> <p>Returns the logger.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.logs","title":"logs","text":"<pre><code>logs: List[str]\n</code></pre> <p>Returns the log buffer.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.connect","title":"connect","text":"<pre><code>connect() -&gt; None\n</code></pre> <p>Connects to the underlying database if not already connected.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def connect(self) -&gt; None:\n    \"\"\"\n    Connects to the underlying database if not already connected.\n    \"\"\"\n    if self.idb:\n        return\n    else:\n        self.idb = await DDB.connect(self.conn_opts)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.df","title":"df","text":"<pre><code>df() -&gt; DataFrame\n</code></pre> <p>Returns a dataframe from the last stage.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from the last stage.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def df(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns a dataframe from the last stage.\n\n    Returns:\n        pd.DataFrame: Data from the last stage.\n    \"\"\"\n    if not self.lastname:\n        return pd.DataFrame()\n    coro = await self.idb.sql(f'SELECT * FROM \"{self.lastname}\"')\n    return await coro.df()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.pull_df","title":"pull_df","text":"<pre><code>pull_df() -&gt; DataFrame\n</code></pre> <p>Convenience method to pull the final dataframe.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from the last stage.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def pull_df(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Convenience method to pull the final dataframe.\n\n    Returns:\n        pd.DataFrame: Data from the last stage.\n    \"\"\"\n    await self.run()\n    return await self.df()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.run","title":"run","text":"<pre><code>run() -&gt; AsyncFlow\n</code></pre> <p>Runs the pipeline stages.</p> <p>Returns:</p> Name Type Description <code>AsyncFlow</code> <code>AsyncFlow</code> <p>Self instance after running the pipeline.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def run(self) -&gt; \"AsyncFlow\":\n    \"\"\"\n    Runs the pipeline stages.\n\n    Returns:\n        AsyncFlow: Self instance after running the pipeline.\n    \"\"\"\n    if not self.idb:\n        await self.connect()\n    start = arrow.get()\n\n    vars_info = {k: v for k, v in self.vars.items()}\n    self.log.info(f\"Pipeline called with variables:\\n{vars_info}\")\n    for step in self.defs.steps():\n        try:\n            if step.kind == \"header\":\n                continue\n            if step.log_level:\n                self.log.setLevel(step.log_level.value)\n            log_ctx = f\"[{self.log_context}]\" if self.log_context else \"\"\n            self.log.info(f\"{step.kind} '{step.name}' {log_ctx}\")\n            stage = StageBlock(\n                self.idb,  # type: ignore[assignment]\n                step,\n                self.ctx,\n                self.conn_opts,\n                variables=self.vars,\n                logger=self.log,\n                prevous_stage=self.lastname,  # type: ignore[assignment]\n                log_context=self.log_context,\n            )\n            self.lastname = await stage.do() or self.lastname\n        except Exception as ex:\n            msg = dd(f\"{ex.__class__.__name__} in {step.name}: {ex}\")\n            if step.on_error == FlowControl.ON_ERROR_FAIL:\n                if self.flow_control.on_error == FlowControl.ON_ERROR_FAIL:\n                    self.log.error(\n                        f\"Pipeline failed at '{step.name}':\\n{dd(msg)}\"\n                    )\n                    end = arrow.get()\n                    self.log.info(f\"Run duration: {end - start}\")\n                    raise e.FlowRunError(\n                        f\"Failed at '{step.name}':\\n{dd(msg)}\"\n                    )\n            else:\n                self.log.warning(f\"Failed '{step.name}':\\n{dd(msg)}\")\n        if step.throttle and step.throttle &gt; 0:\n            self.log.info(f\"Flow throttling {step.throttle}s\")\n            await sleep(step.throttle)\n    end = arrow.get()\n    self.log.setLevel(logging.INFO)\n    self.log.info(f\"Run duration: {end - start}\")\n    return self\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.AsyncFlow.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; List[SchemaItem]\n</code></pre> <p>Returns schemas for all supported stages.</p> <p>Returns:</p> Type Description <code>List[SchemaItem]</code> <p>t.List[m.models.SchemaItem]: List of schema items.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def show_schema(self) -&gt; t.List[m.core.SchemaItem]:\n    \"\"\"\n    Returns schemas for all supported stages.\n\n    Returns:\n        t.List[m.models.SchemaItem]: List of schema items.\n    \"\"\"\n    items: t.List[m.core.SchemaItem] = []\n    for step in self.defs:\n        try:\n            db = StageBlock(\n                self.idb,\n                step,\n                self.ctx,\n                self.conn_opts,\n                variables=self.vars,\n                logger=self.log,\n                prevous_stage=self.lastname,\n            )\n            st = await db.show_schema()\n            if st is None:\n                continue\n            elif isinstance(st, list):\n                items.extend(st)\n            elif isinstance(st, m.core.SchemaItem):\n                items.append(st)\n            else:\n                self.log.warning(\"Unexpected schema item type\")\n        except NotImplementedError:\n            continue\n        except Exception:\n            raise\n    return items\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.BaseStageHandler","title":"BaseStageHandler","text":"<pre><code>BaseStageHandler(block: StageBlock)\n</code></pre> <p>Abstract base class for stage execution handlers.</p> <p>Attributes:</p> Name Type Description <code>datablock</code> <code>Datablock</code> <p>The datablock instance holding execution context.</p> <p>Parameters:</p> Name Type Description Default <code>block</code> <code>StageBlock</code> <p>The stage to be executed.</p> required Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(self, block: \"StageBlock\") -&gt; None:\n    \"\"\"\n    Args:\n        block (StageBlock): The stage to be executed.\n    \"\"\"\n    self.block = block\n    self.defs = block.defs\n    self.log = block.log\n    self.default_connection = block.default_connection\n    self.idb = block.idb\n    self.ctx = block.ctx\n    self.vars = block.vars\n    self._connection: c.Connection | None = None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.BaseStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes the stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Result of the stage execution.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def execute(self) -&gt; t.Union[str, None]:\n    \"\"\"Executes the stage.\n\n    Returns:\n        t.Union[str, None]: Result of the stage execution.\n    \"\"\"\n    raise NotImplementedError(\"Must implement execute() in subclass.\")\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.Flow","title":"Flow","text":"<pre><code>Flow(\n    defs: Stages,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Variables = None,\n    logger: Logger = None,\n    conn: DuckDBPyConnection = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n)\n</code></pre> <p>Flow controls the flow of data based on the pipeline definition. The stages are run in the defined order and can either fetch data from source (tap), transform via SQL statement, or store to sink.</p> <p>This is the sync version and should be used in most cases.</p> <p>Parameters:</p> Name Type Description Default <code>defs</code> <code>Stages</code> <p>List of stages to be processed.</p> required <code>context</code> <code>FlowContext</code> <p>dynamic context that can be used in query templates.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Configuration passed to underlying storage or database (s3 bucket, database connection string &amp;c).</p> required <code>variables</code> <code>dict</code> <p>Any variables that can be referenced by a tap or prepare. Ony data structures that can be passed to pl.from_dicts() are supported in taps. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>If set then show() requests are logged. Defaults to None.</p> <code>None</code> <code>conn</code> <code>DuckDBPyConnection</code> <p>Existing DuckDB connection. If not set new connection will be created.</p> <code>None</code> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(\n    self,\n    defs: m.Stages,\n    context: m.FlowContext,\n    default_connection: m.ConnectionConfiguration,\n    variables: m.Variables = None,\n    logger: logging.Logger = None,\n    conn: duckdb.DuckDBPyConnection = None,\n    flow_control: FlowControl = None,\n    log_context: str = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        defs (Stages): List of stages to be processed.\n        context (FlowContext): dynamic context that can be used\n            in query templates.\n        default_connection (ConnectionConfiguration): Configuration\n            passed to underlying storage or database\n            (s3 bucket, database connection string &amp;c).\n        variables (dict, optional): Any variables that can be referenced\n            by a tap or prepare. Ony data structures that can be passed\n            to pl.from_dicts() are supported in taps. Defaults to {}.\n        logger (Logger, optional): If set then show() requests are logged.\n            Defaults to None.\n        conn (DuckDBPyConnection): Existing DuckDB connection. If not set\n            new connection will be created.\n    \"\"\"\n    self.flow = AsyncFlow(\n        defs,\n        context,\n        default_connection,\n        variables=m.Variables() if variables is None else variables,\n        logger=logger,\n        conn=conn,\n        flow_control=flow_control or FlowControl(),\n        log_context=log_context,\n    )\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.Flow.df","title":"df","text":"<pre><code>df()\n</code></pre> <p>Returns dataframe from the last stage</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def df(self):\n    \"\"\"Returns dataframe from the last stage\"\"\"\n    return asyncio_run(self.flow.df())\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.Flow.pull_df","title":"pull_df","text":"<pre><code>pull_df() -&gt; DataFrame\n</code></pre> <p>Convenience method to pull final dataframe</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: data from last stage</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def pull_df(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Convenience method to pull final dataframe\n\n    Returns:\n        pl.DataFrame: data from last stage\n    \"\"\"\n    asyncio_run(self.flow.run())\n    return asyncio_run(self.flow.df())\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.Flow.show_schema","title":"show_schema","text":"<pre><code>show_schema() -&gt; List[SchemaItem]\n</code></pre> <p>Returns schemas of all supported stages</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def show_schema(self) -&gt; t.List[m.core.SchemaItem]:\n    \"\"\"\n    Returns schemas of all supported stages\n    \"\"\"\n    return asyncio_run(self.flow.show_schema())\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.InternalStageHandler","title":"InternalStageHandler","text":"<pre><code>InternalStageHandler(block: StageBlock)\n</code></pre> <p>               Bases: <code>BaseStageHandler</code></p> <p>Handles execution for internal stages.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(self, block: \"StageBlock\") -&gt; None:\n    \"\"\"\n    Args:\n        block (StageBlock): The stage to be executed.\n    \"\"\"\n    self.block = block\n    self.defs = block.defs\n    self.log = block.log\n    self.default_connection = block.default_connection\n    self.idb = block.idb\n    self.ctx = block.ctx\n    self.vars = block.vars\n    self._connection: c.Connection | None = None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.InternalStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes an internal stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Name of the created table.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def execute(self) -&gt; t.Union[str, None]:\n    \"\"\"\n    Executes an internal stage.\n\n    Returns:\n        t.Union[str, None]: Name of the created table.\n    \"\"\"\n    src = self.defs\n    name = src.name\n    resp = await self.idb.sql(src.query)\n    if src.show:\n        try:\n            df = await resp.df()\n            await self.idb.register(f\"_tmp_{name}\", df)\n            await self.idb.sql(\n                f'CREATE TABLE \"{name}\" AS SELECT * FROM \"_tmp_{name}\"'\n            )\n            self.log.info(await self._show(src.show))\n        except AttributeError:\n            self.log.info(\n                dd(f\"{src.kind} &gt; {src.name}:\\nNothing to show\")\n            )\n    return name\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.PipelineStageHandler","title":"PipelineStageHandler","text":"<pre><code>PipelineStageHandler(block: StageBlock)\n</code></pre> <p>               Bases: <code>BaseStageHandler</code></p> <p>Handles execution for pipeline stages, including loop implementation.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(self, block: \"StageBlock\") -&gt; None:\n    \"\"\"\n    Args:\n        block (StageBlock): The stage to be executed.\n    \"\"\"\n    self.block = block\n    self.defs = block.defs\n    self.log = block.log\n    self.default_connection = block.default_connection\n    self.idb = block.idb\n    self.ctx = block.ctx\n    self.vars = block.vars\n    self._connection: c.Connection | None = None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.PipelineStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes a pipeline stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Result of the pipeline stage execution.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def execute(self) -&gt; t.Union[str, None]:\n    \"\"\"\n    Executes a pipeline stage.\n\n    Returns:\n        t.Union[str, None]: Result of the pipeline stage execution.\n    \"\"\"\n    db = self.block\n    if db.prev:\n        # Pipeline looping over previous stage results\n        rel = await db.idb.sql(f'SELECT * FROM \"{db.prev}\"')\n        df = await rel.df()\n        items = df.to_dict(orient=\"records\")\n        for item in items:\n            # Create new variables for each loop iteration\n            db.vars[\"loop_control\"] = item\n            # Create a deep copy of the definitions for re-rendering\n            local_copy = db.defs.model_copy(deep=True)  # &lt;- Pydantic 2-safe\n            local = db.prepare(local_copy)\n            # local: m.Datablock = db.prepare(deepcopy(db.defs))\n            if bool(local.skip_if):\n                db.log.info(f\"Skip {local.name}\")\n                continue\n            flow = AsyncFlow(\n                local.stages,\n                db.ctx,\n                db.default_connection,\n                variables=db.vars,\n                logger=db.log,\n                conn=db.idb,\n                log_context=item,\n            )\n            try:\n                await flow.run()\n            except Exception as err:\n                db.log.error(\n                    print_error(\n                        \"Failure in flow\",\n                        err,\n                        \"Current control variable:\",\n                        item,\n                    )\n                )\n                raise\n            finally:\n                db.vars.pop(\"loop_control\", None)\n        return None\n    else:\n        # Pipeline with no loop\n        step = db.prepare(db.defs)\n        if bool(step.skip_if):\n            db.log.info(f\"Skip {step.name}\")\n            return None\n        flow = AsyncFlow(\n            step.stages,\n            db.ctx,\n            db.default_connection,\n            variables=db.vars,\n            logger=db.log,\n            conn=db.idb,\n        )\n        await flow.run()\n        return None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.SQLStageHandler","title":"SQLStageHandler","text":"<pre><code>SQLStageHandler(block: StageBlock)\n</code></pre> <p>               Bases: <code>BaseStageHandler</code></p> <p>Handles execution for SQL stages.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(self, block: \"StageBlock\") -&gt; None:\n    \"\"\"\n    Args:\n        block (StageBlock): The stage to be executed.\n    \"\"\"\n    self.block = block\n    self.defs = block.defs\n    self.log = block.log\n    self.default_connection = block.default_connection\n    self.idb = block.idb\n    self.ctx = block.ctx\n    self.vars = block.vars\n    self._connection: c.Connection | None = None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.SQLStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes an SQL stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Name of the target object.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def execute(self) -&gt; t.Union[str, None]:\n    \"\"\"\n    Executes an SQL stage.\n\n    Returns:\n        t.Union[str, None]: Name of the target object.\n    \"\"\"\n    src = self.defs\n    name = src.name\n    conn = self._connection\n    await self.idb.inject_secrets(src.name, src.connection.config)\n    await conn.sql(src.query)\n    if src.show:\n        self.log.info(await self._show(src.show))\n    return name\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.SinkStageHandler","title":"SinkStageHandler","text":"<pre><code>SinkStageHandler(block: StageBlock)\n</code></pre> <p>               Bases: <code>BaseStageHandler</code></p> <p>Handles execution for sink stages.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(self, block: \"StageBlock\") -&gt; None:\n    \"\"\"\n    Args:\n        block (StageBlock): The stage to be executed.\n    \"\"\"\n    self.block = block\n    self.defs = block.defs\n    self.log = block.log\n    self.default_connection = block.default_connection\n    self.idb = block.idb\n    self.ctx = block.ctx\n    self.vars = block.vars\n    self._connection: c.Connection | None = None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.SinkStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes a sink stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: None.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def execute(self) -&gt; t.Union[str, None]:\n    \"\"\"\n    Executes a sink stage.\n\n    Returns:\n        t.Union[str, None]: None.\n    \"\"\"\n    src = self.defs\n    conn = self._connection\n    await self.idb.inject_secrets(src.name, src.connection.config)\n    if src.query:\n        await self.idb.sql(src.query)\n        await conn.sink(src.name)\n    else:\n        await conn.sink(self.block.prev)\n    if src.show:\n        self.log.info(\n            dd(\n                f\"{src.connection.kind} &gt; {src.name}:\\n{src.connection.locator}\"\n            )\n        )\n    return None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.StageBlock","title":"StageBlock","text":"<pre><code>StageBlock(\n    conn: DDB,\n    defs: Stage,\n    context: FlowContext,\n    default_connection: ConnectionConfiguration,\n    variables: Optional[Variables] = None,\n    logger: Optional[Logger] = None,\n    prevous_stage: Optional[str] = None,\n    log_context: Optional[str] = None,\n)\n</code></pre> <p>Executable piece of a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>DDB</code> <p>DuckDB connection.</p> required <code>defs</code> <code>Stage</code> <p>Stage definition.</p> required <code>context</code> <code>FlowContext</code> <p>Context object.</p> required <code>default_connection</code> <code>ConnectionConfiguration</code> <p>Global persistent connection configuration.</p> required <code>variables</code> <code>Variables</code> <p>Any variables passed to pipeline. Defaults to {}.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>If set then messages are sent to this logger. Defaults to None.</p> <code>None</code> <code>prevous_stage</code> <code>str</code> <p>Reference to previous stage, used in sink to obtain data from. Defaults to None.</p> <code>None</code> <code>log_context</code> <code>str</code> <p>Additional logging context. Defaults to None.</p> <code>None</code> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(\n    self,\n    conn: \"DDB\",\n    defs: m.Stage,\n    context: m.FlowContext,\n    default_connection: m.ConnectionConfiguration,\n    variables: t.Optional[m.Variables] = None,\n    logger: t.Optional[logging.Logger] = None,\n    prevous_stage: t.Optional[str] = None,\n    log_context: t.Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        conn (DDB): DuckDB connection.\n        defs (m.Stage): Stage definition.\n        context (m.FlowContext): Context object.\n        default_connection (m.ConnectionConfiguration): Global persistent\n            connection configuration.\n        variables (m.Variables, optional): Any variables passed to pipeline.\n            Defaults to {}.\n        logger (logging.Logger, optional): If set then messages are sent to\n            this logger. Defaults to None.\n        prevous_stage (str, optional): Reference to previous stage, used in\n            sink to obtain data from. Defaults to None.\n        log_context (str, optional): Additional logging context.\n            Defaults to None.\n    \"\"\"\n    self.idb = conn\n    self.log = logger or logging.getLogger(__name__)\n    self.vars = m.Variables() if variables is None else variables\n    self.default_connection = default_connection\n    self.ctx = context\n    self.defs = defs\n    self.prev = prevous_stage\n    self.log_context = log_context\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.StageBlock.do","title":"do","text":"<pre><code>do() -&gt; Union[str, None]\n</code></pre> <p>Executes the current block.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Executed block name.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def do(self) -&gt; t.Union[str, None]:\n    \"\"\"\n    Executes the current block.\n\n    Returns:\n        t.Union[str, None]: Executed block name.\n    \"\"\"\n    self.defs = self.prepare(self.defs)\n    if bool(self.defs.skip_if):\n        self.log.info(f\"Skip '{self.defs.name}'\")\n        return None\n    handler = await StageFactory.get_handler(self)\n    await handler._init_connection()\n    return await handler.execute()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.StageBlock.prepare","title":"prepare","text":"<pre><code>prepare(src: Stage) -&gt; Stage\n</code></pre> <p>Prepare dynamic variables in the block.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Stage</code> <p>Current block.</p> required <p>Returns:</p> Type Description <code>Stage</code> <p>m.Datablock: Block with variables evaluated.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def prepare(self, src: m.Stage) -&gt; m.Stage:\n    \"\"\"\n    Prepare dynamic variables in the block.\n\n    Args:\n        src (m.Stage): Current block.\n\n    Returns:\n        m.Datablock: Block with variables evaluated.\n    \"\"\"\n    self.renderer = Renderer(context=self.ctx, API=API, variables=self.vars)\n    if src.skip_if:\n        src.skip_if = string_to_bool(self.render(src.skip_if))\n    if src.query:\n        src.query = self.render(src.query)\n    if src.connection:\n        src.connection = self.render(src.connection)\n    return src\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.StageBlock.render","title":"render","text":"<pre><code>render(templ: Union[str, BaseModel]) -&gt; str\n</code></pre> <p>Evaluates a template string using context and API.</p> Available variables in the template <ul> <li>context: Context object.</li> <li>API: API object.</li> <li>variables: Variables dictionary.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>templ</code> <code>Union[str, BaseModel]</code> <p>Template containing variables.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Evaluated query string.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def render(self, templ: t.Union[str, BaseModel]) -&gt; str:\n    \"\"\"\n    Evaluates a template string using context and API.\n\n    Available variables in the template:\n      - context: Context object.\n      - API: API object.\n      - variables: Variables dictionary.\n\n    Args:\n        templ (t.Union[str, m.BaseModel]): Template containing variables.\n\n    Returns:\n        str: Evaluated query string.\n    \"\"\"\n    try:\n        cls = None\n        if isinstance(templ, BaseModel):\n            cls = type(templ)\n            templ = templ.model_dump(mode=\"json\")  # type: ignore[assignment]\n        out = self.renderer.render(templ)  # type: ignore[assignment]\n        if cls is not None:\n            out = cls.model_validate(out)\n        return out  # type: ignore\n    except Exception as e:\n        self.log.error(templ)\n        self.log.error(e)\n        raise\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.StageFactory","title":"StageFactory","text":"<p>Factory to obtain the appropriate stage handler based on stage kind.</p>"},{"location":"api/ankaflow/#ankaflow.core.flow.StageFactory.get_handler","title":"get_handler","text":"<pre><code>get_handler(block: StageBlock) -&gt; BaseStageHandler\n</code></pre> <p>Creates a stage handler instance.</p> <p>Parameters:</p> Name Type Description Default <code>block</code> <code>StageBlock</code> <p>The datablock instance.</p> required <p>Returns:</p> Name Type Description <code>BaseStageHandler</code> <code>BaseStageHandler</code> <p>The corresponding stage handler.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>@staticmethod\nasync def get_handler(block: \"StageBlock\") -&gt; BaseStageHandler:\n    \"\"\"\n    Creates a stage handler instance.\n\n    Args:\n       block (\"StageBlock\"): The datablock instance.\n\n    Returns:\n        BaseStageHandler: The corresponding stage handler.\n    \"\"\"\n    try:\n        kind = block.defs.kind.lower()\n        hndlr = StageFactory.HANDLERS[kind]\n        return hndlr(block)\n    except (KeyError, TypeError):\n        raise ValueError(f\"Unknown stage kind: {kind!r}\")\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.TapStageHandler","title":"TapStageHandler","text":"<pre><code>TapStageHandler(block: StageBlock)\n</code></pre> <p>               Bases: <code>BaseStageHandler</code></p> <p>Handles execution for source stages.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(self, block: \"StageBlock\") -&gt; None:\n    \"\"\"\n    Args:\n        block (StageBlock): The stage to be executed.\n    \"\"\"\n    self.block = block\n    self.defs = block.defs\n    self.log = block.log\n    self.default_connection = block.default_connection\n    self.idb = block.idb\n    self.ctx = block.ctx\n    self.vars = block.vars\n    self._connection: c.Connection | None = None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.TapStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes a source stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Name of the created source table.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def execute(self) -&gt; t.Union[str, None]:\n    \"\"\"\n    Executes a source stage.\n\n    Returns:\n        t.Union[str, None]: Name of the created source table.\n    \"\"\"\n    src = self.defs\n    name = src.name\n    conn = self._connection\n    await self.idb.inject_secrets(src.name, src.connection.config)\n    try:\n        await conn.tap(query=src.query)\n    except Exception as e:\n        log.error(e)\n        await self.idb.sql(f'DROP TABLE IF EXISTS \"{name}\"')\n        raise\n    if src.show_schema:\n        try:\n            fields = await conn.show_schema()\n            self.log.info(fields.print())\n        except Exception as e:\n            self.log.warning(f\"Cannot show schema: {e}\")\n    if src.show:\n        self.log.info(await self._show(src.show))\n    return name\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.TransformStageHandler","title":"TransformStageHandler","text":"<pre><code>TransformStageHandler(block: StageBlock)\n</code></pre> <p>               Bases: <code>BaseStageHandler</code></p> <p>Handles execution for transform stages.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>def __init__(self, block: \"StageBlock\") -&gt; None:\n    \"\"\"\n    Args:\n        block (StageBlock): The stage to be executed.\n    \"\"\"\n    self.block = block\n    self.defs = block.defs\n    self.log = block.log\n    self.default_connection = block.default_connection\n    self.idb = block.idb\n    self.ctx = block.ctx\n    self.vars = block.vars\n    self._connection: c.Connection | None = None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.core.flow.TransformStageHandler.execute","title":"execute","text":"<pre><code>execute() -&gt; Union[str, None]\n</code></pre> <p>Executes a transform stage.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>t.Union[str, None]: Name of the created view.</p> Source code in <code>ankaflow/core/flow.py</code> <pre><code>async def execute(self) -&gt; t.Union[str, None]:\n    \"\"\"\n    Executes a transform stage.\n\n    Returns:\n        t.Union[str, None]: Name of the created view.\n    \"\"\"\n    src = self.defs\n    name = src.name\n    await self.idb.sql(f'CREATE OR REPLACE VIEW \"{name}\" as {src.query}')\n    if src.show_schema:\n        sch = c.connection.Schema(self.idb)\n        fields: m.Columns = await sch.show(name)\n        self.log.info(f\"Schema of '{name}'\\n{fields.print()}\")\n    if src.show:\n        self.log.info(await self._show(src.show))\n    return name\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.errors","title":"errors","text":""},{"location":"api/ankaflow/#ankaflow.internal","title":"internal","text":""},{"location":"api/ankaflow/#ankaflow.internal.browser","title":"browser","text":""},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB","title":"DDB","text":"<pre><code>DDB(\n    connection_options: ConnectionConfiguration,\n    persisted: Optional[str] = None,\n)\n</code></pre> <p>Initializes the DuckDB browser-bound client.</p> <p>Parameters:</p> Name Type Description Default <code>connection_options</code> <code>ConnectionConfiguration</code> <p>Connection settings.</p> required Source code in <code>ankaflow/internal/browser.py</code> <pre><code>def __init__(\n    self,\n    connection_options: ConnectionConfiguration,\n    persisted: t.Optional[str] = None,\n):\n    \"\"\"\n    Initializes the DuckDB browser-bound client.\n\n    Args:\n        connection_options (ConnectionConfiguration): Connection settings.\n    \"\"\"\n    self.conn_opts = connection_options\n    self.secrets: dict[str, BucketConfig] = {}\n    self.fs = FileSystem(\"/tmp\")\n    self.remote = RemoteObject(self.secrets, self.fs)\n    self._c: t.Optional[duckdb.DuckDBPyConnection] = None\n    self.dbname = persisted or \":memory:\"\n    self.unsupported_functions = [\"delta_scan\", \"postgres_scan\"]\n    self._init_secrets(connection_options)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.connect","title":"connect","text":"<pre><code>connect() -&gt; DDB\n</code></pre> <p>Establishes a DuckDB connection and initializes context.</p> Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def connect(self) -&gt; \"DDB\":\n    \"\"\"Establishes a DuckDB connection and initializes context.\"\"\"\n    self.c = duckdb.connect(self.dbname)\n    await self._init_settings()\n    await self._init_functions()\n    return self\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.delta_scan","title":"delta_scan","text":"<pre><code>delta_scan()\n</code></pre> <p>Stub for unsupported delta_scan.</p> Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def delta_scan(self):\n    \"\"\"Stub for unsupported delta_scan.\"\"\"\n    raise NotImplementedError(\n        \"delta_scan is not supported in browser environment\"\n    )\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.get","title":"get","text":"<pre><code>get() -&gt; DuckDBPyConnection\n</code></pre> <p>Returns the internal DuckDB connection object.</p> Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def get(self) -&gt; duckdb.DuckDBPyConnection:\n    \"\"\"Returns the internal DuckDB connection object.\"\"\"\n    return self.c\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.inject_secrets","title":"inject_secrets","text":"<pre><code>inject_secrets(\n    name: str, connection_options: ConnectionConfiguration\n)\n</code></pre> <p>Injects secret configuration dynamically.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Secret identifier.</p> required <code>connection_options</code> <code>ConnectionConfiguration</code> <p>Configuration details.</p> required Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def inject_secrets(\n    self, name: str, connection_options: ConnectionConfiguration\n):\n    \"\"\"\n    Injects secret configuration dynamically.\n\n    Args:\n        name (str): Secret identifier.\n        connection_options (ConnectionConfiguration): Configuration details.\n    \"\"\"\n    self._init_secrets(connection_options)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.parquet_scan","title":"parquet_scan","text":"<pre><code>parquet_scan()\n</code></pre> <p>Stub for unsupported parquet_scan.</p> Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def parquet_scan(self):\n    \"\"\"Stub for unsupported parquet_scan.\"\"\"\n    raise NotImplementedError(\"parquet_scan is not yet implemented\")\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.read_csv","title":"read_csv","text":"<pre><code>read_csv(\n    data: str, table: str, read_opts: dict\n) -&gt; DuckDBPyRelation\n</code></pre> <p>Reads CSV data into a DuckDB table.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>File path or URL.</p> required <code>table</code> <code>str</code> <p>Target table name.</p> required <code>read_opts</code> <code>dict</code> <p>Options passed to DuckDB read_csv().</p> required <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>duckdb.DuckDBPyRelation: The resulting table.</p> Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def read_csv(\n    self, data: str, table: str, read_opts: dict\n) -&gt; duckdb.DuckDBPyRelation:\n    \"\"\"\n    Reads CSV data into a DuckDB table.\n\n    Args:\n        data (str): File path or URL.\n        table (str): Target table name.\n        read_opts (dict): Options passed to DuckDB read_csv().\n\n    Returns:\n        duckdb.DuckDBPyRelation: The resulting table.\n    \"\"\"\n    tbl = self.c.read_csv(data, **read_opts)\n    try:\n        self.c.sql(f\"INSERT INTO {table} FROM tbl;\")\n    except duckdb.CatalogException:\n        self.c.sql(f\"CREATE TABLE {table} AS FROM tbl;\")\n    return tbl\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.read_json","title":"read_json","text":"<pre><code>read_json(\n    data: str, table: str, read_opts: dict\n) -&gt; DuckDBPyRelation\n</code></pre> <p>Reads JSON data into a DuckDB table.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>File path or URL.</p> required <code>table</code> <code>str</code> <p>Target table name.</p> required <code>read_opts</code> <code>dict</code> <p>Options passed to DuckDB read_json().</p> required <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>duckdb.DuckDBPyRelation: The resulting table.</p> Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def read_json(\n    self, data: str, table: str, read_opts: dict\n) -&gt; duckdb.DuckDBPyRelation:\n    \"\"\"\n    Reads JSON data into a DuckDB table.\n\n    Args:\n        data (str): File path or URL.\n        table (str): Target table name.\n        read_opts (dict): Options passed to DuckDB read_json().\n\n    Returns:\n        duckdb.DuckDBPyRelation: The resulting table.\n    \"\"\"\n    tbl = self.c.read_json(data, **read_opts)\n    try:\n        self.c.sql(f\"INSERT INTO {table} FROM tbl;\")\n    except duckdb.CatalogException:\n        self.c.sql(f\"CREATE TABLE {table} AS FROM tbl;\")\n    return tbl\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.read_parquet","title":"read_parquet","text":"<pre><code>read_parquet(\n    data: str, table: str, read_opts: dict\n) -&gt; DuckDBPyRelation\n</code></pre> <p>Reads Parquet data into a DuckDB table.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>File path or URL.</p> required <code>table</code> <code>str</code> <p>Target table name.</p> required <code>read_opts</code> <code>dict</code> <p>Options passed to DuckDB read_parquet().</p> required <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>duckdb.DuckDBPyRelation: The resulting table.</p> Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def read_parquet(\n    self, data: str, table: str, read_opts: dict\n) -&gt; duckdb.DuckDBPyRelation:\n    \"\"\"\n    Reads Parquet data into a DuckDB table.\n\n    Args:\n        data (str): File path or URL.\n        table (str): Target table name.\n        read_opts (dict): Options passed to DuckDB read_parquet().\n\n    Returns:\n        duckdb.DuckDBPyRelation: The resulting table.\n    \"\"\"\n    tbl = self.c.read_parquet(data, **read_opts)\n    try:\n        self.c.sql(f\"INSERT INTO {table} FROM tbl;\")\n    except duckdb.CatalogException:\n        self.c.sql(f\"CREATE TABLE {table} AS FROM tbl;\")\n    return tbl\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.register","title":"register","text":"<pre><code>register(view_name: str, object)\n</code></pre> <p>Registers a Python object as a DuckDB view.</p> <p>Parameters:</p> Name Type Description Default <code>view_name</code> <code>str</code> <p>View name.</p> required <code>object</code> <p>Python object to register.</p> required Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def register(self, view_name: str, object):\n    \"\"\"\n    Registers a Python object as a DuckDB view.\n\n    Args:\n        view_name (str): View name.\n        object: Python object to register.\n    \"\"\"\n    self.c.register(view_name, object)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.sql","title":"sql","text":"<pre><code>sql(query: str) -&gt; Relation\n</code></pre> <p>Executes a DuckDB SQL query after intercepting remote I/O references.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL string.</p> required <p>Returns:</p> Name Type Description <code>Relation</code> <code>Relation</code> <p>Resulting relation object.</p> Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def sql(self, query: str) -&gt; Relation:\n    \"\"\"\n    Executes a DuckDB SQL query after intercepting remote I/O references.\n\n    Args:\n        query (str): SQL string.\n\n    Returns:\n        Relation: Resulting relation object.\n    \"\"\"\n    self._check_for_unsupported(query)\n    # query = await self._process_io_calls(query)\n    rewriter = DuckDBIORewriter(self.remote, self.fs)\n    query = await rewriter.rewrite(query)\n    return Relation(self.c.sql(query))\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DDB.unregister","title":"unregister","text":"<pre><code>unregister(view_name: str)\n</code></pre> <p>Unregisters a DuckDB view.</p> <p>Parameters:</p> Name Type Description Default <code>view_name</code> <code>str</code> <p>View name to remove.</p> required Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def unregister(self, view_name: str):\n    \"\"\"\n    Unregisters a DuckDB view.\n\n    Args:\n        view_name (str): View name to remove.\n    \"\"\"\n    self.c.unregister(view_name)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DuckDBIORewriter","title":"DuckDBIORewriter","text":"<pre><code>DuckDBIORewriter(\n    remote: RemoteObject, filesystem: FileSystem\n)\n</code></pre> <p>Intercepts and rewrites DuckDB read() calls that reference remote paths.</p> <p>Parameters:</p> Name Type Description Default <code>scope</code> <code>str</code> <p>scope of the current rewrite (stage name)</p> required <code>remote</code> <code>RemoteObject</code> <p>Remote downloader/uploader.</p> required <code>filesystem</code> <code>FileSystem</code> <p>Local filesystem to store fetched content.</p> required Source code in <code>ankaflow/internal/browser.py</code> <pre><code>def __init__(self, remote: \"RemoteObject\", filesystem: FileSystem):\n    \"\"\"\n    Intercepts and rewrites DuckDB read() calls that reference remote paths.\n\n    Args:\n        scope (str): scope of the current rewrite (stage name)\n        remote (RemoteObject): Remote downloader/uploader.\n        filesystem (FileSystem): Local filesystem to store fetched content.\n    \"\"\"\n    self.remote = remote\n    self.fs: FileSystem = filesystem\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.DuckDBIORewriter.rewrite","title":"rewrite","text":"<pre><code>rewrite(query: str) -&gt; str\n</code></pre> <p>Rewrites read_*() calls with remote URIs to local paths.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The SQL query string.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The rewritten query.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>For globs or multi-file remote reads.</p> Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def rewrite(self, query: str) -&gt; str:\n    \"\"\"\n    Rewrites read_*() calls with remote URIs to local paths.\n\n    Args:\n        query (str): The SQL query string.\n\n    Returns:\n        str: The rewritten query.\n\n    Raises:\n        NotImplementedError: For globs or multi-file remote reads.\n    \"\"\"\n    pattern = (\n        r\"read_(parquet|csv|json)(_auto)?\\s*\\(\\s*(\\[.*?\\]|'.*?'|\\\".*?\\\")\"  # noqa:E501\n    )\n    matches = re.findall(pattern, query)\n\n    for _, _, arg_str in matches:\n        paths = self._parse_paths(arg_str)\n\n        for path_str in paths:\n            path = PathFactory.make(path_str)\n\n            if isinstance(path, RemotePath):\n                if path.is_glob:\n                    raise NotImplementedError(\n                        \"Remote globs are not supported.\"\n                    )\n                if len(paths) &gt; 1:\n                    raise NotImplementedError(\n                        \"Multiple remote files are not supported.\"\n                    )\n\n                local_path = await self._rewrite_remote_to_local(path)\n                query = query.replace(f\"'{path_str}'\", f\"'{local_path}'\")\n\n    return query\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.Relation","title":"Relation","text":"<pre><code>Relation(relation: DuckDBPyRelation)\n</code></pre> <p>Wraps a DuckDBPyRelation for async compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>relation</code> <code>DuckDBPyRelation</code> <p>DuckDB relation object.</p> required Source code in <code>ankaflow/internal/browser.py</code> <pre><code>def __init__(self, relation: duckdb.DuckDBPyRelation):\n    \"\"\"\n    Wraps a DuckDBPyRelation for async compatibility.\n\n    Args:\n        relation (duckdb.DuckDBPyRelation): DuckDB relation object.\n    \"\"\"\n    self.rel = relation\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.Relation.arrow","title":"arrow","text":"<pre><code>arrow() -&gt; DataFrame\n</code></pre> <p>Returns the relation as Arrow table.</p> Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def arrow(self) -&gt; DataFrame:\n    \"\"\"Returns the relation as Arrow table.\"\"\"\n    return self.rel.arrow()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.Relation.df","title":"df","text":"<pre><code>df() -&gt; DataFrame\n</code></pre> <p>Returns the relation as a pandas DataFrame.</p> Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def df(self) -&gt; DataFrame:\n    \"\"\"Returns the relation as a pandas DataFrame.\"\"\"\n    return self.rel.df()\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.RemoteObject","title":"RemoteObject","text":"<pre><code>RemoteObject(secrets: dict, fs: FileSystem)\n</code></pre> <p>Initializes a RemoteObject handler.</p> <p>Parameters:</p> Name Type Description Default <code>secrets</code> <code>dict</code> <p>Credentials or connection metadata.</p> required <code>fs</code> <code>FileSystem</code> <p>File system handler to save fetched content.</p> required Source code in <code>ankaflow/internal/browser.py</code> <pre><code>def __init__(self, secrets: dict, fs: FileSystem):\n    \"\"\"\n    Initializes a RemoteObject handler.\n\n    Args:\n        secrets (dict): Credentials or connection metadata.\n        fs (FileSystem): File system handler to save fetched content.\n    \"\"\"\n    self.secrets: t.Dict[str, BucketConfig] = secrets\n    self.fs = fs\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.RemoteObject.fetch","title":"fetch","text":"<pre><code>fetch(remote_path: RemotePath) -&gt; bytes\n</code></pre> <p>Downloads the content at the remote path.</p> <p>Parameters:</p> Name Type Description Default <code>remote_path</code> <code>RemotePath</code> <p>The remote object URI.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>Content fetched from the remote URL.</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If fetch fails due to network or CORS.</p> Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def fetch(self, remote_path: RemotePath) -&gt; bytes:\n    \"\"\"\n    Downloads the content at the remote path.\n\n    Args:\n        remote_path (RemotePath): The remote object URI.\n\n    Returns:\n        bytes: Content fetched from the remote URL.\n\n    Raises:\n        IOError: If fetch fails due to network or CORS.\n    \"\"\"\n    # secrets dict stores anchor for each bucket.\n    secret = self.secrets.get(remote_path.anchor, BucketConfig())\n    url = remote_path.get_endpoint(secret.region)\n    log.debug(url)\n    try:\n        response = await pyfetch(url)\n        if not response.ok:\n            raise IOError(\n                f\"Fetch failed: {response.status} {response.statusText} for {remote_path}\"  # noqa:E501 # type: ignore\n            )\n        return await response.bytes()\n    except Exception as e:\n        raise IOError(\n            f\"Failed fetch - possible CORS or network issue: {remote_path} =&gt; {e}\"  # noqa:E501\n        )\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.browser.RemoteObject.upload","title":"upload","text":"<pre><code>upload(remote_path: RemotePath, local_file: str)\n</code></pre> <p>Uploads a local file to a remote path (stub).</p> <p>Parameters:</p> Name Type Description Default <code>remote_path</code> <code>RemotePath</code> <p>Destination remote URI.</p> required <code>local_file</code> <code>str</code> <p>Path to the local file.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always.</p> Source code in <code>ankaflow/internal/browser.py</code> <pre><code>async def upload(self, remote_path: RemotePath, local_file: str):\n    \"\"\"\n    Uploads a local file to a remote path (stub).\n\n    Args:\n        remote_path (RemotePath): Destination remote URI.\n        local_file (str): Path to the local file.\n\n    Raises:\n        NotImplementedError: Always.\n    \"\"\"\n    log.warning(f\"Upload stub: {local_file} -&gt; {remote_path}\")\n    raise NotImplementedError(\"Upload not implemented\")\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.duckdb","title":"duckdb","text":""},{"location":"api/ankaflow/#ankaflow.internal.duckdb.Relation","title":"Relation","text":"<p>               Bases: <code>ABC</code></p> <p>Relation is an async shim similar to DuckDBPyRelation object</p>"},{"location":"api/ankaflow/#ankaflow.internal.duckdb.Relation.raw","title":"raw","text":"<pre><code>raw() -&gt; DuckDBPyRelation\n</code></pre> <p>Exposes underlying native DuckDBPyRelation with full api.</p> <p>Returns:</p> Type Description <code>DuckDBPyRelation</code> <p>DuckDBPyRelation</p> Source code in <code>ankaflow/internal/duckdb.py</code> <pre><code>@abstractmethod\ndef raw(self) -&gt; DuckDBPyRelation:\n    \"\"\"\n    Exposes underlying native DuckDBPyRelation with full api.\n\n    Returns:\n        DuckDBPyRelation\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.internal.macros","title":"macros","text":""},{"location":"api/ankaflow/#ankaflow.internal.macros.Fn","title":"Fn","text":"<p>Miscellaneous utility and convenience macros (UDFs)</p>"},{"location":"api/ankaflow/#ankaflow.internal.macros.Fn.dt","title":"dt","text":"<pre><code>dt = \"\\n    (a, fail_on_error := FALSE) AS\\n    CASE\\n        -- Case 1: ISO string with optional timezone \u2192 strip and cast\\n        WHEN TRY_CAST(REGEXP_REPLACE(CAST(a AS TEXT), '(Z|[+-][0-9]{2}:[0-9]{2})$', '') AS TIMESTAMP) IS NOT NULL\\n            THEN CAST(REGEXP_REPLACE(CAST(a AS TEXT), '(Z|[+-][0-9]{2}:[0-9]{2})$', '') AS TIMESTAMP)\\n\\n        -- Case 2: Standard timestamp string\\n        WHEN TRY_CAST(a AS TIMESTAMP) IS NOT NULL\\n            THEN CAST(a AS TIMESTAMP)\\n\\n        -- Case 3: ISO-style date\\n        WHEN TRY_CAST(a AS DATE) IS NOT NULL\\n            THEN CAST(a AS TIMESTAMP)\\n\\n        -- Case 4: Unix time in seconds (int or float)\\n        WHEN TRY_CAST(a AS DOUBLE) IS NOT NULL\\n            AND CAST(a AS TEXT) ~ '^[0-9]+(\\\\.[0-9]+)?$'\\n            AND TRY_CAST(CAST(a AS DOUBLE) AS BIGINT) BETWEEN 1000000000 AND 9999999999\\n        THEN make_timestamp(CAST(CAST(a AS DOUBLE) * 1000000 AS BIGINT))  -- seconds \u2192 microseconds\\n\\n\\n        -- Case 5: nanoseconds (length &gt; 15)\\n        WHEN TRY_CAST(a AS BIGINT) IS NOT NULL\\n            AND CAST(a AS TEXT) ~ '^[0-9]+$'\\n            AND LENGTH(CAST(a AS TEXT)) &gt; 15\\n            THEN make_timestamp(CAST(TRY_CAST(a AS BIGINT) / 1000 AS BIGINT))\\n\\n        -- Case 6: milliseconds\\n        WHEN TRY_CAST(a AS BIGINT) IS NOT NULL\\n            AND CAST(a AS TEXT) ~ '^[0-9]+$'\\n            THEN make_timestamp(CAST(TRY_CAST(a AS BIGINT) * 1000 AS BIGINT))\\n\\n        -- Case 7: Explicit fail for other strings\\n        WHEN TYPEOF(a) = 'VARCHAR' AND LENGTH(CAST(a AS TEXT)) &gt; 1 AND fail_on_error = TRUE\\n            THEN CAST('Unsupported format - use Fn.dt(value, pattern)' AS TIMESTAMP)\\n\\n        -- Fallback\\n        ELSE make_timestamp(0)\\n    END,\\n    (value, pattern) AS (\\n            SELECT * FROM query(\\n                concat(\\n                    'SELECT STRPTIME(''',\\n\\n                    -- Strip TZ suffix from value\\n                    REGEXP_REPLACE(value, '(Z|[+-][0-9]{2}:[0-9]{2}|[A-Za-z/_]+)$', ''),\\n\\n                    ''',''',\\n\\n                    -- Auto-detect and convert human-readable patterns\\n                    CASE\\n                        WHEN POSITION('%' IN pattern) &gt; 0 THEN\\n                            REGEXP_REPLACE(REGEXP_REPLACE(pattern, '%z', ''), '%Z', '')\\n                        ELSE\\n                            REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(\\n                                pattern,\\n                                'YYYY', '%Y'),\\n                                'MM', '%m'),\\n                                'DD', '%d'),\\n                                'HH', '%H'),\\n                                'mm', '%M'),\\n                                'ss', '%S')\\n                    END,\\n\\n                    ''')'\\n                )\\n            )\\n        );\\n    \"\n</code></pre> <p>dt macro provides robust datetime parsing and normalization.</p> <p>Overload (a):     Accepts any input value and attempts to convert it to a TIMESTAMP.     - Supports TIMESTAMP_NS, TIMESTAMP, DATE, and BIGINT (UNIX ms).     - Automatically strips time zone suffixes (Z or \u00b1HH:MM) from strings.     - Fails with a descriptive error if input is unrecognized.</p> <p>Overload (value, pattern):     Dynamically parses a string using STRPTIME.     - Strips unsupported time zone suffixes (Z or \u00b1HH:MM) from input.     - Removes %z and %Z from format string, as DuckDB does not interpret timezones.     - Uses query() to compile a literal SQL expression for parsing.</p> <p>Returns:</p> Type Description <p>A DuckDB TIMESTAMP or a hard failure on invalid input.</p>"},{"location":"api/ankaflow/#ankaflow.internal.server","title":"server","text":""},{"location":"api/ankaflow/#ankaflow.internal.server.DDB","title":"DDB","text":"<pre><code>DDB(\n    connection_options: ConnectionConfiguration,\n    persisted: Optional[str] = None,\n)\n</code></pre> Source code in <code>ankaflow/internal/server.py</code> <pre><code>def __init__(\n    self,\n    connection_options: ConnectionConfiguration,\n    persisted: t.Optional[str] = None,\n):\n    self.dbname = persisted or \":memory:\"\n    self.conn_opts = connection_options\n    self._c: duckdb.DuckDBPyConnection | None = None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models","title":"models","text":""},{"location":"api/ankaflow/#ankaflow.models.BigQueryConfig","title":"BigQueryConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for accessing BigQuery datasets.</p>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to service account JSON credentials for BigQuery access.</p>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConfig.dataset","title":"dataset","text":"<pre><code>dataset: Optional[str] = None\n</code></pre> <p>BigQuery dataset name.</p>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConfig.project","title":"project","text":"<pre><code>project: Optional[str] = None\n</code></pre> <p>GCP project ID containing the BigQuery dataset.</p>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConfig.region","title":"region","text":"<pre><code>region: Optional[str] = None\n</code></pre> <p>BigQuery region (e.g., \"us-central1\").</p>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConnection","title":"BigQueryConnection","text":"<p>               Bases: <code>PhysicalConnection</code>, <code>VersionedConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConnection.data_mode","title":"data_mode","text":"<pre><code>data_mode: str = 'error'\n</code></pre> <p>Data mode for write operation. For Deltatable valid options are:</p> <ul> <li><code>append</code> adds new data</li> <li><code>overwrite</code> replaces all data</li> <li><code>error</code> fails (table is read-only)</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConnection.key","title":"key","text":"<pre><code>key: Optional[List[str]] = None\n</code></pre> <p>List of versioned fields</p>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConnection.kind","title":"kind","text":"<pre><code>kind: Literal['BigQuery']\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConnection.partition","title":"partition","text":"<pre><code>partition: Optional[List[str]] = None\n</code></pre> <p>If set then delta table is partitioned using specified fields for faster reads</p>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConnection.schema_mode","title":"schema_mode","text":"<pre><code>schema_mode: Optional[str] = None\n</code></pre> <p>Deltatable schema behaviour. If not set then write fails with error in case the data schema does not match existing table schema.</p> <p>Schema evolution options:</p> <ul> <li><code>merge</code> adds new columns from data</li> <li><code>overwrite</code> adds new columns, drops missing</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.BigQueryConnection.version","title":"version","text":"<pre><code>version: Optional[str] = None\n</code></pre> <p>Field for record version timestamp</p>"},{"location":"api/ankaflow/#ankaflow.models.BucketConfig","title":"BucketConfig","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.BucketConfig.bucket","title":"bucket","text":"<pre><code>bucket: Optional[str] = None\n</code></pre> <p>Bucket name eg. <code>s3://my-bucket</code> or local absolute path eg. <code>/data/path</code></p>"},{"location":"api/ankaflow/#ankaflow.models.BucketConfig.data_prefix","title":"data_prefix","text":"<pre><code>data_prefix: Optional[str] = None\n</code></pre> <p>Prefix for data files: <code>s3://my-bucket/&lt;prefix&gt;</code> or <code>/data/path/&lt;prefix&gt;</code></p>"},{"location":"api/ankaflow/#ankaflow.models.BucketConfig.locator_wildcard","title":"locator_wildcard","text":"<pre><code>locator_wildcard: Optional[Tuple] = None\n</code></pre> <p>Regular expression and wildcard to modify <code>locator</code>. Useful in cases when you sink data to <code>data-YYYY.parquet</code> but want to read <code>data-*.parquet</code>. Provide tuple with patten as first element and replacement as second one. Example:</p> <pre><code>('-\\d{4}-', '*')\n</code></pre> <p>This will create wildcard for <code>data-YYYY-base.parquet</code> as <code>data*base.parquet</code>.</p> Wildcard Description <code>*</code> matches any number of any characters (including none) <code>**</code> matches any number of subdirectories (including none) <code>[abc]</code> matches one character given in the bracket <code>[a-z]</code> matches one character from the range given in the bracket <p>Wildcard is automatically applied in tap and show_schema operations.</p>"},{"location":"api/ankaflow/#ankaflow.models.BucketConfig.region","title":"region","text":"<pre><code>region: str | None = None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.CSVConnection","title":"CSVConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.ClickhouseConfig","title":"ClickhouseConfig","text":"<p>               Bases: <code>DatabaseConfig</code></p> <p>Configuration for ClickHouse database, extending generic SQL config.</p>"},{"location":"api/ankaflow/#ankaflow.models.ClickhouseConfig.blocksize","title":"blocksize","text":"<pre><code>blocksize: int = 50000\n</code></pre> <p>Number of rows to process per block for batch operations.</p>"},{"location":"api/ankaflow/#ankaflow.models.ClickhouseConnection","title":"ClickhouseConnection","text":"<p>               Bases: <code>PhysicalConnection</code>, <code>VersionedConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.ClickhouseConnection.key","title":"key","text":"<pre><code>key: Optional[List[str]] = None\n</code></pre> <p>List of versioned fields</p>"},{"location":"api/ankaflow/#ankaflow.models.ClickhouseConnection.kind","title":"kind","text":"<pre><code>kind: Literal['Clickhouse']\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.ClickhouseConnection.version","title":"version","text":"<pre><code>version: Optional[str] = None\n</code></pre> <p>Field for record version timestamp</p>"},{"location":"api/ankaflow/#ankaflow.models.Column","title":"Column","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data column (equivalent to database column)</p>"},{"location":"api/ankaflow/#ankaflow.models.Column.name","title":"name","text":"<pre><code>name: str\n</code></pre> <p>Column name must follow rules set to SQL engine column names</p>"},{"location":"api/ankaflow/#ankaflow.models.Column.type","title":"type","text":"<pre><code>type: str\n</code></pre> <p>Any data type support by SQL engine</p>"},{"location":"api/ankaflow/#ankaflow.models.Columns","title":"Columns","text":"<p>               Bases: <code>RootModel[List[Column]]</code></p> <p>Iterable list-like collection of Fields.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection","title":"Connection","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.config","title":"config","text":"<pre><code>config: Optional[ConnectionConfiguration] = None\n</code></pre> <p>Optional configuration for the current connection. If not present then global configuration will be used.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.create_statement","title":"create_statement","text":"<pre><code>create_statement: str | None = None\n</code></pre> <p>Create statement for given table. Must be in requested dialect.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.fields","title":"fields","text":"<pre><code>fields: Optional[Columns] = None\n</code></pre> <p>If set then schema is used to generate source structure in case actual source does not provide data in which case generation of ephemeral view fails.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.kind","title":"kind","text":"<pre><code>kind: str\n</code></pre> <p>Model type e.g. Deltatable, Bigquery, Clickhouse, Parquet, File Custom connections can be loaded from module.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.params","title":"params","text":"<pre><code>params: Optional[dict] = Field(default_factory=dict)\n</code></pre> <p>Any parameters that can be passed to connection.</p>"},{"location":"api/ankaflow/#ankaflow.models.Connection.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If true then schema is automatically detected from the input data and logged '</p>"},{"location":"api/ankaflow/#ankaflow.models.ConnectionConfiguration","title":"ConnectionConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level container for all connection configurations in a pipeline.</p> <p>Includes default configuration blocks for supported sources and sinks like: - Local file systems - S3 and GCS buckets - BigQuery datasets - ClickHouse databases</p> <p>These can be customized per pipeline or extended with new sources.</p> Example <p>To support MySQL, you can define a new model like this:</p> <pre><code>class MySQLConfig(DatabaseConfig):\n    pass  # You can add MySQL-specific fields here if needed\n\nclass ExtendedConnectionConfiguration(ConnectionConfiguration):\n    mysql: MySQLConfig = PydanticField(default_factory=MySQLConfig)\n</code></pre> <p>This will allow you to specify MySQL connection settings in your YAML:</p> <pre><code>connection:\n  kind: MySQL\n  config:\n    host: localhost\n    port: 3306\n    database: sales\n    username: admin\n    password: secret\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.ConnectionConfiguration.bigquery","title":"bigquery","text":"<pre><code>bigquery: BigQueryConfig = Field(\n    default_factory=BigQueryConfig\n)\n</code></pre> <p>BigQuery connection configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.ConnectionConfiguration.clickhouse","title":"clickhouse","text":"<pre><code>clickhouse: ClickhouseConfig = Field(\n    default_factory=ClickhouseConfig\n)\n</code></pre> <p>Language model configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.ConnectionConfiguration.gs","title":"gs","text":"<pre><code>gs: GSConfig = Field(default_factory=GSConfig)\n</code></pre> <p>Google Cloud Storage configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.ConnectionConfiguration.llm","title":"llm","text":"<pre><code>llm: LLMConfig = Field(default_factory=LLMConfig)\n</code></pre> <p>Language model configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.ConnectionConfiguration.local","title":"local","text":"<pre><code>local: BucketConfig = Field(default_factory=BucketConfig)\n</code></pre> <p>Local file system configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.ConnectionConfiguration.s3","title":"s3","text":"<pre><code>s3: S3Config = Field(default_factory=S3Config)\n</code></pre> <p>S3 cloud storage configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.CustomConnection","title":"CustomConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p> <p>Custom connection provider. Custom connection may implement its own logic but must derive from base Connection class, and expose tap(), sink(), sql() and show_schema() even if they are no-op.</p>"},{"location":"api/ankaflow/#ankaflow.models.CustomConnection.classname","title":"classname","text":"<pre><code>classname: str\n</code></pre> <p>Name of the class to load from the module</p>"},{"location":"api/ankaflow/#ankaflow.models.CustomConnection.module","title":"module","text":"<pre><code>module: str\n</code></pre> <p>Python module where the connection class is defined</p>"},{"location":"api/ankaflow/#ankaflow.models.CustomConnection.params","title":"params","text":"<pre><code>params: Optional[dict] = Field(default_factory=dict)\n</code></pre> <p>Free-form configuration parameters passed to the loaded class</p>"},{"location":"api/ankaflow/#ankaflow.models.CustomConnection.load","title":"load","text":"<pre><code>load() -&gt; Type[Connection]\n</code></pre> <p>Dynamically load the connection class from the given module and member.</p> <p>Returns:</p> Type Description <code>Type[Connection]</code> <p>A subclass of Connection.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module or member can't be imported.</p> Source code in <code>ankaflow/models/connections.py</code> <pre><code>def load(self) -&gt; t.Type[Connection]:\n    \"\"\"\n    Dynamically load the connection class from the given module and member.\n\n    Returns:\n        A subclass of Connection.\n\n    Raises:\n        ImportError: If the module or member can't be imported.\n    \"\"\"\n    mod = importlib.import_module(self.module)\n    try:\n        cls = getattr(mod, self.classname)\n    except AttributeError as e:\n        raise ImportError(\n            f\"Could not load '{self.classname}' from module '{self.module}'\"\n        ) from e\n    # TODO: Refactor imports to facilitate this check early\n    # if not issubclass(cls, Connection):\n    #     raise TypeError(\n    #         f\"{cls.__name__} is not a subclass of Connection\"\n    #     )\n\n    return cls\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.DeltatableConnection","title":"DeltatableConnection","text":"<p>               Bases: <code>PhysicalConnection</code>, <code>VersionedConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.DeltatableConnection.data_mode","title":"data_mode","text":"<pre><code>data_mode: str = 'error'\n</code></pre> <p>Data mode for write operation. For Deltatable valid options are:</p> <ul> <li><code>append</code> adds new data</li> <li><code>overwrite</code> replaces all data</li> <li><code>error</code> fails (table is read-only)</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.DeltatableConnection.kind","title":"kind","text":"<pre><code>kind: Literal['Deltatable']\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.DeltatableConnection.optimize","title":"optimize","text":"<pre><code>optimize: Optional[Union[str, int]] = 1\n</code></pre> <p>Use with Deltatable and other engines whether to optimize after each sink operation. With larger tables this may be a lengthy synchronous operation.</p> <p>Default value is optimize and vacuum with 7 day retention.</p>"},{"location":"api/ankaflow/#ankaflow.models.DeltatableConnection.optimize--deltatable","title":"Deltatable","text":"<p>Values are <code>optimize,vacuum,all,Literal[int]</code>.   If value is literal int is provided then parts older than   number of days will be removed. Note that this will override   default retention days.</p> <p>String options <code>vacuum,all</code> are equivalent to 0.</p> <p>Delta connection also supports raw SQL optimization. Pass statement as sql command:</p> <p><code>OPTIMIZE Deltatable [COMPACT] [VACUUM] [AGE=&lt;int&gt;[d|h]] [DRY_RUN] [CLEANUP]</code></p> <p><code>optimize deltatable</code> \u2192 compact + vacuum with default 7 days</p> <p><code>optimize deltatable compact</code> \u2192 compact only</p> <p><code>optimize deltatable vacuum age=36h dry_run</code> \u2192 list files older than 36 hours, don't delete</p> <p><code>optimize deltatable compact vacuum age=1 cleanup</code> \u2192 compact, vacuum 1 day, then cleanup metadata</p>"},{"location":"api/ankaflow/#ankaflow.models.DeltatableConnection.partition","title":"partition","text":"<pre><code>partition: Optional[List[str]] = None\n</code></pre> <p>If set then delta table is partitioned using specified fields for faster reads</p>"},{"location":"api/ankaflow/#ankaflow.models.DeltatableConnection.schema_mode","title":"schema_mode","text":"<pre><code>schema_mode: Optional[str] = None\n</code></pre> <p>Deltatable schema behaviour. If not set then write fails with error in case the data schema does not match existing table schema.</p> <p>Schema evolution options:</p> <ul> <li><code>merge</code> adds new columns from data</li> <li><code>overwrite</code> adds new columns, drops missing</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.DeltatableConnection.writer_features","title":"writer_features","text":"<pre><code>writer_features: Optional[List] | None = None\n</code></pre> <p>Any supported Delta-rs parameters can be passed to the writer. Example:</p> <pre><code>writer_features: [TimestampWithoutTimezone]\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.Dimension","title":"Dimension","text":"<p>               Bases: <code>EphemeralConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Dimension.load","title":"load","text":"<pre><code>load() -&gt; Type[Connection]\n</code></pre> <p>Dynamically load the connection class from the given module and member.</p> <p>Returns:</p> Type Description <code>Type[Connection]</code> <p>A subclass of Connection.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module or member can't be imported.</p> Source code in <code>ankaflow/models/connections.py</code> <pre><code>def load(self) -&gt; t.Type[Connection]:\n    \"\"\"\n    Dynamically load the connection class from the given module and member.\n\n    Returns:\n        A subclass of Connection.\n\n    Raises:\n        ImportError: If the module or member can't be imported.\n    \"\"\"\n    mod = importlib.import_module(self.module)\n    try:\n        cls = getattr(mod, self.kind)\n    except AttributeError as e:\n        raise ImportError(\n            f\"Could not load '{self.kind}' from module '{self.module}'\"\n        ) from e\n\n    return cls\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.FileConnection","title":"FileConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.FlowContext","title":"FlowContext","text":"<p>               Bases: <code>ImmutableMap</code></p> <p>Context dictionary can be used to supply arbitrary data to pipeline that can be referenced in templates much much like variables with the difference that they cannot be modified at runtime.</p> <p>Context can be initiated via dictionary:</p> <p><code>context = FlowContext(**{'foo':'Bar'}])</code></p> <p>Items can be reference using both bracket and dot notation:</p> <p><code>context.foo == context['foo'] == 'Bar'</code></p>"},{"location":"api/ankaflow/#ankaflow.models.GSConfig","title":"GSConfig","text":"<p>               Bases: <code>BucketConfig</code></p> <p>Google Cloud Storage (GCS) configuration.</p> <ul> <li>HMAC credentials allow reading/writing Parquet, JSON, and CSV.</li> <li>Delta table writes require a full service account credential file.</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.GSConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to GCP service account credential file (JSON). Required for certain write operations (e.g., Delta tables).</p>"},{"location":"api/ankaflow/#ankaflow.models.GSConfig.hmac_key","title":"hmac_key","text":"<pre><code>hmac_key: Optional[str] = None\n</code></pre> <p>GCS HMAC key ID for authenticated access.</p>"},{"location":"api/ankaflow/#ankaflow.models.GSConfig.hmac_secret","title":"hmac_secret","text":"<pre><code>hmac_secret: Optional[str] = None\n</code></pre> <p>GCS HMAC secret for authenticated access.</p>"},{"location":"api/ankaflow/#ankaflow.models.JSONConnection","title":"JSONConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.ParquetConnection","title":"ParquetConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.PhysicalConnection","title":"PhysicalConnection","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.PhysicalConnection.locator","title":"locator","text":"<pre><code>locator: str\n</code></pre> <p>Table name or file name or URI, or other identifier required by the connection.</p>"},{"location":"api/ankaflow/#ankaflow.models.PhysicalConnection.raw_dispatch","title":"raw_dispatch","text":"<pre><code>raw_dispatch: bool | None = None\n</code></pre> <p>If True, sends the provided SQL query directly without altering locators if fully qualified, or adding FROM clauses. The query must be a valid SELECT statement. Useful for full control over complex queries.</p> <p>In Deltatable and Parquet connections supports rewriting short locators as convenience.</p>"},{"location":"api/ankaflow/#ankaflow.models.RestConnection","title":"RestConnection","text":"<p>               Bases: <code>EphemeralConnection</code></p> <p>Configuration for a REST-based data connection.</p> <p>This model defines how to configure and execute REST API requests as part of a pipeline step. It includes the request definition, client behavior (e.g., retries, headers), and optional schema discovery.</p>"},{"location":"api/ankaflow/#ankaflow.models.RestConnection.client","title":"client","text":"<pre><code>client: RestClientConfig\n</code></pre> <p>Configuration for the REST client (e.g., base URL, headers, auth).</p>"},{"location":"api/ankaflow/#ankaflow.models.RestConnection.fields","title":"fields","text":"<pre><code>fields: Optional[Columns] = None\n</code></pre> <p>Optional schema definition used to validate or transform response data.</p> <p>It is recommended to manually specify the schema after initial discovery. This ensures downstream pipeline stages remain stable, even when the remote API returns no results (e.g., due to no updates in an incremental fetch). Explicit schema prevents silent failures or schema drift in such cases.</p>"},{"location":"api/ankaflow/#ankaflow.models.RestConnection.kind","title":"kind","text":"<pre><code>kind: Literal['Rest']\n</code></pre> <p>Specifies the connection type as \"Rest\".</p>"},{"location":"api/ankaflow/#ankaflow.models.RestConnection.request","title":"request","text":"<pre><code>request: Request\n</code></pre> <p>Request template specifying method, path, body, etc.</p>"},{"location":"api/ankaflow/#ankaflow.models.RestConnection.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If True, the connector will attempt to infer or display the response schema automatically.</p>"},{"location":"api/ankaflow/#ankaflow.models.S3Config","title":"S3Config","text":"<p>               Bases: <code>BucketConfig</code></p> <p>S3-specific bucket configuration including optional authentication.</p>"},{"location":"api/ankaflow/#ankaflow.models.S3Config.access_key_id","title":"access_key_id","text":"<pre><code>access_key_id: Optional[str] = None\n</code></pre> <p>AWS access key ID for private S3 access.</p>"},{"location":"api/ankaflow/#ankaflow.models.S3Config.secret_access_key","title":"secret_access_key","text":"<pre><code>secret_access_key: Optional[str] = None\n</code></pre> <p>AWS secret access key for private S3 access.</p>"},{"location":"api/ankaflow/#ankaflow.models.SQLGenConnection","title":"SQLGenConnection","text":"<p>               Bases: <code>EphemeralConnection</code></p> <p>SQLGen connection is intended for SQL code generation:</p> <p>Prompt should instruct the model to generate SQL query and and executes resutling a VIEW in the internal database.</p> <p>Example:</p> <pre><code>Stage 1: Name: ReadSomeParquetData\n\nStage 2: Name: CodeGen, query: Given SQL table `ReadSomeParquetData` generate SQL query to\n    count number of rows in the table.\n\nInside stage 2 the following happens:\n\n1. Prompt is sent to inferecne endpoint\n\n2. Endpoint is expected to respond with valid SQL\n\n3. Connection will execute a statement `CREATE OR REPLACE VIEW StageName AS &lt;received_select_statement&gt;`\n    where statement in the exmaple is likely `SELECT COUNT() FROM ReadSomeParquetData`\n</code></pre> <p>.</p>"},{"location":"api/ankaflow/#ankaflow.models.SQLGenConnection.kind","title":"kind","text":"<pre><code>kind: Literal['SQLGen']\n</code></pre> <p>Specifies the <code>kind==SQLGen</code></p>"},{"location":"api/ankaflow/#ankaflow.models.SQLGenConnection.variables","title":"variables","text":"<pre><code>variables: dict | None = None\n</code></pre> <p>Variables passed to Prompt. Prompt must be supplied in the <code>query</code> field of the <code>Stage</code>.</p> <p>Prompt may contain Jinja2-style placeholders:</p> Example <p><code>Here's my name: {{name}}.</code></p> <p>The connection will render the prompt template using <code>variables</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Stage","title":"Stage","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.connection","title":"connection","text":"<pre><code>connection: Optional[\n    Union[\n        RestConnection,\n        VariableConnection,\n        BigQueryConnection,\n        DeltatableConnection,\n        ParquetConnection,\n        ClickhouseConnection,\n        CustomConnection,\n        JSONConnection,\n        CSVConnection,\n        FileConnection,\n        SQLGenConnection,\n        Dimension,\n    ]\n] = Field(None, discriminator=\"kind\")\n</code></pre> <p>Defines how the data is read from / written to the target.</p> <p>Connection fields may contain templates and they will be recursively.</p> <p>Special construct is JSON&gt; which allows dynamically generating parameters as runtime:</p> <pre><code>- kind: source\n  name: source_name\n  connection:\n    kind: File\n    params: &gt;\n      JSON&gt;{\n        \"key\": \"value\",\n        \"dynkey\": &lt;&lt;API.property&gt;&gt;,\n      }\n</code></pre> <p>In the above app <code>params</code> are constructed as JSON string. It is possible to even construct parameter keys dynamically:</p> <pre><code>params: &gt;\n  JSON&gt;\n  {\n    &lt;% for number in [1,2,3] %&gt;\n    \"key_&lt;&lt; number &gt;&gt;\":&lt;&lt; number &gt;&gt;&lt;% if not loop.last %&gt;,&lt;% endif %&gt;\n    &lt;% endfor %&gt;\n  }\n</code></pre> <p>Above example results the following:</p> <pre><code>params: &gt;\n  {\n    \"key_1\": 1,\n    \"key_2\": 2,\n    \"key_3\": 3\n  }\n</code></pre> <p>JSON&gt; structure cannot contain nested JSON&gt; structures, the entire string following the JSON&gt; header must result a valid JSON.</p> <p>Inbuilt connections include:</p> <ul> <li>Deltatable (read)</li> </ul> <p>If connection is <code>Deltatable</code> then query is required to narrow   down data stored in the delta table. <code>FROM</code> clause must be <code>Deltatable</code>: <code>- kind: source     name: delta_tap     connection:       kind: Deltatable       locator: delta_table     query: &gt;       select * from Deltatable</code></p> <ul> <li>Deltatable (write)</li> </ul> <p>See also StorageOptions</p> <p>The following example writes data from preceding stage   to delta table, appending the data, partitions using   <code>part</code> column, and optimizes and vacuums immediately without   retention after write.   <code>- kind: sink     name: delta_sink     connection:       kind: Deltatable       locator: delta_table       optimize: 0       data_mode: append       partition:       - part_field</code></p> <ul> <li>Parquet (read/write)</li> <li>JSON (read/write; NOTE: write operation generates newline-delimited JSON)</li> <li>CSV (read/write)</li> <li> <p>Variable (read/write)</p> </li> <li> <p>File (read)</p> </li> </ul> <p>File can be read from a connected filesystem (including s3). File name   and file type must be specified in the pipeline context:</p> <pre><code>- `context.FileName`: file name relative to `file_prefix`\n- `context.FileType`: file type, one of CSV, XLSX, JSON, HTML\n</code></pre> <p>Any file reader configuration must be passed in <code>params</code>.</p> <ul> <li>Rest (bidirectional)</li> </ul> <p>Rest connections consist of two parts: Client and Request. Client contains   base URL and authentication (basic, digest, header and oauth2 are supported).</p> <p><code>- name: TheRest     kind: source     connection:       kind: Rest       client:         base_url: https://api.example.com         auth:  # Every request to given endpoint share the same authentication           method: basic           values:             username: TheName             password: ThePassword       request:         endpoint: /v1/some-path         content_type: application/json         method: post         query:  # Query parameters           date: &lt;&lt;API.dt(None).isoformat()&gt;&gt;         body:  # JSON payload           param: 1         response:           content_type: application/json           locator: \"JMESPath.to.data\"</code></p> <p>Any custom source (API) can be used as long as available via module.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.context","title":"context","text":"<pre><code>context: Optional[FlowContext] = None\n</code></pre> <p>@private Global context passed to given stage</p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.explain","title":"explain","text":"<pre><code>explain: Optional[bool] = None\n</code></pre> <p>If set to true then SQL query explanation will be logged.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.fields","title":"fields","text":"<pre><code>fields: Optional[List[Column]] = None\n</code></pre> <p>Explicitly defined output fields.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.kind","title":"kind","text":"<pre><code>kind: str\n</code></pre> <p>Defines which action will be performed:</p> <ul> <li>source</li> <li>transform</li> <li>sink</li> <li>pipeline</li> <li>sql</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.Stage.kind--sink","title":"Sink","text":"<p>Sink reads output from previous stage and stores in specified location.</p> <p>NOTE: If query is supplied with the stage then sink uses output of the query rather than preceding stage directly. Subsequent sink will use preceding stage. If supplied query must create either view or table with same name as current stage.</p> <pre><code>- name: my_sink\nkind: sink\nconnection:\n    kind: Variable\nquery: &gt;\n    CREATE VIEW my_sink AS\n    SELECT\n    1 as foo\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.Stage.kind--pipeline","title":"Pipeline","text":"<p>If pipeline is preceded by any stage then the subpipeline will be executed as many times as there are rows in the previous stage output. This is useful if you want to run same pipeline with different parameters. Make sure the pipeline is preceded by source or transform producing required number of rows. If you need to run subpipeline only once there are two options:</p> <ol> <li>Place it to the top</li> <li>Preced with tranform producing single row only</li> </ol> <p>Each row is then passed to subpipeline in a special variable.</p> <p>Example pipeline iterating subpipeline 5 times:</p> <ul> <li>kind: transform       name: generate_5       # Generate 5 rows       query: &gt;           select unnest as current_variable from unnest(generate_series(1,5))       show: 5</li> <li>kind: pipeline     name: looped_5x     stages:         - kind: transform           name: inside_loop           # In query we can reference the value passed from parent pipeline           query: &gt;               select 'Currently running iteration: {API.look('loop_control.current_variable', variables)}' as value           show: 5</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.Stage.locator","title":"locator","text":"<pre><code>locator: Optional[str] = None\n</code></pre> <p>Currently unused: Name for the connection configuration: name, or URI.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.log_level","title":"log_level","text":"<pre><code>log_level: Optional[LogLevel] = None\n</code></pre> <p>Set logging level. All stages after (including current) will log with specified level. Possible values: INFO (default), DEBUG, WARNING. Log level will be reset to INFO after each pipeline (including nested pipelines).</p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.name","title":"name","text":"<pre><code>name: str\n</code></pre> <p>Name of the stage, must be unique across all stages in the pipeline and conform to the rules: Must start with letter, may contain lowercase letters, number and underscores. Name is used to reference this stage by other subsequent stages.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.on_error","title":"on_error","text":"<pre><code>on_error: str = 'fail'\n</code></pre> <p>If set to 'continue' then pipeline will not fail. Subsequent stages referring to failed one must handle missing data.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.query","title":"query","text":"<pre><code>query: Optional[str] = None\n</code></pre> <p>SQL Query or dictionary with custom source parameters. May contain {dynamic variables}.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.show","title":"show","text":"<pre><code>show: int = 0\n</code></pre> <p>If set to positive integer then given number of rows from this stage will get logged. If set to -1 then all rows will be loggged. Set to 0 to disable logging.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If True then schema is logged</p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.skip_if","title":"skip_if","text":"<pre><code>skip_if: Optional[Any] = None\n</code></pre> <p>Any value that can evaluated using bool(). or template string e.g. <code>&lt;&lt; True &gt;&gt;</code>. When the expression evaluates to True then the stage is skipped.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.stages","title":"stages","text":"<pre><code>stages: Optional[Stages] = None\n</code></pre> <p>Used when kind is <code>Flow</code></p>"},{"location":"api/ankaflow/#ankaflow.models.Stage.throttle","title":"throttle","text":"<pre><code>throttle: Optional[Union[int, float]] = None\n</code></pre> <p>If set to positive value then flow execution will be paused after the stage for given number of seconds.</p> <p>Useful when dealing with rate limits or otherwise spreading the load over time.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stages","title":"Stages","text":"<p>               Bases: <code>RootModel[List[Stage]]</code></p> <p>A sequence of processing stages in a data pipeline.</p> <p>Each <code>Datablock</code> in <code>root</code> represents one discrete step or transformation in an end-to-end business workflow (e.g., tap, transform, sink, validate).</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>List[Datablock]</code> <p>Ordered list of pipeline stages to execute.</p>"},{"location":"api/ankaflow/#ankaflow.models.Stages.enumerate_steps","title":"enumerate_steps","text":"<pre><code>enumerate_steps() -&gt; Iterator[tuple[int, Stage]]\n</code></pre> <p>Yield each stage along with its 0-based position.</p> <p>Use this when you need both the stage and its index for logging, metrics, or conditional branching.</p> <p>Returns:</p> Type Description <code>Iterator[tuple[int, Stage]]</code> <p>Iterator[Tuple[int, Datablock]]: Pairs of (index, stage).</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>def enumerate_steps(self) -&gt; t.Iterator[tuple[int, Stage]]:\n    \"\"\"Yield each stage along with its 0-based position.\n\n    Use this when you need both the stage and its index for logging,\n    metrics, or conditional branching.\n\n    Returns:\n        Iterator[Tuple[int, Datablock]]: Pairs of (index, stage).\n    \"\"\"\n    return enumerate(self.root)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.Stages.load","title":"load","text":"<pre><code>load(source: Union[str, Path, IO[str], Loadable]) -&gt; Stages\n</code></pre> <p>Load a pipeline from YAML (path, YAML-string, file-like or Loadable).</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | Path | IO[str] | Loadable</code> <ul> <li>Path to a .yaml file</li> <li>Raw YAML content</li> <li>File-like object returning YAML</li> <li>Any object with a <code>.load()</code> method returning Python data</li> </ul> required <p>Returns:</p> Name Type Description <code>Stages</code> <code>Stages</code> <p>a validated <code>Stages</code> instance.</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    source: t.Union[str, Path, t.IO[str], Loadable],\n) -&gt; \"Stages\":\n    \"\"\"Load a pipeline from YAML (path, YAML-string, file-like or Loadable).\n\n    Args:\n        source (str | Path | IO[str] | Loadable):\n            - Path to a .yaml file\n            - Raw YAML content\n            - File-like object returning YAML\n            - Any object with a `.load()` method returning Python data\n\n    Returns:\n        Stages: a validated `Stages` instance.\n    \"\"\"\n    # 1) If it\u2019s a loader-object, pull Python data directly\n    if isinstance(source, Loadable):\n        data = source.load()\n\n    else:\n        # 2) Read text for YAML parsing:\n        if hasattr(source, \"read\"):\n            text = t.cast(t.IO[str], source).read()\n        else:\n            text = str(source)\n\n        # 3) First, try parsing as raw YAML\n        try:\n            data = yaml.safe_load(text)\n        except yaml.YAMLError:\n            data = None\n\n        # 4) Only if that parse returned a `str` we treat it as a filename\n        if isinstance(data, str):\n            try:\n                text = Path(data).read_text()\n                data = yaml.safe_load(text)\n            except (OSError, yaml.YAMLError) as e:\n                raise ValueError(\n                    f\"Could not interpret {data!r} as YAML or file path\"\n                ) from e\n\n    # 5) Validate final shape\n    if not isinstance(data, list):\n        raise ValueError(\n            f\"Expected a list of pipeline stages, got {type(data).__name__}\"\n        )\n\n    # 5) Finally, validate into our model\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.Stages.steps","title":"steps","text":"<pre><code>steps() -&gt; Iterator[Stage]\n</code></pre> <p>Yield each stage in execution order.</p> <p>Returns:</p> Type Description <code>Iterator[Stage]</code> <p>Iterator[Datablock]: An iterator over the stages,</p> <code>Iterator[Stage]</code> <p>from first to last.</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>def steps(self) -&gt; t.Iterator[Stage]:\n    \"\"\"Yield each stage in execution order.\n\n    Returns:\n        Iterator[Datablock]: An iterator over the stages,\n        from first to last.\n    \"\"\"\n    return iter(self.root)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.Variables","title":"Variables","text":"<p>               Bases: <code>dict</code></p> <p>Variables is a <code>dict</code>-based collection storing arbitrary data under keys. Variable can be populated via:</p> <ul> <li>Dictionary passed to pipeline:   <code>Flow(defs, context, variables={'foo': 'bar'})</code></li> <li>Sink operation:<pre><code>- kind: sink\n\n    name: my_sink\n\n    connection:\n\n        kind: Variable\n\n        locator: variable_name\n</code></pre> </li> </ul> <p>This step will place a list of records from preceding step in   variables as <code>{'variable_name': t.List[dict]}</code></p> <p>In most cases sainking to variable is not needed as all preceding stages in a pipeline can be referenced via <code>name</code>. Sinking to variable is useful when you need to share data with subpipeline.</p> <p>Special variable is <code>loop_control</code> which is populated dynamically from previous step and current stage type is pipeline. In case previous step generates multiple records then the new pipeline is called for each record, and control variable holds a <code>dict</code> representing the current record.</p>"},{"location":"api/ankaflow/#ankaflow.models.components","title":"components","text":""},{"location":"api/ankaflow/#ankaflow.models.components.Column","title":"Column","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data column (equivalent to database column)</p>"},{"location":"api/ankaflow/#ankaflow.models.components.Column.name","title":"name","text":"<pre><code>name: str\n</code></pre> <p>Column name must follow rules set to SQL engine column names</p>"},{"location":"api/ankaflow/#ankaflow.models.components.Column.type","title":"type","text":"<pre><code>type: str\n</code></pre> <p>Any data type support by SQL engine</p>"},{"location":"api/ankaflow/#ankaflow.models.components.Columns","title":"Columns","text":"<p>               Bases: <code>RootModel[List[Column]]</code></p> <p>Iterable list-like collection of Fields.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs","title":"configs","text":""},{"location":"api/ankaflow/#ankaflow.models.configs.BigQueryConfig","title":"BigQueryConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for accessing BigQuery datasets.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.BigQueryConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to service account JSON credentials for BigQuery access.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.BigQueryConfig.dataset","title":"dataset","text":"<pre><code>dataset: Optional[str] = None\n</code></pre> <p>BigQuery dataset name.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.BigQueryConfig.project","title":"project","text":"<pre><code>project: Optional[str] = None\n</code></pre> <p>GCP project ID containing the BigQuery dataset.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.BigQueryConfig.region","title":"region","text":"<pre><code>region: Optional[str] = None\n</code></pre> <p>BigQuery region (e.g., \"us-central1\").</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.BucketConfig","title":"BucketConfig","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.configs.BucketConfig.bucket","title":"bucket","text":"<pre><code>bucket: Optional[str] = None\n</code></pre> <p>Bucket name eg. <code>s3://my-bucket</code> or local absolute path eg. <code>/data/path</code></p>"},{"location":"api/ankaflow/#ankaflow.models.configs.BucketConfig.data_prefix","title":"data_prefix","text":"<pre><code>data_prefix: Optional[str] = None\n</code></pre> <p>Prefix for data files: <code>s3://my-bucket/&lt;prefix&gt;</code> or <code>/data/path/&lt;prefix&gt;</code></p>"},{"location":"api/ankaflow/#ankaflow.models.configs.BucketConfig.locator_wildcard","title":"locator_wildcard","text":"<pre><code>locator_wildcard: Optional[Tuple] = None\n</code></pre> <p>Regular expression and wildcard to modify <code>locator</code>. Useful in cases when you sink data to <code>data-YYYY.parquet</code> but want to read <code>data-*.parquet</code>. Provide tuple with patten as first element and replacement as second one. Example:</p> <pre><code>('-\\d{4}-', '*')\n</code></pre> <p>This will create wildcard for <code>data-YYYY-base.parquet</code> as <code>data*base.parquet</code>.</p> Wildcard Description <code>*</code> matches any number of any characters (including none) <code>**</code> matches any number of subdirectories (including none) <code>[abc]</code> matches one character given in the bracket <code>[a-z]</code> matches one character from the range given in the bracket <p>Wildcard is automatically applied in tap and show_schema operations.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.BucketConfig.region","title":"region","text":"<pre><code>region: str | None = None\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.configs.ClickhouseConfig","title":"ClickhouseConfig","text":"<p>               Bases: <code>DatabaseConfig</code></p> <p>Configuration for ClickHouse database, extending generic SQL config.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.ClickhouseConfig.blocksize","title":"blocksize","text":"<pre><code>blocksize: int = 50000\n</code></pre> <p>Number of rows to process per block for batch operations.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.ConnectionConfiguration","title":"ConnectionConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level container for all connection configurations in a pipeline.</p> <p>Includes default configuration blocks for supported sources and sinks like: - Local file systems - S3 and GCS buckets - BigQuery datasets - ClickHouse databases</p> <p>These can be customized per pipeline or extended with new sources.</p> Example <p>To support MySQL, you can define a new model like this:</p> <pre><code>class MySQLConfig(DatabaseConfig):\n    pass  # You can add MySQL-specific fields here if needed\n\nclass ExtendedConnectionConfiguration(ConnectionConfiguration):\n    mysql: MySQLConfig = PydanticField(default_factory=MySQLConfig)\n</code></pre> <p>This will allow you to specify MySQL connection settings in your YAML:</p> <pre><code>connection:\n  kind: MySQL\n  config:\n    host: localhost\n    port: 3306\n    database: sales\n    username: admin\n    password: secret\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.configs.ConnectionConfiguration.bigquery","title":"bigquery","text":"<pre><code>bigquery: BigQueryConfig = Field(\n    default_factory=BigQueryConfig\n)\n</code></pre> <p>BigQuery connection configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.ConnectionConfiguration.clickhouse","title":"clickhouse","text":"<pre><code>clickhouse: ClickhouseConfig = Field(\n    default_factory=ClickhouseConfig\n)\n</code></pre> <p>Language model configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.ConnectionConfiguration.gs","title":"gs","text":"<pre><code>gs: GSConfig = Field(default_factory=GSConfig)\n</code></pre> <p>Google Cloud Storage configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.ConnectionConfiguration.llm","title":"llm","text":"<pre><code>llm: LLMConfig = Field(default_factory=LLMConfig)\n</code></pre> <p>Language model configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.ConnectionConfiguration.local","title":"local","text":"<pre><code>local: BucketConfig = Field(default_factory=BucketConfig)\n</code></pre> <p>Local file system configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.ConnectionConfiguration.s3","title":"s3","text":"<pre><code>s3: S3Config = Field(default_factory=S3Config)\n</code></pre> <p>S3 cloud storage configuration.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.DatabaseConfig","title":"DatabaseConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for SQL database connection configurations.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.DatabaseConfig.cluster","title":"cluster","text":"<pre><code>cluster: Optional[str] = None\n</code></pre> <p>Optional cluster name or identifier (used in ClickHouse and other distributed systems).</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.DatabaseConfig.database","title":"database","text":"<pre><code>database: Optional[str] = None\n</code></pre> <p>Database name.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.DatabaseConfig.host","title":"host","text":"<pre><code>host: Optional[str] = None\n</code></pre> <p>Hostname or IP address of the database server.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.DatabaseConfig.password","title":"password","text":"<pre><code>password: Optional[str] = None\n</code></pre> <p>Password for authentication.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.DatabaseConfig.port","title":"port","text":"<pre><code>port: Optional[int | str] = None\n</code></pre> <p>Database port.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.DatabaseConfig.username","title":"username","text":"<pre><code>username: Optional[str] = None\n</code></pre> <p>Username for authentication.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.GSConfig","title":"GSConfig","text":"<p>               Bases: <code>BucketConfig</code></p> <p>Google Cloud Storage (GCS) configuration.</p> <ul> <li>HMAC credentials allow reading/writing Parquet, JSON, and CSV.</li> <li>Delta table writes require a full service account credential file.</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.configs.GSConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to GCP service account credential file (JSON). Required for certain write operations (e.g., Delta tables).</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.GSConfig.hmac_key","title":"hmac_key","text":"<pre><code>hmac_key: Optional[str] = None\n</code></pre> <p>GCS HMAC key ID for authenticated access.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.GSConfig.hmac_secret","title":"hmac_secret","text":"<pre><code>hmac_secret: Optional[str] = None\n</code></pre> <p>GCS HMAC secret for authenticated access.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.S3Config","title":"S3Config","text":"<p>               Bases: <code>BucketConfig</code></p> <p>S3-specific bucket configuration including optional authentication.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.S3Config.access_key_id","title":"access_key_id","text":"<pre><code>access_key_id: Optional[str] = None\n</code></pre> <p>AWS access key ID for private S3 access.</p>"},{"location":"api/ankaflow/#ankaflow.models.configs.S3Config.secret_access_key","title":"secret_access_key","text":"<pre><code>secret_access_key: Optional[str] = None\n</code></pre> <p>AWS secret access key for private S3 access.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections","title":"connections","text":""},{"location":"api/ankaflow/#ankaflow.models.connections.BigQueryConnection","title":"BigQueryConnection","text":"<p>               Bases: <code>PhysicalConnection</code>, <code>VersionedConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.BigQueryConnection.data_mode","title":"data_mode","text":"<pre><code>data_mode: str = 'error'\n</code></pre> <p>Data mode for write operation. For Deltatable valid options are:</p> <ul> <li><code>append</code> adds new data</li> <li><code>overwrite</code> replaces all data</li> <li><code>error</code> fails (table is read-only)</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.connections.BigQueryConnection.key","title":"key","text":"<pre><code>key: Optional[List[str]] = None\n</code></pre> <p>List of versioned fields</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.BigQueryConnection.kind","title":"kind","text":"<pre><code>kind: Literal['BigQuery']\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.connections.BigQueryConnection.partition","title":"partition","text":"<pre><code>partition: Optional[List[str]] = None\n</code></pre> <p>If set then delta table is partitioned using specified fields for faster reads</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.BigQueryConnection.schema_mode","title":"schema_mode","text":"<pre><code>schema_mode: Optional[str] = None\n</code></pre> <p>Deltatable schema behaviour. If not set then write fails with error in case the data schema does not match existing table schema.</p> <p>Schema evolution options:</p> <ul> <li><code>merge</code> adds new columns from data</li> <li><code>overwrite</code> adds new columns, drops missing</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.connections.BigQueryConnection.version","title":"version","text":"<pre><code>version: Optional[str] = None\n</code></pre> <p>Field for record version timestamp</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.CSVConnection","title":"CSVConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.ClickhouseConnection","title":"ClickhouseConnection","text":"<p>               Bases: <code>PhysicalConnection</code>, <code>VersionedConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.ClickhouseConnection.key","title":"key","text":"<pre><code>key: Optional[List[str]] = None\n</code></pre> <p>List of versioned fields</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.ClickhouseConnection.kind","title":"kind","text":"<pre><code>kind: Literal['Clickhouse']\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.connections.ClickhouseConnection.version","title":"version","text":"<pre><code>version: Optional[str] = None\n</code></pre> <p>Field for record version timestamp</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.Connection","title":"Connection","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.Connection.config","title":"config","text":"<pre><code>config: Optional[ConnectionConfiguration] = None\n</code></pre> <p>Optional configuration for the current connection. If not present then global configuration will be used.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.Connection.create_statement","title":"create_statement","text":"<pre><code>create_statement: str | None = None\n</code></pre> <p>Create statement for given table. Must be in requested dialect.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.Connection.fields","title":"fields","text":"<pre><code>fields: Optional[Columns] = None\n</code></pre> <p>If set then schema is used to generate source structure in case actual source does not provide data in which case generation of ephemeral view fails.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.Connection.kind","title":"kind","text":"<pre><code>kind: str\n</code></pre> <p>Model type e.g. Deltatable, Bigquery, Clickhouse, Parquet, File Custom connections can be loaded from module.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.Connection.params","title":"params","text":"<pre><code>params: Optional[dict] = Field(default_factory=dict)\n</code></pre> <p>Any parameters that can be passed to connection.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.Connection.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If true then schema is automatically detected from the input data and logged '</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.CustomConnection","title":"CustomConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p> <p>Custom connection provider. Custom connection may implement its own logic but must derive from base Connection class, and expose tap(), sink(), sql() and show_schema() even if they are no-op.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.CustomConnection.classname","title":"classname","text":"<pre><code>classname: str\n</code></pre> <p>Name of the class to load from the module</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.CustomConnection.module","title":"module","text":"<pre><code>module: str\n</code></pre> <p>Python module where the connection class is defined</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.CustomConnection.params","title":"params","text":"<pre><code>params: Optional[dict] = Field(default_factory=dict)\n</code></pre> <p>Free-form configuration parameters passed to the loaded class</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.CustomConnection.load","title":"load","text":"<pre><code>load() -&gt; Type[Connection]\n</code></pre> <p>Dynamically load the connection class from the given module and member.</p> <p>Returns:</p> Type Description <code>Type[Connection]</code> <p>A subclass of Connection.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module or member can't be imported.</p> Source code in <code>ankaflow/models/connections.py</code> <pre><code>def load(self) -&gt; t.Type[Connection]:\n    \"\"\"\n    Dynamically load the connection class from the given module and member.\n\n    Returns:\n        A subclass of Connection.\n\n    Raises:\n        ImportError: If the module or member can't be imported.\n    \"\"\"\n    mod = importlib.import_module(self.module)\n    try:\n        cls = getattr(mod, self.classname)\n    except AttributeError as e:\n        raise ImportError(\n            f\"Could not load '{self.classname}' from module '{self.module}'\"\n        ) from e\n    # TODO: Refactor imports to facilitate this check early\n    # if not issubclass(cls, Connection):\n    #     raise TypeError(\n    #         f\"{cls.__name__} is not a subclass of Connection\"\n    #     )\n\n    return cls\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.connections.DeltatableConnection","title":"DeltatableConnection","text":"<p>               Bases: <code>PhysicalConnection</code>, <code>VersionedConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.DeltatableConnection.data_mode","title":"data_mode","text":"<pre><code>data_mode: str = 'error'\n</code></pre> <p>Data mode for write operation. For Deltatable valid options are:</p> <ul> <li><code>append</code> adds new data</li> <li><code>overwrite</code> replaces all data</li> <li><code>error</code> fails (table is read-only)</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.connections.DeltatableConnection.kind","title":"kind","text":"<pre><code>kind: Literal['Deltatable']\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.connections.DeltatableConnection.optimize","title":"optimize","text":"<pre><code>optimize: Optional[Union[str, int]] = 1\n</code></pre> <p>Use with Deltatable and other engines whether to optimize after each sink operation. With larger tables this may be a lengthy synchronous operation.</p> <p>Default value is optimize and vacuum with 7 day retention.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.DeltatableConnection.optimize--deltatable","title":"Deltatable","text":"<p>Values are <code>optimize,vacuum,all,Literal[int]</code>.   If value is literal int is provided then parts older than   number of days will be removed. Note that this will override   default retention days.</p> <p>String options <code>vacuum,all</code> are equivalent to 0.</p> <p>Delta connection also supports raw SQL optimization. Pass statement as sql command:</p> <p><code>OPTIMIZE Deltatable [COMPACT] [VACUUM] [AGE=&lt;int&gt;[d|h]] [DRY_RUN] [CLEANUP]</code></p> <p><code>optimize deltatable</code> \u2192 compact + vacuum with default 7 days</p> <p><code>optimize deltatable compact</code> \u2192 compact only</p> <p><code>optimize deltatable vacuum age=36h dry_run</code> \u2192 list files older than 36 hours, don't delete</p> <p><code>optimize deltatable compact vacuum age=1 cleanup</code> \u2192 compact, vacuum 1 day, then cleanup metadata</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.DeltatableConnection.partition","title":"partition","text":"<pre><code>partition: Optional[List[str]] = None\n</code></pre> <p>If set then delta table is partitioned using specified fields for faster reads</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.DeltatableConnection.schema_mode","title":"schema_mode","text":"<pre><code>schema_mode: Optional[str] = None\n</code></pre> <p>Deltatable schema behaviour. If not set then write fails with error in case the data schema does not match existing table schema.</p> <p>Schema evolution options:</p> <ul> <li><code>merge</code> adds new columns from data</li> <li><code>overwrite</code> adds new columns, drops missing</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.connections.DeltatableConnection.writer_features","title":"writer_features","text":"<pre><code>writer_features: Optional[List] | None = None\n</code></pre> <p>Any supported Delta-rs parameters can be passed to the writer. Example:</p> <pre><code>writer_features: [TimestampWithoutTimezone]\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.connections.Dimension","title":"Dimension","text":"<p>               Bases: <code>EphemeralConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.Dimension.load","title":"load","text":"<pre><code>load() -&gt; Type[Connection]\n</code></pre> <p>Dynamically load the connection class from the given module and member.</p> <p>Returns:</p> Type Description <code>Type[Connection]</code> <p>A subclass of Connection.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module or member can't be imported.</p> Source code in <code>ankaflow/models/connections.py</code> <pre><code>def load(self) -&gt; t.Type[Connection]:\n    \"\"\"\n    Dynamically load the connection class from the given module and member.\n\n    Returns:\n        A subclass of Connection.\n\n    Raises:\n        ImportError: If the module or member can't be imported.\n    \"\"\"\n    mod = importlib.import_module(self.module)\n    try:\n        cls = getattr(mod, self.kind)\n    except AttributeError as e:\n        raise ImportError(\n            f\"Could not load '{self.kind}' from module '{self.module}'\"\n        ) from e\n\n    return cls\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.connections.FileConnection","title":"FileConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.JSONConnection","title":"JSONConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.ParquetConnection","title":"ParquetConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.PhysicalConnection","title":"PhysicalConnection","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.PhysicalConnection.locator","title":"locator","text":"<pre><code>locator: str\n</code></pre> <p>Table name or file name or URI, or other identifier required by the connection.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.PhysicalConnection.raw_dispatch","title":"raw_dispatch","text":"<pre><code>raw_dispatch: bool | None = None\n</code></pre> <p>If True, sends the provided SQL query directly without altering locators if fully qualified, or adding FROM clauses. The query must be a valid SELECT statement. Useful for full control over complex queries.</p> <p>In Deltatable and Parquet connections supports rewriting short locators as convenience.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.RestConnection","title":"RestConnection","text":"<p>               Bases: <code>EphemeralConnection</code></p> <p>Configuration for a REST-based data connection.</p> <p>This model defines how to configure and execute REST API requests as part of a pipeline step. It includes the request definition, client behavior (e.g., retries, headers), and optional schema discovery.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.RestConnection.client","title":"client","text":"<pre><code>client: RestClientConfig\n</code></pre> <p>Configuration for the REST client (e.g., base URL, headers, auth).</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.RestConnection.fields","title":"fields","text":"<pre><code>fields: Optional[Columns] = None\n</code></pre> <p>Optional schema definition used to validate or transform response data.</p> <p>It is recommended to manually specify the schema after initial discovery. This ensures downstream pipeline stages remain stable, even when the remote API returns no results (e.g., due to no updates in an incremental fetch). Explicit schema prevents silent failures or schema drift in such cases.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.RestConnection.kind","title":"kind","text":"<pre><code>kind: Literal['Rest']\n</code></pre> <p>Specifies the connection type as \"Rest\".</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.RestConnection.request","title":"request","text":"<pre><code>request: Request\n</code></pre> <p>Request template specifying method, path, body, etc.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.RestConnection.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If True, the connector will attempt to infer or display the response schema automatically.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.SQLGenConnection","title":"SQLGenConnection","text":"<p>               Bases: <code>EphemeralConnection</code></p> <p>SQLGen connection is intended for SQL code generation:</p> <p>Prompt should instruct the model to generate SQL query and and executes resutling a VIEW in the internal database.</p> <p>Example:</p> <pre><code>Stage 1: Name: ReadSomeParquetData\n\nStage 2: Name: CodeGen, query: Given SQL table `ReadSomeParquetData` generate SQL query to\n    count number of rows in the table.\n\nInside stage 2 the following happens:\n\n1. Prompt is sent to inferecne endpoint\n\n2. Endpoint is expected to respond with valid SQL\n\n3. Connection will execute a statement `CREATE OR REPLACE VIEW StageName AS &lt;received_select_statement&gt;`\n    where statement in the exmaple is likely `SELECT COUNT() FROM ReadSomeParquetData`\n</code></pre> <p>.</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.SQLGenConnection.kind","title":"kind","text":"<pre><code>kind: Literal['SQLGen']\n</code></pre> <p>Specifies the <code>kind==SQLGen</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.SQLGenConnection.variables","title":"variables","text":"<pre><code>variables: dict | None = None\n</code></pre> <p>Variables passed to Prompt. Prompt must be supplied in the <code>query</code> field of the <code>Stage</code>.</p> <p>Prompt may contain Jinja2-style placeholders:</p> Example <p><code>Here's my name: {{name}}.</code></p> <p>The connection will render the prompt template using <code>variables</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.VariableConnection","title":"VariableConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.VersionedConnection","title":"VersionedConnection","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.connections.VersionedConnection.key","title":"key","text":"<pre><code>key: Optional[List[str]] = None\n</code></pre> <p>List of versioned fields</p>"},{"location":"api/ankaflow/#ankaflow.models.connections.VersionedConnection.version","title":"version","text":"<pre><code>version: Optional[str] = None\n</code></pre> <p>Field for record version timestamp</p>"},{"location":"api/ankaflow/#ankaflow.models.core","title":"core","text":""},{"location":"api/ankaflow/#ankaflow.models.core.FlowContext","title":"FlowContext","text":"<p>               Bases: <code>ImmutableMap</code></p> <p>Context dictionary can be used to supply arbitrary data to pipeline that can be referenced in templates much much like variables with the difference that they cannot be modified at runtime.</p> <p>Context can be initiated via dictionary:</p> <p><code>context = FlowContext(**{'foo':'Bar'}])</code></p> <p>Items can be reference using both bracket and dot notation:</p> <p><code>context.foo == context['foo'] == 'Bar'</code></p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage","title":"Stage","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.connection","title":"connection","text":"<pre><code>connection: Optional[\n    Union[\n        RestConnection,\n        VariableConnection,\n        BigQueryConnection,\n        DeltatableConnection,\n        ParquetConnection,\n        ClickhouseConnection,\n        CustomConnection,\n        JSONConnection,\n        CSVConnection,\n        FileConnection,\n        SQLGenConnection,\n        Dimension,\n    ]\n] = Field(None, discriminator=\"kind\")\n</code></pre> <p>Defines how the data is read from / written to the target.</p> <p>Connection fields may contain templates and they will be recursively.</p> <p>Special construct is JSON&gt; which allows dynamically generating parameters as runtime:</p> <pre><code>- kind: source\n  name: source_name\n  connection:\n    kind: File\n    params: &gt;\n      JSON&gt;{\n        \"key\": \"value\",\n        \"dynkey\": &lt;&lt;API.property&gt;&gt;,\n      }\n</code></pre> <p>In the above app <code>params</code> are constructed as JSON string. It is possible to even construct parameter keys dynamically:</p> <pre><code>params: &gt;\n  JSON&gt;\n  {\n    &lt;% for number in [1,2,3] %&gt;\n    \"key_&lt;&lt; number &gt;&gt;\":&lt;&lt; number &gt;&gt;&lt;% if not loop.last %&gt;,&lt;% endif %&gt;\n    &lt;% endfor %&gt;\n  }\n</code></pre> <p>Above example results the following:</p> <pre><code>params: &gt;\n  {\n    \"key_1\": 1,\n    \"key_2\": 2,\n    \"key_3\": 3\n  }\n</code></pre> <p>JSON&gt; structure cannot contain nested JSON&gt; structures, the entire string following the JSON&gt; header must result a valid JSON.</p> <p>Inbuilt connections include:</p> <ul> <li>Deltatable (read)</li> </ul> <p>If connection is <code>Deltatable</code> then query is required to narrow   down data stored in the delta table. <code>FROM</code> clause must be <code>Deltatable</code>: <code>- kind: source     name: delta_tap     connection:       kind: Deltatable       locator: delta_table     query: &gt;       select * from Deltatable</code></p> <ul> <li>Deltatable (write)</li> </ul> <p>See also StorageOptions</p> <p>The following example writes data from preceding stage   to delta table, appending the data, partitions using   <code>part</code> column, and optimizes and vacuums immediately without   retention after write.   <code>- kind: sink     name: delta_sink     connection:       kind: Deltatable       locator: delta_table       optimize: 0       data_mode: append       partition:       - part_field</code></p> <ul> <li>Parquet (read/write)</li> <li>JSON (read/write; NOTE: write operation generates newline-delimited JSON)</li> <li>CSV (read/write)</li> <li> <p>Variable (read/write)</p> </li> <li> <p>File (read)</p> </li> </ul> <p>File can be read from a connected filesystem (including s3). File name   and file type must be specified in the pipeline context:</p> <pre><code>- `context.FileName`: file name relative to `file_prefix`\n- `context.FileType`: file type, one of CSV, XLSX, JSON, HTML\n</code></pre> <p>Any file reader configuration must be passed in <code>params</code>.</p> <ul> <li>Rest (bidirectional)</li> </ul> <p>Rest connections consist of two parts: Client and Request. Client contains   base URL and authentication (basic, digest, header and oauth2 are supported).</p> <p><code>- name: TheRest     kind: source     connection:       kind: Rest       client:         base_url: https://api.example.com         auth:  # Every request to given endpoint share the same authentication           method: basic           values:             username: TheName             password: ThePassword       request:         endpoint: /v1/some-path         content_type: application/json         method: post         query:  # Query parameters           date: &lt;&lt;API.dt(None).isoformat()&gt;&gt;         body:  # JSON payload           param: 1         response:           content_type: application/json           locator: \"JMESPath.to.data\"</code></p> <p>Any custom source (API) can be used as long as available via module.</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.context","title":"context","text":"<pre><code>context: Optional[FlowContext] = None\n</code></pre> <p>@private Global context passed to given stage</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.explain","title":"explain","text":"<pre><code>explain: Optional[bool] = None\n</code></pre> <p>If set to true then SQL query explanation will be logged.</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.fields","title":"fields","text":"<pre><code>fields: Optional[List[Column]] = None\n</code></pre> <p>Explicitly defined output fields.</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.kind","title":"kind","text":"<pre><code>kind: str\n</code></pre> <p>Defines which action will be performed:</p> <ul> <li>source</li> <li>transform</li> <li>sink</li> <li>pipeline</li> <li>sql</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.kind--sink","title":"Sink","text":"<p>Sink reads output from previous stage and stores in specified location.</p> <p>NOTE: If query is supplied with the stage then sink uses output of the query rather than preceding stage directly. Subsequent sink will use preceding stage. If supplied query must create either view or table with same name as current stage.</p> <pre><code>- name: my_sink\nkind: sink\nconnection:\n    kind: Variable\nquery: &gt;\n    CREATE VIEW my_sink AS\n    SELECT\n    1 as foo\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.kind--pipeline","title":"Pipeline","text":"<p>If pipeline is preceded by any stage then the subpipeline will be executed as many times as there are rows in the previous stage output. This is useful if you want to run same pipeline with different parameters. Make sure the pipeline is preceded by source or transform producing required number of rows. If you need to run subpipeline only once there are two options:</p> <ol> <li>Place it to the top</li> <li>Preced with tranform producing single row only</li> </ol> <p>Each row is then passed to subpipeline in a special variable.</p> <p>Example pipeline iterating subpipeline 5 times:</p> <ul> <li>kind: transform       name: generate_5       # Generate 5 rows       query: &gt;           select unnest as current_variable from unnest(generate_series(1,5))       show: 5</li> <li>kind: pipeline     name: looped_5x     stages:         - kind: transform           name: inside_loop           # In query we can reference the value passed from parent pipeline           query: &gt;               select 'Currently running iteration: {API.look('loop_control.current_variable', variables)}' as value           show: 5</li> </ul>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.locator","title":"locator","text":"<pre><code>locator: Optional[str] = None\n</code></pre> <p>Currently unused: Name for the connection configuration: name, or URI.</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.log_level","title":"log_level","text":"<pre><code>log_level: Optional[LogLevel] = None\n</code></pre> <p>Set logging level. All stages after (including current) will log with specified level. Possible values: INFO (default), DEBUG, WARNING. Log level will be reset to INFO after each pipeline (including nested pipelines).</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.name","title":"name","text":"<pre><code>name: str\n</code></pre> <p>Name of the stage, must be unique across all stages in the pipeline and conform to the rules: Must start with letter, may contain lowercase letters, number and underscores. Name is used to reference this stage by other subsequent stages.</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.on_error","title":"on_error","text":"<pre><code>on_error: str = 'fail'\n</code></pre> <p>If set to 'continue' then pipeline will not fail. Subsequent stages referring to failed one must handle missing data.</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.query","title":"query","text":"<pre><code>query: Optional[str] = None\n</code></pre> <p>SQL Query or dictionary with custom source parameters. May contain {dynamic variables}.</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.show","title":"show","text":"<pre><code>show: int = 0\n</code></pre> <p>If set to positive integer then given number of rows from this stage will get logged. If set to -1 then all rows will be loggged. Set to 0 to disable logging.</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If True then schema is logged</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.skip_if","title":"skip_if","text":"<pre><code>skip_if: Optional[Any] = None\n</code></pre> <p>Any value that can evaluated using bool(). or template string e.g. <code>&lt;&lt; True &gt;&gt;</code>. When the expression evaluates to True then the stage is skipped.</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.stages","title":"stages","text":"<pre><code>stages: Optional[Stages] = None\n</code></pre> <p>Used when kind is <code>Flow</code></p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stage.throttle","title":"throttle","text":"<pre><code>throttle: Optional[Union[int, float]] = None\n</code></pre> <p>If set to positive value then flow execution will be paused after the stage for given number of seconds.</p> <p>Useful when dealing with rate limits or otherwise spreading the load over time.</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stages","title":"Stages","text":"<p>               Bases: <code>RootModel[List[Stage]]</code></p> <p>A sequence of processing stages in a data pipeline.</p> <p>Each <code>Datablock</code> in <code>root</code> represents one discrete step or transformation in an end-to-end business workflow (e.g., tap, transform, sink, validate).</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>List[Datablock]</code> <p>Ordered list of pipeline stages to execute.</p>"},{"location":"api/ankaflow/#ankaflow.models.core.Stages.enumerate_steps","title":"enumerate_steps","text":"<pre><code>enumerate_steps() -&gt; Iterator[tuple[int, Stage]]\n</code></pre> <p>Yield each stage along with its 0-based position.</p> <p>Use this when you need both the stage and its index for logging, metrics, or conditional branching.</p> <p>Returns:</p> Type Description <code>Iterator[tuple[int, Stage]]</code> <p>Iterator[Tuple[int, Datablock]]: Pairs of (index, stage).</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>def enumerate_steps(self) -&gt; t.Iterator[tuple[int, Stage]]:\n    \"\"\"Yield each stage along with its 0-based position.\n\n    Use this when you need both the stage and its index for logging,\n    metrics, or conditional branching.\n\n    Returns:\n        Iterator[Tuple[int, Datablock]]: Pairs of (index, stage).\n    \"\"\"\n    return enumerate(self.root)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.core.Stages.load","title":"load","text":"<pre><code>load(source: Union[str, Path, IO[str], Loadable]) -&gt; Stages\n</code></pre> <p>Load a pipeline from YAML (path, YAML-string, file-like or Loadable).</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | Path | IO[str] | Loadable</code> <ul> <li>Path to a .yaml file</li> <li>Raw YAML content</li> <li>File-like object returning YAML</li> <li>Any object with a <code>.load()</code> method returning Python data</li> </ul> required <p>Returns:</p> Name Type Description <code>Stages</code> <code>Stages</code> <p>a validated <code>Stages</code> instance.</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    source: t.Union[str, Path, t.IO[str], Loadable],\n) -&gt; \"Stages\":\n    \"\"\"Load a pipeline from YAML (path, YAML-string, file-like or Loadable).\n\n    Args:\n        source (str | Path | IO[str] | Loadable):\n            - Path to a .yaml file\n            - Raw YAML content\n            - File-like object returning YAML\n            - Any object with a `.load()` method returning Python data\n\n    Returns:\n        Stages: a validated `Stages` instance.\n    \"\"\"\n    # 1) If it\u2019s a loader-object, pull Python data directly\n    if isinstance(source, Loadable):\n        data = source.load()\n\n    else:\n        # 2) Read text for YAML parsing:\n        if hasattr(source, \"read\"):\n            text = t.cast(t.IO[str], source).read()\n        else:\n            text = str(source)\n\n        # 3) First, try parsing as raw YAML\n        try:\n            data = yaml.safe_load(text)\n        except yaml.YAMLError:\n            data = None\n\n        # 4) Only if that parse returned a `str` we treat it as a filename\n        if isinstance(data, str):\n            try:\n                text = Path(data).read_text()\n                data = yaml.safe_load(text)\n            except (OSError, yaml.YAMLError) as e:\n                raise ValueError(\n                    f\"Could not interpret {data!r} as YAML or file path\"\n                ) from e\n\n    # 5) Validate final shape\n    if not isinstance(data, list):\n        raise ValueError(\n            f\"Expected a list of pipeline stages, got {type(data).__name__}\"\n        )\n\n    # 5) Finally, validate into our model\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.core.Stages.steps","title":"steps","text":"<pre><code>steps() -&gt; Iterator[Stage]\n</code></pre> <p>Yield each stage in execution order.</p> <p>Returns:</p> Type Description <code>Iterator[Stage]</code> <p>Iterator[Datablock]: An iterator over the stages,</p> <code>Iterator[Stage]</code> <p>from first to last.</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>def steps(self) -&gt; t.Iterator[Stage]:\n    \"\"\"Yield each stage in execution order.\n\n    Returns:\n        Iterator[Datablock]: An iterator over the stages,\n        from first to last.\n    \"\"\"\n    return iter(self.root)\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.core.Variables","title":"Variables","text":"<p>               Bases: <code>dict</code></p> <p>Variables is a <code>dict</code>-based collection storing arbitrary data under keys. Variable can be populated via:</p> <ul> <li>Dictionary passed to pipeline:   <code>Flow(defs, context, variables={'foo': 'bar'})</code></li> <li>Sink operation:<pre><code>- kind: sink\n\n    name: my_sink\n\n    connection:\n\n        kind: Variable\n\n        locator: variable_name\n</code></pre> </li> </ul> <p>This step will place a list of records from preceding step in   variables as <code>{'variable_name': t.List[dict]}</code></p> <p>In most cases sainking to variable is not needed as all preceding stages in a pipeline can be referenced via <code>name</code>. Sinking to variable is useful when you need to share data with subpipeline.</p> <p>Special variable is <code>loop_control</code> which is populated dynamically from previous step and current stage type is pipeline. In case previous step generates multiple records then the new pipeline is called for each record, and control variable holds a <code>dict</code> representing the current record.</p>"},{"location":"api/ankaflow/#ankaflow.models.enums","title":"enums","text":""},{"location":"api/ankaflow/#ankaflow.models.enums.AuthType","title":"AuthType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.enums.ContentType","title":"ContentType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.enums.DataType","title":"DataType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.enums.LogLevel","title":"LogLevel","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.enums.ModelType","title":"ModelType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.enums.ParameterDisposition","title":"ParameterDisposition","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.enums.RequestMethod","title":"RequestMethod","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow/#ankaflow.models.llm","title":"llm","text":""},{"location":"api/ankaflow/#ankaflow.models.llm.LLMConfig","title":"LLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Confuration for Language Model</p>"},{"location":"api/ankaflow/#ankaflow.models.llm.LLMConfig.kind","title":"kind","text":"<pre><code>kind: LLMKind = MOCK\n</code></pre> <p>Language model provider: <code>mock</code>(default)|<code>openai</code></p>"},{"location":"api/ankaflow/#ankaflow.models.llm.LLMConfig.model","title":"model","text":"<pre><code>model: str | None = None\n</code></pre> <p>Language model, if not set then uses default</p>"},{"location":"api/ankaflow/#ankaflow.models.llm.LLMConfig.proxy","title":"proxy","text":"<pre><code>proxy: LLMProxy | None = None\n</code></pre> <p>Reverse proxy used to connect to provider (optional)</p>"},{"location":"api/ankaflow/#ankaflow.models.llm.LLMKind","title":"LLMKind","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported LLM protocol backends.</p>"},{"location":"api/ankaflow/#ankaflow.models.llm.LLMKind.MOCK","title":"MOCK","text":"<pre><code>MOCK = 'mock'\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.llm.LLMKind.OPENAI","title":"OPENAI","text":"<pre><code>OPENAI = 'openai'\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.llm.LLMProxy","title":"LLMProxy","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for proxy-based LLM usage.</p>"},{"location":"api/ankaflow/#ankaflow.models.llm.LLMProxy.client","title":"client","text":"<pre><code>client: RestClientConfig\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.llm.LLMProxy.request","title":"request","text":"<pre><code>request: Request\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.rest","title":"rest","text":""},{"location":"api/ankaflow/#ankaflow.models.rest.BasicHandler","title":"BasicHandler","text":"<p>               Bases: <code>BaseModel</code></p> <p>A no-op response handler used when no special processing (like pagination or transformation) is required.</p> <p>Typically used for single-response REST endpoints where the entire payload is returned in one request.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal</code> <p>Specifies the handler type as BASIC.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.BasicHandler.kind","title":"kind","text":"<pre><code>kind: Literal[BASIC]\n</code></pre> <p>Specifies the handler type as Basic.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Paginator","title":"Paginator","text":"<p>               Bases: <code>BaseModel</code></p> <p>A response handler for paginated REST APIs.</p> <p>This handler generates repeated requests by incrementing a page-related parameter until no more data is available. The stopping condition is usually inferred from the number of records in the response being less than <code>page_size</code>, or from a total record count field.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Paginator.increment","title":"increment","text":"<pre><code>increment: int\n</code></pre> <p>Page parameter increment. Original request configuration should include initial value e.g. <code>page_no=1</code></p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Paginator.kind","title":"kind","text":"<pre><code>kind: Literal[PAGINATOR]\n</code></pre> <p>Specifies the handler type as Paginator.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Paginator.page_param","title":"page_param","text":"<pre><code>page_param: str\n</code></pre> <p>Page parameter in the request (query or body) This will be incremented from request to request</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Paginator.page_size","title":"page_size","text":"<pre><code>page_size: int\n</code></pre> <p>Page size should be explicitly defined. If response contains less records it is considered to be last page</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Paginator.param_locator","title":"param_locator","text":"<pre><code>param_locator: ParameterDisposition\n</code></pre> <p>Define where the parameter is located: body or query</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Paginator.throttle","title":"throttle","text":"<pre><code>throttle: Optional[Union[int, float]] = None\n</code></pre> <p>If set to positive value then each page request is throttled given number of seconds.</p> <p>Useful when dealing with rate limits or otherwise spreading the load over time.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Paginator.total_records","title":"total_records","text":"<pre><code>total_records: Optional[str] = None\n</code></pre> <p>JMESPath to total records count in the response.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Request","title":"Request","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Request.body","title":"body","text":"<pre><code>body: Optional[Union[str, Dict]] = None\n</code></pre> <p>Request body parameters.</p> <p>This field accepts either: - A Python <code>dict</code> representing a direct key-value mapping, or - A Jinja-templated JSON string with magic <code>@json</code> prefix, e.g.: <code>@json{\"parameter\": \"value\"}</code></p> <p>The template will be rendered using the following custom delimiters: - <code>&lt;&lt; ... &gt;&gt;</code> for variable interpolation - <code>&lt;% ... %&gt;</code> for logic/control flow (e.g., for-loops) - <code>&lt;# ... #&gt;</code> for inline comments</p> <p>The template will be rendered before being parsed into a valid JSON object. This allows the use of dynamic expressions, filters, and control flow such as loops.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Request.body--example-with-looping","title":"Example with looping","text":"<p>Given:</p> <pre><code>variables = {\n    \"MyList\": [\n        {\"id\": 1, \"value\": 10},\n        {\"id\": 2, \"value\": 20}\n    ]\n}\n</code></pre> <p>You can generate a dynamic body with:</p> <pre><code>body: &gt;\n@json[\n    &lt;% for row in API.look(\"MyTable\", variables) %&gt;\n        { \"id\": &lt;&lt; row.id &gt;&gt;, \"value\": &lt;&lt; row.value &gt;&gt; }&lt;% if not loop.last %&gt;,&lt;% endif %&gt;\n    &lt;% endfor %&gt;\n]\n</code></pre> <p>This will render to a proper JSON list:</p> <pre><code>[\n    { \"id\": 1, \"value\": 10 },\n    { \"id\": 2, \"value\": 20 }\n]\n</code></pre> <p>Notes: - When using @json, the entire string is rendered as a Jinja template     and then parsed with json.loads(). - Nested @json blocks are not supported. - Newlines and whitespace are automatically collapsed during rendering.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Request.content_type","title":"content_type","text":"<pre><code>content_type: ContentType = JSON\n</code></pre> <p>Request content type</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Request.endpoint","title":"endpoint","text":"<pre><code>endpoint: str\n</code></pre> <p>Request endpoint e.g. <code>get/data</code> under base url:</p> <p>Example <code>https://api.example.com/v1</code> + <code>get/data</code></p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Request.errorhandler","title":"errorhandler","text":"<pre><code>errorhandler: RestErrorHandler = Field(\n    default_factory=RestErrorHandler\n)\n</code></pre> <p>Custom error handler e.g. for searching conditions in response or custom status codes</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Request.initial_backoff","title":"initial_backoff","text":"<pre><code>initial_backoff: float = 0.5\n</code></pre> <p>Initial backoff time in seconds. Will be multiplied exponentially for subsequent retries (2^n).</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Request.max_retries","title":"max_retries","text":"<pre><code>max_retries: int = 0\n</code></pre> <p>Maximum number of retries on transport errors. Default is 0 (no retry).</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Request.method","title":"method","text":"<pre><code>method: RequestMethod\n</code></pre> <p>Request method e.g. <code>post,get,put</code></p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Request.query","title":"query","text":"<pre><code>query: Dict = {}\n</code></pre> <p>Query parameters. Parameters may contain template variables.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.Request.response","title":"response","text":"<pre><code>response: RestResponse\n</code></pre> <p>Response handling configuration</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.ResponseHandlerTypes","title":"ResponseHandlerTypes","text":""},{"location":"api/ankaflow/#ankaflow.models.rest.RestAuth","title":"RestAuth","text":"<p>               Bases: <code>BaseModel</code></p> <p>Authenctication configuration for Rest connection.</p> <p>NOTE: Not all authentication methods may not work in browser due to limitations in the network API.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.RestAuth.method","title":"method","text":"<pre><code>method: AuthType\n</code></pre> <p>Specifies authentiation type.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.RestAuth.values","title":"values","text":"<pre><code>values: StringDict\n</code></pre> <p>Mapping of parameter names and values.</p> <p>{     'X-Auth-Token': '' }"},{"location":"api/ankaflow/#ankaflow.models.rest.RestAuth.coerce_to_stringdict","title":"coerce_to_stringdict","text":"<pre><code>coerce_to_stringdict(v)\n</code></pre> <p>Rest header values must be strings. This convenience validator  automatically converts regiular dictionary to StringDict.</p> Source code in <code>ankaflow/models/rest.py</code> <pre><code>@field_validator(\"values\", mode=\"before\")\n@classmethod\ndef coerce_to_stringdict(cls, v):\n    \"\"\"Rest header values must be strings.\n    This convenience validator  automatically\n    converts regiular dictionary to StringDict.\n    \"\"\"\n    if isinstance(v, StringDict):\n        return v\n    if isinstance(v, dict):\n        return StringDict(v)\n    raise TypeError(\"Expected a StringDict or a dict for `values`\")\n</code></pre>"},{"location":"api/ankaflow/#ankaflow.models.rest.RestClientConfig","title":"RestClientConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Rest client for given base URL. Includes transport and authentication configuration</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.RestClientConfig.base_url","title":"base_url","text":"<pre><code>base_url: str\n</code></pre> <p>Base URL, typically server or API root. All endpoints with the same base URL share the same authentication.</p> <p>Example: <code>https://api.example.com/v1</code></p>"},{"location":"api/ankaflow/#ankaflow.models.rest.RestClientConfig.timeout","title":"timeout","text":"<pre><code>timeout: Optional[float] = None\n</code></pre> <p>Request timeout in seconds. Default is 5. Set 0 to disable timout.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.RestErrorHandler","title":"RestErrorHandler","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow/#ankaflow.models.rest.RestErrorHandler.condition","title":"condition","text":"<pre><code>condition: Optional[str] = None\n</code></pre> <p>JMESPath expression to look for in the response body. Error will be generated if expression evaluates to True</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.RestErrorHandler.error_status_codes","title":"error_status_codes","text":"<pre><code>error_status_codes: List[int] = []\n</code></pre> <p>List of HTTP status codes to be treated as errors.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.RestErrorHandler.message","title":"message","text":"<pre><code>message: Optional[str] = None\n</code></pre> <p>JMESPath expression to extract error message from respose. If omitted entire response will be included in error.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.RestResponse","title":"RestResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response configuration. Response can be paged, polled URL or in body.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.RestResponse.content_type","title":"content_type","text":"<pre><code>content_type: DataType\n</code></pre> <p>Returned data type</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.RestResponse.locator","title":"locator","text":"<pre><code>locator: Optional[str] = None\n</code></pre> <p>JMESPath to read data from JSON body. If not set then entire body is treated as data.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.StatePoller","title":"StatePoller","text":"<p>               Bases: <code>BaseModel</code></p> <p>A response handler for state-based polling APIs.</p> <p>This handler is designed for asynchronous workflows where the client repeatedly polls an endpoint until a certain state is reached (e.g., job completion, resource readiness). Once the condition is met, the pipeline continues by reading from the final data <code>locator</code>.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.StatePoller.kind","title":"kind","text":"<pre><code>kind: Literal[STATEPOLLING]\n</code></pre> <p>Specifies the handler type as StatePolling.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.StatePoller.ready_status","title":"ready_status","text":"<pre><code>ready_status: str\n</code></pre> <p>JMESPath to read status from response. If the value at path evaluates to True then no more requests are made, and the API tries to read data from URL specified by <code>locator</code>.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.URLPoller","title":"URLPoller","text":"<p>               Bases: <code>BaseModel</code></p> <p>URL Poller makes request(s) to remote API until an URL is returned</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.URLPoller.kind","title":"kind","text":"<pre><code>kind: Literal[URLPOLLING]\n</code></pre> <p>Specifies the handler type as URLPolling.</p>"},{"location":"api/ankaflow/#ankaflow.models.rest.URLPoller.ready_status","title":"ready_status","text":"<pre><code>ready_status: Optional[str] = None\n</code></pre> <p>JMESPath to read status from response. If the value at path evaluates to True then no more requests are made, and the API tries to read data from URL specified by <code>locator</code>.</p>"},{"location":"api/ankaflow/#ankaflow.tests","title":"tests","text":""},{"location":"api/ankaflow.models/","title":"Pipeline API","text":"<p>Pipeline API provides building blocks for YAML pipeline.</p> <p>Each stage and configuration object here represents a specific part of the YAML definition file.</p> <p>Example pipeline stage with all possible keys. Actual usage of keys depends on used model <code>kind</code>.</p> <pre><code>- name: str\n  kind: source | sink | transform\n  log_level: DEBUG\n  skip_if: \n  on_error: error | continue\n  connection: Connection\n    kind: Parquet | Deltatable | Rest | ...\n    locator: str\n    config: ConnectionConfiguration\n    client: RestClient\n    request: RestRequest\n      endpoint: str\n      method: RequestMethod\n      errorhandler: RestErrorHandler\n      auth: RestAuth\n        method: header | oauth2 | ...\n        values: StringDict\n      query: dict | @json-magic\n      body: dict | @json-magic\n      response: RestResponse\n        handler: \n            kind: ResponseHandlerTypes\n            page_param: str\n            page_size: str\n            param_locator: ParameterDisposition\n            total_records: str\n            throttle: int | float \n        content_type: DataType\n        locator: str\n    fields: Fields\n  show: int\n  show_schema: bool\n  query: &gt; str\n  stages: Stages\n  throttle: int\n\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.__all__","title":"__all__","text":"<pre><code>__all__ = [\n    \"Column\",\n    \"Columns\",\n    \"Stages\",\n    \"Stage\",\n    \"ConnectionConfiguration\",\n    \"Variables\",\n    \"FlowContext\",\n    \"S3Config\",\n    \"GSConfig\",\n    \"ClickhouseConfig\",\n    \"BigQueryConfig\",\n    \"BucketConfig\",\n    \"Connection\",\n    \"PhysicalConnection\",\n    \"EphemeralConnection\",\n    \"ParquetConnection\",\n    \"CSVConnection\",\n    \"JSONConnection\",\n    \"FileConnection\",\n    \"BigQueryConnection\",\n    \"ClickhouseConnection\",\n    \"DeltatableConnection\",\n    \"Dimension\",\n    \"CustomConnection\",\n    \"RestConnection\",\n    \"SQLGenConnection\",\n]\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConfig","title":"BigQueryConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for accessing BigQuery datasets.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to service account JSON credentials for BigQuery access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConfig.dataset","title":"dataset","text":"<pre><code>dataset: Optional[str] = None\n</code></pre> <p>BigQuery dataset name.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConfig.project","title":"project","text":"<pre><code>project: Optional[str] = None\n</code></pre> <p>GCP project ID containing the BigQuery dataset.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConfig.region","title":"region","text":"<pre><code>region: Optional[str] = None\n</code></pre> <p>BigQuery region (e.g., \"us-central1\").</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConnection","title":"BigQueryConnection","text":"<p>               Bases: <code>PhysicalConnection</code>, <code>VersionedConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConnection.data_mode","title":"data_mode","text":"<pre><code>data_mode: str = 'error'\n</code></pre> <p>Data mode for write operation. For Deltatable valid options are:</p> <ul> <li><code>append</code> adds new data</li> <li><code>overwrite</code> replaces all data</li> <li><code>error</code> fails (table is read-only)</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConnection.key","title":"key","text":"<pre><code>key: Optional[List[str]] = None\n</code></pre> <p>List of versioned fields</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConnection.kind","title":"kind","text":"<pre><code>kind: Literal['BigQuery']\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConnection.partition","title":"partition","text":"<pre><code>partition: Optional[List[str]] = None\n</code></pre> <p>If set then delta table is partitioned using specified fields for faster reads</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConnection.schema_mode","title":"schema_mode","text":"<pre><code>schema_mode: Optional[str] = None\n</code></pre> <p>Deltatable schema behaviour. If not set then write fails with error in case the data schema does not match existing table schema.</p> <p>Schema evolution options:</p> <ul> <li><code>merge</code> adds new columns from data</li> <li><code>overwrite</code> adds new columns, drops missing</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.BigQueryConnection.version","title":"version","text":"<pre><code>version: Optional[str] = None\n</code></pre> <p>Field for record version timestamp</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BucketConfig","title":"BucketConfig","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.BucketConfig.bucket","title":"bucket","text":"<pre><code>bucket: Optional[str] = None\n</code></pre> <p>Bucket name eg. <code>s3://my-bucket</code> or local absolute path eg. <code>/data/path</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.BucketConfig.data_prefix","title":"data_prefix","text":"<pre><code>data_prefix: Optional[str] = None\n</code></pre> <p>Prefix for data files: <code>s3://my-bucket/&lt;prefix&gt;</code> or <code>/data/path/&lt;prefix&gt;</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.BucketConfig.locator_wildcard","title":"locator_wildcard","text":"<pre><code>locator_wildcard: Optional[Tuple] = None\n</code></pre> <p>Regular expression and wildcard to modify <code>locator</code>. Useful in cases when you sink data to <code>data-YYYY.parquet</code> but want to read <code>data-*.parquet</code>. Provide tuple with patten as first element and replacement as second one. Example:</p> <pre><code>('-\\d{4}-', '*')\n</code></pre> <p>This will create wildcard for <code>data-YYYY-base.parquet</code> as <code>data*base.parquet</code>.</p> Wildcard Description <code>*</code> matches any number of any characters (including none) <code>**</code> matches any number of subdirectories (including none) <code>[abc]</code> matches one character given in the bracket <code>[a-z]</code> matches one character from the range given in the bracket <p>Wildcard is automatically applied in tap and show_schema operations.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.BucketConfig.region","title":"region","text":"<pre><code>region: str | None = None\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.CSVConnection","title":"CSVConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.ClickhouseConfig","title":"ClickhouseConfig","text":"<p>               Bases: <code>DatabaseConfig</code></p> <p>Configuration for ClickHouse database, extending generic SQL config.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ClickhouseConfig.blocksize","title":"blocksize","text":"<pre><code>blocksize: int = 50000\n</code></pre> <p>Number of rows to process per block for batch operations.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ClickhouseConnection","title":"ClickhouseConnection","text":"<p>               Bases: <code>PhysicalConnection</code>, <code>VersionedConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.ClickhouseConnection.key","title":"key","text":"<pre><code>key: Optional[List[str]] = None\n</code></pre> <p>List of versioned fields</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ClickhouseConnection.kind","title":"kind","text":"<pre><code>kind: Literal['Clickhouse']\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.ClickhouseConnection.version","title":"version","text":"<pre><code>version: Optional[str] = None\n</code></pre> <p>Field for record version timestamp</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Column","title":"Column","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data column (equivalent to database column)</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Column.name","title":"name","text":"<pre><code>name: str\n</code></pre> <p>Column name must follow rules set to SQL engine column names</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Column.type","title":"type","text":"<pre><code>type: str\n</code></pre> <p>Any data type support by SQL engine</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Columns","title":"Columns","text":"<p>               Bases: <code>RootModel[List[Column]]</code></p> <p>Iterable list-like collection of Fields.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection","title":"Connection","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.config","title":"config","text":"<pre><code>config: Optional[ConnectionConfiguration] = None\n</code></pre> <p>Optional configuration for the current connection. If not present then global configuration will be used.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.create_statement","title":"create_statement","text":"<pre><code>create_statement: str | None = None\n</code></pre> <p>Create statement for given table. Must be in requested dialect.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.fields","title":"fields","text":"<pre><code>fields: Optional[Columns] = None\n</code></pre> <p>If set then schema is used to generate source structure in case actual source does not provide data in which case generation of ephemeral view fails.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.kind","title":"kind","text":"<pre><code>kind: str\n</code></pre> <p>Model type e.g. Deltatable, Bigquery, Clickhouse, Parquet, File Custom connections can be loaded from module.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.params","title":"params","text":"<pre><code>params: Optional[dict] = Field(default_factory=dict)\n</code></pre> <p>Any parameters that can be passed to connection.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Connection.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If true then schema is automatically detected from the input data and logged '</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ConnectionConfiguration","title":"ConnectionConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level container for all connection configurations in a pipeline.</p> <p>Includes default configuration blocks for supported sources and sinks like: - Local file systems - S3 and GCS buckets - BigQuery datasets - ClickHouse databases</p> <p>These can be customized per pipeline or extended with new sources.</p> Example <p>To support MySQL, you can define a new model like this:</p> <pre><code>class MySQLConfig(DatabaseConfig):\n    pass  # You can add MySQL-specific fields here if needed\n\nclass ExtendedConnectionConfiguration(ConnectionConfiguration):\n    mysql: MySQLConfig = PydanticField(default_factory=MySQLConfig)\n</code></pre> <p>This will allow you to specify MySQL connection settings in your YAML:</p> <pre><code>connection:\n  kind: MySQL\n  config:\n    host: localhost\n    port: 3306\n    database: sales\n    username: admin\n    password: secret\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.ConnectionConfiguration.bigquery","title":"bigquery","text":"<pre><code>bigquery: BigQueryConfig = Field(\n    default_factory=BigQueryConfig\n)\n</code></pre> <p>BigQuery connection configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ConnectionConfiguration.clickhouse","title":"clickhouse","text":"<pre><code>clickhouse: ClickhouseConfig = Field(\n    default_factory=ClickhouseConfig\n)\n</code></pre> <p>Language model configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ConnectionConfiguration.gs","title":"gs","text":"<pre><code>gs: GSConfig = Field(default_factory=GSConfig)\n</code></pre> <p>Google Cloud Storage configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ConnectionConfiguration.llm","title":"llm","text":"<pre><code>llm: LLMConfig = Field(default_factory=LLMConfig)\n</code></pre> <p>Language model configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ConnectionConfiguration.local","title":"local","text":"<pre><code>local: BucketConfig = Field(default_factory=BucketConfig)\n</code></pre> <p>Local file system configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.ConnectionConfiguration.s3","title":"s3","text":"<pre><code>s3: S3Config = Field(default_factory=S3Config)\n</code></pre> <p>S3 cloud storage configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.CustomConnection","title":"CustomConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p> <p>Custom connection provider. Custom connection may implement its own logic but must derive from base Connection class, and expose tap(), sink(), sql() and show_schema() even if they are no-op.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.CustomConnection.classname","title":"classname","text":"<pre><code>classname: str\n</code></pre> <p>Name of the class to load from the module</p>"},{"location":"api/ankaflow.models/#ankaflow.models.CustomConnection.module","title":"module","text":"<pre><code>module: str\n</code></pre> <p>Python module where the connection class is defined</p>"},{"location":"api/ankaflow.models/#ankaflow.models.CustomConnection.params","title":"params","text":"<pre><code>params: Optional[dict] = Field(default_factory=dict)\n</code></pre> <p>Free-form configuration parameters passed to the loaded class</p>"},{"location":"api/ankaflow.models/#ankaflow.models.CustomConnection.load","title":"load","text":"<pre><code>load() -&gt; Type[Connection]\n</code></pre> <p>Dynamically load the connection class from the given module and member.</p> <p>Returns:</p> Type Description <code>Type[Connection]</code> <p>A subclass of Connection.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module or member can't be imported.</p> Source code in <code>ankaflow/models/connections.py</code> <pre><code>def load(self) -&gt; t.Type[Connection]:\n    \"\"\"\n    Dynamically load the connection class from the given module and member.\n\n    Returns:\n        A subclass of Connection.\n\n    Raises:\n        ImportError: If the module or member can't be imported.\n    \"\"\"\n    mod = importlib.import_module(self.module)\n    try:\n        cls = getattr(mod, self.classname)\n    except AttributeError as e:\n        raise ImportError(\n            f\"Could not load '{self.classname}' from module '{self.module}'\"\n        ) from e\n    # TODO: Refactor imports to facilitate this check early\n    # if not issubclass(cls, Connection):\n    #     raise TypeError(\n    #         f\"{cls.__name__} is not a subclass of Connection\"\n    #     )\n\n    return cls\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.DeltatableConnection","title":"DeltatableConnection","text":"<p>               Bases: <code>PhysicalConnection</code>, <code>VersionedConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.DeltatableConnection.data_mode","title":"data_mode","text":"<pre><code>data_mode: str = 'error'\n</code></pre> <p>Data mode for write operation. For Deltatable valid options are:</p> <ul> <li><code>append</code> adds new data</li> <li><code>overwrite</code> replaces all data</li> <li><code>error</code> fails (table is read-only)</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.DeltatableConnection.kind","title":"kind","text":"<pre><code>kind: Literal['Deltatable']\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.DeltatableConnection.optimize","title":"optimize","text":"<pre><code>optimize: Optional[Union[str, int]] = 1\n</code></pre> <p>Use with Deltatable and other engines whether to optimize after each sink operation. With larger tables this may be a lengthy synchronous operation.</p> <p>Default value is optimize and vacuum with 7 day retention.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.DeltatableConnection.optimize--deltatable","title":"Deltatable","text":"<p>Values are <code>optimize,vacuum,all,Literal[int]</code>.   If value is literal int is provided then parts older than   number of days will be removed. Note that this will override   default retention days.</p> <p>String options <code>vacuum,all</code> are equivalent to 0.</p> <p>Delta connection also supports raw SQL optimization. Pass statement as sql command:</p> <p><code>OPTIMIZE Deltatable [COMPACT] [VACUUM] [AGE=&lt;int&gt;[d|h]] [DRY_RUN] [CLEANUP]</code></p> <p><code>optimize deltatable</code> \u2192 compact + vacuum with default 7 days</p> <p><code>optimize deltatable compact</code> \u2192 compact only</p> <p><code>optimize deltatable vacuum age=36h dry_run</code> \u2192 list files older than 36 hours, don't delete</p> <p><code>optimize deltatable compact vacuum age=1 cleanup</code> \u2192 compact, vacuum 1 day, then cleanup metadata</p>"},{"location":"api/ankaflow.models/#ankaflow.models.DeltatableConnection.partition","title":"partition","text":"<pre><code>partition: Optional[List[str]] = None\n</code></pre> <p>If set then delta table is partitioned using specified fields for faster reads</p>"},{"location":"api/ankaflow.models/#ankaflow.models.DeltatableConnection.schema_mode","title":"schema_mode","text":"<pre><code>schema_mode: Optional[str] = None\n</code></pre> <p>Deltatable schema behaviour. If not set then write fails with error in case the data schema does not match existing table schema.</p> <p>Schema evolution options:</p> <ul> <li><code>merge</code> adds new columns from data</li> <li><code>overwrite</code> adds new columns, drops missing</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.DeltatableConnection.writer_features","title":"writer_features","text":"<pre><code>writer_features: Optional[List] | None = None\n</code></pre> <p>Any supported Delta-rs parameters can be passed to the writer. Example:</p> <pre><code>writer_features: [TimestampWithoutTimezone]\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.Dimension","title":"Dimension","text":"<p>               Bases: <code>EphemeralConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Dimension.load","title":"load","text":"<pre><code>load() -&gt; Type[Connection]\n</code></pre> <p>Dynamically load the connection class from the given module and member.</p> <p>Returns:</p> Type Description <code>Type[Connection]</code> <p>A subclass of Connection.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module or member can't be imported.</p> Source code in <code>ankaflow/models/connections.py</code> <pre><code>def load(self) -&gt; t.Type[Connection]:\n    \"\"\"\n    Dynamically load the connection class from the given module and member.\n\n    Returns:\n        A subclass of Connection.\n\n    Raises:\n        ImportError: If the module or member can't be imported.\n    \"\"\"\n    mod = importlib.import_module(self.module)\n    try:\n        cls = getattr(mod, self.kind)\n    except AttributeError as e:\n        raise ImportError(\n            f\"Could not load '{self.kind}' from module '{self.module}'\"\n        ) from e\n\n    return cls\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.EphemeralConnection","title":"EphemeralConnection","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.FileConnection","title":"FileConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.FlowContext","title":"FlowContext","text":"<p>               Bases: <code>ImmutableMap</code></p> <p>Context dictionary can be used to supply arbitrary data to pipeline that can be referenced in templates much much like variables with the difference that they cannot be modified at runtime.</p> <p>Context can be initiated via dictionary:</p> <p><code>context = FlowContext(**{'foo':'Bar'}])</code></p> <p>Items can be reference using both bracket and dot notation:</p> <p><code>context.foo == context['foo'] == 'Bar'</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.GSConfig","title":"GSConfig","text":"<p>               Bases: <code>BucketConfig</code></p> <p>Google Cloud Storage (GCS) configuration.</p> <ul> <li>HMAC credentials allow reading/writing Parquet, JSON, and CSV.</li> <li>Delta table writes require a full service account credential file.</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.GSConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to GCP service account credential file (JSON). Required for certain write operations (e.g., Delta tables).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.GSConfig.hmac_key","title":"hmac_key","text":"<pre><code>hmac_key: Optional[str] = None\n</code></pre> <p>GCS HMAC key ID for authenticated access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.GSConfig.hmac_secret","title":"hmac_secret","text":"<pre><code>hmac_secret: Optional[str] = None\n</code></pre> <p>GCS HMAC secret for authenticated access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.JSONConnection","title":"JSONConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.ParquetConnection","title":"ParquetConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.PhysicalConnection","title":"PhysicalConnection","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.PhysicalConnection.locator","title":"locator","text":"<pre><code>locator: str\n</code></pre> <p>Table name or file name or URI, or other identifier required by the connection.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.PhysicalConnection.raw_dispatch","title":"raw_dispatch","text":"<pre><code>raw_dispatch: bool | None = None\n</code></pre> <p>If True, sends the provided SQL query directly without altering locators if fully qualified, or adding FROM clauses. The query must be a valid SELECT statement. Useful for full control over complex queries.</p> <p>In Deltatable and Parquet connections supports rewriting short locators as convenience.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestConnection","title":"RestConnection","text":"<p>               Bases: <code>EphemeralConnection</code></p> <p>Configuration for a REST-based data connection.</p> <p>This model defines how to configure and execute REST API requests as part of a pipeline step. It includes the request definition, client behavior (e.g., retries, headers), and optional schema discovery.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestConnection.client","title":"client","text":"<pre><code>client: RestClientConfig\n</code></pre> <p>Configuration for the REST client (e.g., base URL, headers, auth).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestConnection.fields","title":"fields","text":"<pre><code>fields: Optional[Columns] = None\n</code></pre> <p>Optional schema definition used to validate or transform response data.</p> <p>It is recommended to manually specify the schema after initial discovery. This ensures downstream pipeline stages remain stable, even when the remote API returns no results (e.g., due to no updates in an incremental fetch). Explicit schema prevents silent failures or schema drift in such cases.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestConnection.kind","title":"kind","text":"<pre><code>kind: Literal['Rest']\n</code></pre> <p>Specifies the connection type as \"Rest\".</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestConnection.request","title":"request","text":"<pre><code>request: Request\n</code></pre> <p>Request template specifying method, path, body, etc.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.RestConnection.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If True, the connector will attempt to infer or display the response schema automatically.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.S3Config","title":"S3Config","text":"<p>               Bases: <code>BucketConfig</code></p> <p>S3-specific bucket configuration including optional authentication.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.S3Config.access_key_id","title":"access_key_id","text":"<pre><code>access_key_id: Optional[str] = None\n</code></pre> <p>AWS access key ID for private S3 access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.S3Config.secret_access_key","title":"secret_access_key","text":"<pre><code>secret_access_key: Optional[str] = None\n</code></pre> <p>AWS secret access key for private S3 access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.SQLGenConnection","title":"SQLGenConnection","text":"<p>               Bases: <code>EphemeralConnection</code></p> <p>SQLGen connection is intended for SQL code generation:</p> <p>Prompt should instruct the model to generate SQL query and and executes resutling a VIEW in the internal database.</p> <p>Example:</p> <pre><code>Stage 1: Name: ReadSomeParquetData\n\nStage 2: Name: CodeGen, query: Given SQL table `ReadSomeParquetData` generate SQL query to\n    count number of rows in the table.\n\nInside stage 2 the following happens:\n\n1. Prompt is sent to inferecne endpoint\n\n2. Endpoint is expected to respond with valid SQL\n\n3. Connection will execute a statement `CREATE OR REPLACE VIEW StageName AS &lt;received_select_statement&gt;`\n    where statement in the exmaple is likely `SELECT COUNT() FROM ReadSomeParquetData`\n</code></pre> <p>.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.SQLGenConnection.kind","title":"kind","text":"<pre><code>kind: Literal['SQLGen']\n</code></pre> <p>Specifies the <code>kind==SQLGen</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.SQLGenConnection.variables","title":"variables","text":"<pre><code>variables: dict | None = None\n</code></pre> <p>Variables passed to Prompt. Prompt must be supplied in the <code>query</code> field of the <code>Stage</code>.</p> <p>Prompt may contain Jinja2-style placeholders:</p> Example <p><code>Here's my name: {{name}}.</code></p> <p>The connection will render the prompt template using <code>variables</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage","title":"Stage","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.connection","title":"connection","text":"<pre><code>connection: Optional[\n    Union[\n        RestConnection,\n        VariableConnection,\n        BigQueryConnection,\n        DeltatableConnection,\n        ParquetConnection,\n        ClickhouseConnection,\n        CustomConnection,\n        JSONConnection,\n        CSVConnection,\n        FileConnection,\n        SQLGenConnection,\n        Dimension,\n    ]\n] = Field(None, discriminator=\"kind\")\n</code></pre> <p>Defines how the data is read from / written to the target.</p> <p>Connection fields may contain templates and they will be recursively.</p> <p>Special construct is JSON&gt; which allows dynamically generating parameters as runtime:</p> <pre><code>- kind: source\n  name: source_name\n  connection:\n    kind: File\n    params: &gt;\n      JSON&gt;{\n        \"key\": \"value\",\n        \"dynkey\": &lt;&lt;API.property&gt;&gt;,\n      }\n</code></pre> <p>In the above app <code>params</code> are constructed as JSON string. It is possible to even construct parameter keys dynamically:</p> <pre><code>params: &gt;\n  JSON&gt;\n  {\n    &lt;% for number in [1,2,3] %&gt;\n    \"key_&lt;&lt; number &gt;&gt;\":&lt;&lt; number &gt;&gt;&lt;% if not loop.last %&gt;,&lt;% endif %&gt;\n    &lt;% endfor %&gt;\n  }\n</code></pre> <p>Above example results the following:</p> <pre><code>params: &gt;\n  {\n    \"key_1\": 1,\n    \"key_2\": 2,\n    \"key_3\": 3\n  }\n</code></pre> <p>JSON&gt; structure cannot contain nested JSON&gt; structures, the entire string following the JSON&gt; header must result a valid JSON.</p> <p>Inbuilt connections include:</p> <ul> <li>Deltatable (read)</li> </ul> <p>If connection is <code>Deltatable</code> then query is required to narrow   down data stored in the delta table. <code>FROM</code> clause must be <code>Deltatable</code>: <code>- kind: source     name: delta_tap     connection:       kind: Deltatable       locator: delta_table     query: &gt;       select * from Deltatable</code></p> <ul> <li>Deltatable (write)</li> </ul> <p>See also StorageOptions</p> <p>The following example writes data from preceding stage   to delta table, appending the data, partitions using   <code>part</code> column, and optimizes and vacuums immediately without   retention after write.   <code>- kind: sink     name: delta_sink     connection:       kind: Deltatable       locator: delta_table       optimize: 0       data_mode: append       partition:       - part_field</code></p> <ul> <li>Parquet (read/write)</li> <li>JSON (read/write; NOTE: write operation generates newline-delimited JSON)</li> <li>CSV (read/write)</li> <li> <p>Variable (read/write)</p> </li> <li> <p>File (read)</p> </li> </ul> <p>File can be read from a connected filesystem (including s3). File name   and file type must be specified in the pipeline context:</p> <pre><code>- `context.FileName`: file name relative to `file_prefix`\n- `context.FileType`: file type, one of CSV, XLSX, JSON, HTML\n</code></pre> <p>Any file reader configuration must be passed in <code>params</code>.</p> <ul> <li>Rest (bidirectional)</li> </ul> <p>Rest connections consist of two parts: Client and Request. Client contains   base URL and authentication (basic, digest, header and oauth2 are supported).</p> <p><code>- name: TheRest     kind: source     connection:       kind: Rest       client:         base_url: https://api.example.com         auth:  # Every request to given endpoint share the same authentication           method: basic           values:             username: TheName             password: ThePassword       request:         endpoint: /v1/some-path         content_type: application/json         method: post         query:  # Query parameters           date: &lt;&lt;API.dt(None).isoformat()&gt;&gt;         body:  # JSON payload           param: 1         response:           content_type: application/json           locator: \"JMESPath.to.data\"</code></p> <p>Any custom source (API) can be used as long as available via module.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.context","title":"context","text":"<pre><code>context: Optional[FlowContext] = None\n</code></pre> <p>@private Global context passed to given stage</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.explain","title":"explain","text":"<pre><code>explain: Optional[bool] = None\n</code></pre> <p>If set to true then SQL query explanation will be logged.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.fields","title":"fields","text":"<pre><code>fields: Optional[List[Column]] = None\n</code></pre> <p>Explicitly defined output fields.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.kind","title":"kind","text":"<pre><code>kind: str\n</code></pre> <p>Defines which action will be performed:</p> <ul> <li>source</li> <li>transform</li> <li>sink</li> <li>pipeline</li> <li>sql</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.kind--sink","title":"Sink","text":"<p>Sink reads output from previous stage and stores in specified location.</p> <p>NOTE: If query is supplied with the stage then sink uses output of the query rather than preceding stage directly. Subsequent sink will use preceding stage. If supplied query must create either view or table with same name as current stage.</p> <pre><code>- name: my_sink\nkind: sink\nconnection:\n    kind: Variable\nquery: &gt;\n    CREATE VIEW my_sink AS\n    SELECT\n    1 as foo\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.kind--pipeline","title":"Pipeline","text":"<p>If pipeline is preceded by any stage then the subpipeline will be executed as many times as there are rows in the previous stage output. This is useful if you want to run same pipeline with different parameters. Make sure the pipeline is preceded by source or transform producing required number of rows. If you need to run subpipeline only once there are two options:</p> <ol> <li>Place it to the top</li> <li>Preced with tranform producing single row only</li> </ol> <p>Each row is then passed to subpipeline in a special variable.</p> <p>Example pipeline iterating subpipeline 5 times:</p> <ul> <li>kind: transform       name: generate_5       # Generate 5 rows       query: &gt;           select unnest as current_variable from unnest(generate_series(1,5))       show: 5</li> <li>kind: pipeline     name: looped_5x     stages:         - kind: transform           name: inside_loop           # In query we can reference the value passed from parent pipeline           query: &gt;               select 'Currently running iteration: {API.look('loop_control.current_variable', variables)}' as value           show: 5</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.locator","title":"locator","text":"<pre><code>locator: Optional[str] = None\n</code></pre> <p>Currently unused: Name for the connection configuration: name, or URI.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.log_level","title":"log_level","text":"<pre><code>log_level: Optional[LogLevel] = None\n</code></pre> <p>Set logging level. All stages after (including current) will log with specified level. Possible values: INFO (default), DEBUG, WARNING. Log level will be reset to INFO after each pipeline (including nested pipelines).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.name","title":"name","text":"<pre><code>name: str\n</code></pre> <p>Name of the stage, must be unique across all stages in the pipeline and conform to the rules: Must start with letter, may contain lowercase letters, number and underscores. Name is used to reference this stage by other subsequent stages.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.on_error","title":"on_error","text":"<pre><code>on_error: str = 'fail'\n</code></pre> <p>If set to 'continue' then pipeline will not fail. Subsequent stages referring to failed one must handle missing data.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.query","title":"query","text":"<pre><code>query: Optional[str] = None\n</code></pre> <p>SQL Query or dictionary with custom source parameters. May contain {dynamic variables}.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.show","title":"show","text":"<pre><code>show: int = 0\n</code></pre> <p>If set to positive integer then given number of rows from this stage will get logged. If set to -1 then all rows will be loggged. Set to 0 to disable logging.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If True then schema is logged</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.skip_if","title":"skip_if","text":"<pre><code>skip_if: Optional[Any] = None\n</code></pre> <p>Any value that can evaluated using bool(). or template string e.g. <code>&lt;&lt; True &gt;&gt;</code>. When the expression evaluates to True then the stage is skipped.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.stages","title":"stages","text":"<pre><code>stages: Optional[Stages] = None\n</code></pre> <p>Used when kind is <code>Flow</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stage.throttle","title":"throttle","text":"<pre><code>throttle: Optional[Union[int, float]] = None\n</code></pre> <p>If set to positive value then flow execution will be paused after the stage for given number of seconds.</p> <p>Useful when dealing with rate limits or otherwise spreading the load over time.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stages","title":"Stages","text":"<p>               Bases: <code>RootModel[List[Stage]]</code></p> <p>A sequence of processing stages in a data pipeline.</p> <p>Each <code>Datablock</code> in <code>root</code> represents one discrete step or transformation in an end-to-end business workflow (e.g., tap, transform, sink, validate).</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>List[Datablock]</code> <p>Ordered list of pipeline stages to execute.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.Stages.enumerate_steps","title":"enumerate_steps","text":"<pre><code>enumerate_steps() -&gt; Iterator[tuple[int, Stage]]\n</code></pre> <p>Yield each stage along with its 0-based position.</p> <p>Use this when you need both the stage and its index for logging, metrics, or conditional branching.</p> <p>Returns:</p> Type Description <code>Iterator[tuple[int, Stage]]</code> <p>Iterator[Tuple[int, Datablock]]: Pairs of (index, stage).</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>def enumerate_steps(self) -&gt; t.Iterator[tuple[int, Stage]]:\n    \"\"\"Yield each stage along with its 0-based position.\n\n    Use this when you need both the stage and its index for logging,\n    metrics, or conditional branching.\n\n    Returns:\n        Iterator[Tuple[int, Datablock]]: Pairs of (index, stage).\n    \"\"\"\n    return enumerate(self.root)\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.Stages.load","title":"load","text":"<pre><code>load(source: Union[str, Path, IO[str], Loadable]) -&gt; Stages\n</code></pre> <p>Load a pipeline from YAML (path, YAML-string, file-like or Loadable).</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | Path | IO[str] | Loadable</code> <ul> <li>Path to a .yaml file</li> <li>Raw YAML content</li> <li>File-like object returning YAML</li> <li>Any object with a <code>.load()</code> method returning Python data</li> </ul> required <p>Returns:</p> Name Type Description <code>Stages</code> <code>Stages</code> <p>a validated <code>Stages</code> instance.</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    source: t.Union[str, Path, t.IO[str], Loadable],\n) -&gt; \"Stages\":\n    \"\"\"Load a pipeline from YAML (path, YAML-string, file-like or Loadable).\n\n    Args:\n        source (str | Path | IO[str] | Loadable):\n            - Path to a .yaml file\n            - Raw YAML content\n            - File-like object returning YAML\n            - Any object with a `.load()` method returning Python data\n\n    Returns:\n        Stages: a validated `Stages` instance.\n    \"\"\"\n    # 1) If it\u2019s a loader-object, pull Python data directly\n    if isinstance(source, Loadable):\n        data = source.load()\n\n    else:\n        # 2) Read text for YAML parsing:\n        if hasattr(source, \"read\"):\n            text = t.cast(t.IO[str], source).read()\n        else:\n            text = str(source)\n\n        # 3) First, try parsing as raw YAML\n        try:\n            data = yaml.safe_load(text)\n        except yaml.YAMLError:\n            data = None\n\n        # 4) Only if that parse returned a `str` we treat it as a filename\n        if isinstance(data, str):\n            try:\n                text = Path(data).read_text()\n                data = yaml.safe_load(text)\n            except (OSError, yaml.YAMLError) as e:\n                raise ValueError(\n                    f\"Could not interpret {data!r} as YAML or file path\"\n                ) from e\n\n    # 5) Validate final shape\n    if not isinstance(data, list):\n        raise ValueError(\n            f\"Expected a list of pipeline stages, got {type(data).__name__}\"\n        )\n\n    # 5) Finally, validate into our model\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.Stages.steps","title":"steps","text":"<pre><code>steps() -&gt; Iterator[Stage]\n</code></pre> <p>Yield each stage in execution order.</p> <p>Returns:</p> Type Description <code>Iterator[Stage]</code> <p>Iterator[Datablock]: An iterator over the stages,</p> <code>Iterator[Stage]</code> <p>from first to last.</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>def steps(self) -&gt; t.Iterator[Stage]:\n    \"\"\"Yield each stage in execution order.\n\n    Returns:\n        Iterator[Datablock]: An iterator over the stages,\n        from first to last.\n    \"\"\"\n    return iter(self.root)\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.Variables","title":"Variables","text":"<p>               Bases: <code>dict</code></p> <p>Variables is a <code>dict</code>-based collection storing arbitrary data under keys. Variable can be populated via:</p> <ul> <li>Dictionary passed to pipeline:   <code>Flow(defs, context, variables={'foo': 'bar'})</code></li> <li>Sink operation:<pre><code>- kind: sink\n\n    name: my_sink\n\n    connection:\n\n        kind: Variable\n\n        locator: variable_name\n</code></pre> </li> </ul> <p>This step will place a list of records from preceding step in   variables as <code>{'variable_name': t.List[dict]}</code></p> <p>In most cases sainking to variable is not needed as all preceding stages in a pipeline can be referenced via <code>name</code>. Sinking to variable is useful when you need to share data with subpipeline.</p> <p>Special variable is <code>loop_control</code> which is populated dynamically from previous step and current stage type is pipeline. In case previous step generates multiple records then the new pipeline is called for each record, and control variable holds a <code>dict</code> representing the current record.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.components","title":"components","text":""},{"location":"api/ankaflow.models/#ankaflow.models.components.Column","title":"Column","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data column (equivalent to database column)</p>"},{"location":"api/ankaflow.models/#ankaflow.models.components.Column.name","title":"name","text":"<pre><code>name: str\n</code></pre> <p>Column name must follow rules set to SQL engine column names</p>"},{"location":"api/ankaflow.models/#ankaflow.models.components.Column.type","title":"type","text":"<pre><code>type: str\n</code></pre> <p>Any data type support by SQL engine</p>"},{"location":"api/ankaflow.models/#ankaflow.models.components.Columns","title":"Columns","text":"<p>               Bases: <code>RootModel[List[Column]]</code></p> <p>Iterable list-like collection of Fields.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs","title":"configs","text":""},{"location":"api/ankaflow.models/#ankaflow.models.configs.BigQueryConfig","title":"BigQueryConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for accessing BigQuery datasets.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.BigQueryConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to service account JSON credentials for BigQuery access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.BigQueryConfig.dataset","title":"dataset","text":"<pre><code>dataset: Optional[str] = None\n</code></pre> <p>BigQuery dataset name.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.BigQueryConfig.project","title":"project","text":"<pre><code>project: Optional[str] = None\n</code></pre> <p>GCP project ID containing the BigQuery dataset.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.BigQueryConfig.region","title":"region","text":"<pre><code>region: Optional[str] = None\n</code></pre> <p>BigQuery region (e.g., \"us-central1\").</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.BucketConfig","title":"BucketConfig","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.BucketConfig.bucket","title":"bucket","text":"<pre><code>bucket: Optional[str] = None\n</code></pre> <p>Bucket name eg. <code>s3://my-bucket</code> or local absolute path eg. <code>/data/path</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.BucketConfig.data_prefix","title":"data_prefix","text":"<pre><code>data_prefix: Optional[str] = None\n</code></pre> <p>Prefix for data files: <code>s3://my-bucket/&lt;prefix&gt;</code> or <code>/data/path/&lt;prefix&gt;</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.BucketConfig.locator_wildcard","title":"locator_wildcard","text":"<pre><code>locator_wildcard: Optional[Tuple] = None\n</code></pre> <p>Regular expression and wildcard to modify <code>locator</code>. Useful in cases when you sink data to <code>data-YYYY.parquet</code> but want to read <code>data-*.parquet</code>. Provide tuple with patten as first element and replacement as second one. Example:</p> <pre><code>('-\\d{4}-', '*')\n</code></pre> <p>This will create wildcard for <code>data-YYYY-base.parquet</code> as <code>data*base.parquet</code>.</p> Wildcard Description <code>*</code> matches any number of any characters (including none) <code>**</code> matches any number of subdirectories (including none) <code>[abc]</code> matches one character given in the bracket <code>[a-z]</code> matches one character from the range given in the bracket <p>Wildcard is automatically applied in tap and show_schema operations.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.BucketConfig.region","title":"region","text":"<pre><code>region: str | None = None\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.ClickhouseConfig","title":"ClickhouseConfig","text":"<p>               Bases: <code>DatabaseConfig</code></p> <p>Configuration for ClickHouse database, extending generic SQL config.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.ClickhouseConfig.blocksize","title":"blocksize","text":"<pre><code>blocksize: int = 50000\n</code></pre> <p>Number of rows to process per block for batch operations.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.ConnectionConfiguration","title":"ConnectionConfiguration","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level container for all connection configurations in a pipeline.</p> <p>Includes default configuration blocks for supported sources and sinks like: - Local file systems - S3 and GCS buckets - BigQuery datasets - ClickHouse databases</p> <p>These can be customized per pipeline or extended with new sources.</p> Example <p>To support MySQL, you can define a new model like this:</p> <pre><code>class MySQLConfig(DatabaseConfig):\n    pass  # You can add MySQL-specific fields here if needed\n\nclass ExtendedConnectionConfiguration(ConnectionConfiguration):\n    mysql: MySQLConfig = PydanticField(default_factory=MySQLConfig)\n</code></pre> <p>This will allow you to specify MySQL connection settings in your YAML:</p> <pre><code>connection:\n  kind: MySQL\n  config:\n    host: localhost\n    port: 3306\n    database: sales\n    username: admin\n    password: secret\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.ConnectionConfiguration.bigquery","title":"bigquery","text":"<pre><code>bigquery: BigQueryConfig = Field(\n    default_factory=BigQueryConfig\n)\n</code></pre> <p>BigQuery connection configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.ConnectionConfiguration.clickhouse","title":"clickhouse","text":"<pre><code>clickhouse: ClickhouseConfig = Field(\n    default_factory=ClickhouseConfig\n)\n</code></pre> <p>Language model configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.ConnectionConfiguration.gs","title":"gs","text":"<pre><code>gs: GSConfig = Field(default_factory=GSConfig)\n</code></pre> <p>Google Cloud Storage configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.ConnectionConfiguration.llm","title":"llm","text":"<pre><code>llm: LLMConfig = Field(default_factory=LLMConfig)\n</code></pre> <p>Language model configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.ConnectionConfiguration.local","title":"local","text":"<pre><code>local: BucketConfig = Field(default_factory=BucketConfig)\n</code></pre> <p>Local file system configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.ConnectionConfiguration.s3","title":"s3","text":"<pre><code>s3: S3Config = Field(default_factory=S3Config)\n</code></pre> <p>S3 cloud storage configuration.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.DatabaseConfig","title":"DatabaseConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for SQL database connection configurations.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.DatabaseConfig.cluster","title":"cluster","text":"<pre><code>cluster: Optional[str] = None\n</code></pre> <p>Optional cluster name or identifier (used in ClickHouse and other distributed systems).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.DatabaseConfig.database","title":"database","text":"<pre><code>database: Optional[str] = None\n</code></pre> <p>Database name.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.DatabaseConfig.host","title":"host","text":"<pre><code>host: Optional[str] = None\n</code></pre> <p>Hostname or IP address of the database server.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.DatabaseConfig.password","title":"password","text":"<pre><code>password: Optional[str] = None\n</code></pre> <p>Password for authentication.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.DatabaseConfig.port","title":"port","text":"<pre><code>port: Optional[int | str] = None\n</code></pre> <p>Database port.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.DatabaseConfig.username","title":"username","text":"<pre><code>username: Optional[str] = None\n</code></pre> <p>Username for authentication.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.GSConfig","title":"GSConfig","text":"<p>               Bases: <code>BucketConfig</code></p> <p>Google Cloud Storage (GCS) configuration.</p> <ul> <li>HMAC credentials allow reading/writing Parquet, JSON, and CSV.</li> <li>Delta table writes require a full service account credential file.</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.GSConfig.credential_file","title":"credential_file","text":"<pre><code>credential_file: Optional[str] = None\n</code></pre> <p>Path to GCP service account credential file (JSON). Required for certain write operations (e.g., Delta tables).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.GSConfig.hmac_key","title":"hmac_key","text":"<pre><code>hmac_key: Optional[str] = None\n</code></pre> <p>GCS HMAC key ID for authenticated access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.GSConfig.hmac_secret","title":"hmac_secret","text":"<pre><code>hmac_secret: Optional[str] = None\n</code></pre> <p>GCS HMAC secret for authenticated access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.S3Config","title":"S3Config","text":"<p>               Bases: <code>BucketConfig</code></p> <p>S3-specific bucket configuration including optional authentication.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.S3Config.access_key_id","title":"access_key_id","text":"<pre><code>access_key_id: Optional[str] = None\n</code></pre> <p>AWS access key ID for private S3 access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.configs.S3Config.secret_access_key","title":"secret_access_key","text":"<pre><code>secret_access_key: Optional[str] = None\n</code></pre> <p>AWS secret access key for private S3 access.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections","title":"connections","text":""},{"location":"api/ankaflow.models/#ankaflow.models.connections.BigQueryConnection","title":"BigQueryConnection","text":"<p>               Bases: <code>PhysicalConnection</code>, <code>VersionedConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.BigQueryConnection.data_mode","title":"data_mode","text":"<pre><code>data_mode: str = 'error'\n</code></pre> <p>Data mode for write operation. For Deltatable valid options are:</p> <ul> <li><code>append</code> adds new data</li> <li><code>overwrite</code> replaces all data</li> <li><code>error</code> fails (table is read-only)</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.BigQueryConnection.key","title":"key","text":"<pre><code>key: Optional[List[str]] = None\n</code></pre> <p>List of versioned fields</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.BigQueryConnection.kind","title":"kind","text":"<pre><code>kind: Literal['BigQuery']\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.BigQueryConnection.partition","title":"partition","text":"<pre><code>partition: Optional[List[str]] = None\n</code></pre> <p>If set then delta table is partitioned using specified fields for faster reads</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.BigQueryConnection.schema_mode","title":"schema_mode","text":"<pre><code>schema_mode: Optional[str] = None\n</code></pre> <p>Deltatable schema behaviour. If not set then write fails with error in case the data schema does not match existing table schema.</p> <p>Schema evolution options:</p> <ul> <li><code>merge</code> adds new columns from data</li> <li><code>overwrite</code> adds new columns, drops missing</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.BigQueryConnection.version","title":"version","text":"<pre><code>version: Optional[str] = None\n</code></pre> <p>Field for record version timestamp</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.CSVConnection","title":"CSVConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.ClickhouseConnection","title":"ClickhouseConnection","text":"<p>               Bases: <code>PhysicalConnection</code>, <code>VersionedConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.ClickhouseConnection.key","title":"key","text":"<pre><code>key: Optional[List[str]] = None\n</code></pre> <p>List of versioned fields</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.ClickhouseConnection.kind","title":"kind","text":"<pre><code>kind: Literal['Clickhouse']\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.ClickhouseConnection.version","title":"version","text":"<pre><code>version: Optional[str] = None\n</code></pre> <p>Field for record version timestamp</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.Connection","title":"Connection","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.Connection.config","title":"config","text":"<pre><code>config: Optional[ConnectionConfiguration] = None\n</code></pre> <p>Optional configuration for the current connection. If not present then global configuration will be used.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.Connection.create_statement","title":"create_statement","text":"<pre><code>create_statement: str | None = None\n</code></pre> <p>Create statement for given table. Must be in requested dialect.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.Connection.fields","title":"fields","text":"<pre><code>fields: Optional[Columns] = None\n</code></pre> <p>If set then schema is used to generate source structure in case actual source does not provide data in which case generation of ephemeral view fails.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.Connection.kind","title":"kind","text":"<pre><code>kind: str\n</code></pre> <p>Model type e.g. Deltatable, Bigquery, Clickhouse, Parquet, File Custom connections can be loaded from module.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.Connection.params","title":"params","text":"<pre><code>params: Optional[dict] = Field(default_factory=dict)\n</code></pre> <p>Any parameters that can be passed to connection.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.Connection.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If true then schema is automatically detected from the input data and logged '</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.CustomConnection","title":"CustomConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p> <p>Custom connection provider. Custom connection may implement its own logic but must derive from base Connection class, and expose tap(), sink(), sql() and show_schema() even if they are no-op.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.CustomConnection.classname","title":"classname","text":"<pre><code>classname: str\n</code></pre> <p>Name of the class to load from the module</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.CustomConnection.module","title":"module","text":"<pre><code>module: str\n</code></pre> <p>Python module where the connection class is defined</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.CustomConnection.params","title":"params","text":"<pre><code>params: Optional[dict] = Field(default_factory=dict)\n</code></pre> <p>Free-form configuration parameters passed to the loaded class</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.CustomConnection.load","title":"load","text":"<pre><code>load() -&gt; Type[Connection]\n</code></pre> <p>Dynamically load the connection class from the given module and member.</p> <p>Returns:</p> Type Description <code>Type[Connection]</code> <p>A subclass of Connection.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module or member can't be imported.</p> Source code in <code>ankaflow/models/connections.py</code> <pre><code>def load(self) -&gt; t.Type[Connection]:\n    \"\"\"\n    Dynamically load the connection class from the given module and member.\n\n    Returns:\n        A subclass of Connection.\n\n    Raises:\n        ImportError: If the module or member can't be imported.\n    \"\"\"\n    mod = importlib.import_module(self.module)\n    try:\n        cls = getattr(mod, self.classname)\n    except AttributeError as e:\n        raise ImportError(\n            f\"Could not load '{self.classname}' from module '{self.module}'\"\n        ) from e\n    # TODO: Refactor imports to facilitate this check early\n    # if not issubclass(cls, Connection):\n    #     raise TypeError(\n    #         f\"{cls.__name__} is not a subclass of Connection\"\n    #     )\n\n    return cls\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.DeltatableConnection","title":"DeltatableConnection","text":"<p>               Bases: <code>PhysicalConnection</code>, <code>VersionedConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.DeltatableConnection.data_mode","title":"data_mode","text":"<pre><code>data_mode: str = 'error'\n</code></pre> <p>Data mode for write operation. For Deltatable valid options are:</p> <ul> <li><code>append</code> adds new data</li> <li><code>overwrite</code> replaces all data</li> <li><code>error</code> fails (table is read-only)</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.DeltatableConnection.kind","title":"kind","text":"<pre><code>kind: Literal['Deltatable']\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.DeltatableConnection.optimize","title":"optimize","text":"<pre><code>optimize: Optional[Union[str, int]] = 1\n</code></pre> <p>Use with Deltatable and other engines whether to optimize after each sink operation. With larger tables this may be a lengthy synchronous operation.</p> <p>Default value is optimize and vacuum with 7 day retention.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.DeltatableConnection.optimize--deltatable","title":"Deltatable","text":"<p>Values are <code>optimize,vacuum,all,Literal[int]</code>.   If value is literal int is provided then parts older than   number of days will be removed. Note that this will override   default retention days.</p> <p>String options <code>vacuum,all</code> are equivalent to 0.</p> <p>Delta connection also supports raw SQL optimization. Pass statement as sql command:</p> <p><code>OPTIMIZE Deltatable [COMPACT] [VACUUM] [AGE=&lt;int&gt;[d|h]] [DRY_RUN] [CLEANUP]</code></p> <p><code>optimize deltatable</code> \u2192 compact + vacuum with default 7 days</p> <p><code>optimize deltatable compact</code> \u2192 compact only</p> <p><code>optimize deltatable vacuum age=36h dry_run</code> \u2192 list files older than 36 hours, don't delete</p> <p><code>optimize deltatable compact vacuum age=1 cleanup</code> \u2192 compact, vacuum 1 day, then cleanup metadata</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.DeltatableConnection.partition","title":"partition","text":"<pre><code>partition: Optional[List[str]] = None\n</code></pre> <p>If set then delta table is partitioned using specified fields for faster reads</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.DeltatableConnection.schema_mode","title":"schema_mode","text":"<pre><code>schema_mode: Optional[str] = None\n</code></pre> <p>Deltatable schema behaviour. If not set then write fails with error in case the data schema does not match existing table schema.</p> <p>Schema evolution options:</p> <ul> <li><code>merge</code> adds new columns from data</li> <li><code>overwrite</code> adds new columns, drops missing</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.DeltatableConnection.writer_features","title":"writer_features","text":"<pre><code>writer_features: Optional[List] | None = None\n</code></pre> <p>Any supported Delta-rs parameters can be passed to the writer. Example:</p> <pre><code>writer_features: [TimestampWithoutTimezone]\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.Dimension","title":"Dimension","text":"<p>               Bases: <code>EphemeralConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.Dimension.load","title":"load","text":"<pre><code>load() -&gt; Type[Connection]\n</code></pre> <p>Dynamically load the connection class from the given module and member.</p> <p>Returns:</p> Type Description <code>Type[Connection]</code> <p>A subclass of Connection.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module or member can't be imported.</p> Source code in <code>ankaflow/models/connections.py</code> <pre><code>def load(self) -&gt; t.Type[Connection]:\n    \"\"\"\n    Dynamically load the connection class from the given module and member.\n\n    Returns:\n        A subclass of Connection.\n\n    Raises:\n        ImportError: If the module or member can't be imported.\n    \"\"\"\n    mod = importlib.import_module(self.module)\n    try:\n        cls = getattr(mod, self.kind)\n    except AttributeError as e:\n        raise ImportError(\n            f\"Could not load '{self.kind}' from module '{self.module}'\"\n        ) from e\n\n    return cls\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.FileConnection","title":"FileConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.JSONConnection","title":"JSONConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.ParquetConnection","title":"ParquetConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.PhysicalConnection","title":"PhysicalConnection","text":"<p>               Bases: <code>Connection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.PhysicalConnection.locator","title":"locator","text":"<pre><code>locator: str\n</code></pre> <p>Table name or file name or URI, or other identifier required by the connection.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.PhysicalConnection.raw_dispatch","title":"raw_dispatch","text":"<pre><code>raw_dispatch: bool | None = None\n</code></pre> <p>If True, sends the provided SQL query directly without altering locators if fully qualified, or adding FROM clauses. The query must be a valid SELECT statement. Useful for full control over complex queries.</p> <p>In Deltatable and Parquet connections supports rewriting short locators as convenience.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.RestConnection","title":"RestConnection","text":"<p>               Bases: <code>EphemeralConnection</code></p> <p>Configuration for a REST-based data connection.</p> <p>This model defines how to configure and execute REST API requests as part of a pipeline step. It includes the request definition, client behavior (e.g., retries, headers), and optional schema discovery.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.RestConnection.client","title":"client","text":"<pre><code>client: RestClientConfig\n</code></pre> <p>Configuration for the REST client (e.g., base URL, headers, auth).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.RestConnection.fields","title":"fields","text":"<pre><code>fields: Optional[Columns] = None\n</code></pre> <p>Optional schema definition used to validate or transform response data.</p> <p>It is recommended to manually specify the schema after initial discovery. This ensures downstream pipeline stages remain stable, even when the remote API returns no results (e.g., due to no updates in an incremental fetch). Explicit schema prevents silent failures or schema drift in such cases.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.RestConnection.kind","title":"kind","text":"<pre><code>kind: Literal['Rest']\n</code></pre> <p>Specifies the connection type as \"Rest\".</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.RestConnection.request","title":"request","text":"<pre><code>request: Request\n</code></pre> <p>Request template specifying method, path, body, etc.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.RestConnection.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If True, the connector will attempt to infer or display the response schema automatically.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.SQLGenConnection","title":"SQLGenConnection","text":"<p>               Bases: <code>EphemeralConnection</code></p> <p>SQLGen connection is intended for SQL code generation:</p> <p>Prompt should instruct the model to generate SQL query and and executes resutling a VIEW in the internal database.</p> <p>Example:</p> <pre><code>Stage 1: Name: ReadSomeParquetData\n\nStage 2: Name: CodeGen, query: Given SQL table `ReadSomeParquetData` generate SQL query to\n    count number of rows in the table.\n\nInside stage 2 the following happens:\n\n1. Prompt is sent to inferecne endpoint\n\n2. Endpoint is expected to respond with valid SQL\n\n3. Connection will execute a statement `CREATE OR REPLACE VIEW StageName AS &lt;received_select_statement&gt;`\n    where statement in the exmaple is likely `SELECT COUNT() FROM ReadSomeParquetData`\n</code></pre> <p>.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.SQLGenConnection.kind","title":"kind","text":"<pre><code>kind: Literal['SQLGen']\n</code></pre> <p>Specifies the <code>kind==SQLGen</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.SQLGenConnection.variables","title":"variables","text":"<pre><code>variables: dict | None = None\n</code></pre> <p>Variables passed to Prompt. Prompt must be supplied in the <code>query</code> field of the <code>Stage</code>.</p> <p>Prompt may contain Jinja2-style placeholders:</p> Example <p><code>Here's my name: {{name}}.</code></p> <p>The connection will render the prompt template using <code>variables</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.VariableConnection","title":"VariableConnection","text":"<p>               Bases: <code>PhysicalConnection</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.VersionedConnection","title":"VersionedConnection","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.VersionedConnection.key","title":"key","text":"<pre><code>key: Optional[List[str]] = None\n</code></pre> <p>List of versioned fields</p>"},{"location":"api/ankaflow.models/#ankaflow.models.connections.VersionedConnection.version","title":"version","text":"<pre><code>version: Optional[str] = None\n</code></pre> <p>Field for record version timestamp</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core","title":"core","text":""},{"location":"api/ankaflow.models/#ankaflow.models.core.FlowContext","title":"FlowContext","text":"<p>               Bases: <code>ImmutableMap</code></p> <p>Context dictionary can be used to supply arbitrary data to pipeline that can be referenced in templates much much like variables with the difference that they cannot be modified at runtime.</p> <p>Context can be initiated via dictionary:</p> <p><code>context = FlowContext(**{'foo':'Bar'}])</code></p> <p>Items can be reference using both bracket and dot notation:</p> <p><code>context.foo == context['foo'] == 'Bar'</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage","title":"Stage","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.connection","title":"connection","text":"<pre><code>connection: Optional[\n    Union[\n        RestConnection,\n        VariableConnection,\n        BigQueryConnection,\n        DeltatableConnection,\n        ParquetConnection,\n        ClickhouseConnection,\n        CustomConnection,\n        JSONConnection,\n        CSVConnection,\n        FileConnection,\n        SQLGenConnection,\n        Dimension,\n    ]\n] = Field(None, discriminator=\"kind\")\n</code></pre> <p>Defines how the data is read from / written to the target.</p> <p>Connection fields may contain templates and they will be recursively.</p> <p>Special construct is JSON&gt; which allows dynamically generating parameters as runtime:</p> <pre><code>- kind: source\n  name: source_name\n  connection:\n    kind: File\n    params: &gt;\n      JSON&gt;{\n        \"key\": \"value\",\n        \"dynkey\": &lt;&lt;API.property&gt;&gt;,\n      }\n</code></pre> <p>In the above app <code>params</code> are constructed as JSON string. It is possible to even construct parameter keys dynamically:</p> <pre><code>params: &gt;\n  JSON&gt;\n  {\n    &lt;% for number in [1,2,3] %&gt;\n    \"key_&lt;&lt; number &gt;&gt;\":&lt;&lt; number &gt;&gt;&lt;% if not loop.last %&gt;,&lt;% endif %&gt;\n    &lt;% endfor %&gt;\n  }\n</code></pre> <p>Above example results the following:</p> <pre><code>params: &gt;\n  {\n    \"key_1\": 1,\n    \"key_2\": 2,\n    \"key_3\": 3\n  }\n</code></pre> <p>JSON&gt; structure cannot contain nested JSON&gt; structures, the entire string following the JSON&gt; header must result a valid JSON.</p> <p>Inbuilt connections include:</p> <ul> <li>Deltatable (read)</li> </ul> <p>If connection is <code>Deltatable</code> then query is required to narrow   down data stored in the delta table. <code>FROM</code> clause must be <code>Deltatable</code>: <code>- kind: source     name: delta_tap     connection:       kind: Deltatable       locator: delta_table     query: &gt;       select * from Deltatable</code></p> <ul> <li>Deltatable (write)</li> </ul> <p>See also StorageOptions</p> <p>The following example writes data from preceding stage   to delta table, appending the data, partitions using   <code>part</code> column, and optimizes and vacuums immediately without   retention after write.   <code>- kind: sink     name: delta_sink     connection:       kind: Deltatable       locator: delta_table       optimize: 0       data_mode: append       partition:       - part_field</code></p> <ul> <li>Parquet (read/write)</li> <li>JSON (read/write; NOTE: write operation generates newline-delimited JSON)</li> <li>CSV (read/write)</li> <li> <p>Variable (read/write)</p> </li> <li> <p>File (read)</p> </li> </ul> <p>File can be read from a connected filesystem (including s3). File name   and file type must be specified in the pipeline context:</p> <pre><code>- `context.FileName`: file name relative to `file_prefix`\n- `context.FileType`: file type, one of CSV, XLSX, JSON, HTML\n</code></pre> <p>Any file reader configuration must be passed in <code>params</code>.</p> <ul> <li>Rest (bidirectional)</li> </ul> <p>Rest connections consist of two parts: Client and Request. Client contains   base URL and authentication (basic, digest, header and oauth2 are supported).</p> <p><code>- name: TheRest     kind: source     connection:       kind: Rest       client:         base_url: https://api.example.com         auth:  # Every request to given endpoint share the same authentication           method: basic           values:             username: TheName             password: ThePassword       request:         endpoint: /v1/some-path         content_type: application/json         method: post         query:  # Query parameters           date: &lt;&lt;API.dt(None).isoformat()&gt;&gt;         body:  # JSON payload           param: 1         response:           content_type: application/json           locator: \"JMESPath.to.data\"</code></p> <p>Any custom source (API) can be used as long as available via module.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.context","title":"context","text":"<pre><code>context: Optional[FlowContext] = None\n</code></pre> <p>@private Global context passed to given stage</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.explain","title":"explain","text":"<pre><code>explain: Optional[bool] = None\n</code></pre> <p>If set to true then SQL query explanation will be logged.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.fields","title":"fields","text":"<pre><code>fields: Optional[List[Column]] = None\n</code></pre> <p>Explicitly defined output fields.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.kind","title":"kind","text":"<pre><code>kind: str\n</code></pre> <p>Defines which action will be performed:</p> <ul> <li>source</li> <li>transform</li> <li>sink</li> <li>pipeline</li> <li>sql</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.kind--sink","title":"Sink","text":"<p>Sink reads output from previous stage and stores in specified location.</p> <p>NOTE: If query is supplied with the stage then sink uses output of the query rather than preceding stage directly. Subsequent sink will use preceding stage. If supplied query must create either view or table with same name as current stage.</p> <pre><code>- name: my_sink\nkind: sink\nconnection:\n    kind: Variable\nquery: &gt;\n    CREATE VIEW my_sink AS\n    SELECT\n    1 as foo\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.kind--pipeline","title":"Pipeline","text":"<p>If pipeline is preceded by any stage then the subpipeline will be executed as many times as there are rows in the previous stage output. This is useful if you want to run same pipeline with different parameters. Make sure the pipeline is preceded by source or transform producing required number of rows. If you need to run subpipeline only once there are two options:</p> <ol> <li>Place it to the top</li> <li>Preced with tranform producing single row only</li> </ol> <p>Each row is then passed to subpipeline in a special variable.</p> <p>Example pipeline iterating subpipeline 5 times:</p> <ul> <li>kind: transform       name: generate_5       # Generate 5 rows       query: &gt;           select unnest as current_variable from unnest(generate_series(1,5))       show: 5</li> <li>kind: pipeline     name: looped_5x     stages:         - kind: transform           name: inside_loop           # In query we can reference the value passed from parent pipeline           query: &gt;               select 'Currently running iteration: {API.look('loop_control.current_variable', variables)}' as value           show: 5</li> </ul>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.locator","title":"locator","text":"<pre><code>locator: Optional[str] = None\n</code></pre> <p>Currently unused: Name for the connection configuration: name, or URI.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.log_level","title":"log_level","text":"<pre><code>log_level: Optional[LogLevel] = None\n</code></pre> <p>Set logging level. All stages after (including current) will log with specified level. Possible values: INFO (default), DEBUG, WARNING. Log level will be reset to INFO after each pipeline (including nested pipelines).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.name","title":"name","text":"<pre><code>name: str\n</code></pre> <p>Name of the stage, must be unique across all stages in the pipeline and conform to the rules: Must start with letter, may contain lowercase letters, number and underscores. Name is used to reference this stage by other subsequent stages.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.on_error","title":"on_error","text":"<pre><code>on_error: str = 'fail'\n</code></pre> <p>If set to 'continue' then pipeline will not fail. Subsequent stages referring to failed one must handle missing data.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.query","title":"query","text":"<pre><code>query: Optional[str] = None\n</code></pre> <p>SQL Query or dictionary with custom source parameters. May contain {dynamic variables}.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.show","title":"show","text":"<pre><code>show: int = 0\n</code></pre> <p>If set to positive integer then given number of rows from this stage will get logged. If set to -1 then all rows will be loggged. Set to 0 to disable logging.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.show_schema","title":"show_schema","text":"<pre><code>show_schema: Optional[bool] = None\n</code></pre> <p>If True then schema is logged</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.skip_if","title":"skip_if","text":"<pre><code>skip_if: Optional[Any] = None\n</code></pre> <p>Any value that can evaluated using bool(). or template string e.g. <code>&lt;&lt; True &gt;&gt;</code>. When the expression evaluates to True then the stage is skipped.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.stages","title":"stages","text":"<pre><code>stages: Optional[Stages] = None\n</code></pre> <p>Used when kind is <code>Flow</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stage.throttle","title":"throttle","text":"<pre><code>throttle: Optional[Union[int, float]] = None\n</code></pre> <p>If set to positive value then flow execution will be paused after the stage for given number of seconds.</p> <p>Useful when dealing with rate limits or otherwise spreading the load over time.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stages","title":"Stages","text":"<p>               Bases: <code>RootModel[List[Stage]]</code></p> <p>A sequence of processing stages in a data pipeline.</p> <p>Each <code>Datablock</code> in <code>root</code> represents one discrete step or transformation in an end-to-end business workflow (e.g., tap, transform, sink, validate).</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>List[Datablock]</code> <p>Ordered list of pipeline stages to execute.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stages.enumerate_steps","title":"enumerate_steps","text":"<pre><code>enumerate_steps() -&gt; Iterator[tuple[int, Stage]]\n</code></pre> <p>Yield each stage along with its 0-based position.</p> <p>Use this when you need both the stage and its index for logging, metrics, or conditional branching.</p> <p>Returns:</p> Type Description <code>Iterator[tuple[int, Stage]]</code> <p>Iterator[Tuple[int, Datablock]]: Pairs of (index, stage).</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>def enumerate_steps(self) -&gt; t.Iterator[tuple[int, Stage]]:\n    \"\"\"Yield each stage along with its 0-based position.\n\n    Use this when you need both the stage and its index for logging,\n    metrics, or conditional branching.\n\n    Returns:\n        Iterator[Tuple[int, Datablock]]: Pairs of (index, stage).\n    \"\"\"\n    return enumerate(self.root)\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stages.load","title":"load","text":"<pre><code>load(source: Union[str, Path, IO[str], Loadable]) -&gt; Stages\n</code></pre> <p>Load a pipeline from YAML (path, YAML-string, file-like or Loadable).</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | Path | IO[str] | Loadable</code> <ul> <li>Path to a .yaml file</li> <li>Raw YAML content</li> <li>File-like object returning YAML</li> <li>Any object with a <code>.load()</code> method returning Python data</li> </ul> required <p>Returns:</p> Name Type Description <code>Stages</code> <code>Stages</code> <p>a validated <code>Stages</code> instance.</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    source: t.Union[str, Path, t.IO[str], Loadable],\n) -&gt; \"Stages\":\n    \"\"\"Load a pipeline from YAML (path, YAML-string, file-like or Loadable).\n\n    Args:\n        source (str | Path | IO[str] | Loadable):\n            - Path to a .yaml file\n            - Raw YAML content\n            - File-like object returning YAML\n            - Any object with a `.load()` method returning Python data\n\n    Returns:\n        Stages: a validated `Stages` instance.\n    \"\"\"\n    # 1) If it\u2019s a loader-object, pull Python data directly\n    if isinstance(source, Loadable):\n        data = source.load()\n\n    else:\n        # 2) Read text for YAML parsing:\n        if hasattr(source, \"read\"):\n            text = t.cast(t.IO[str], source).read()\n        else:\n            text = str(source)\n\n        # 3) First, try parsing as raw YAML\n        try:\n            data = yaml.safe_load(text)\n        except yaml.YAMLError:\n            data = None\n\n        # 4) Only if that parse returned a `str` we treat it as a filename\n        if isinstance(data, str):\n            try:\n                text = Path(data).read_text()\n                data = yaml.safe_load(text)\n            except (OSError, yaml.YAMLError) as e:\n                raise ValueError(\n                    f\"Could not interpret {data!r} as YAML or file path\"\n                ) from e\n\n    # 5) Validate final shape\n    if not isinstance(data, list):\n        raise ValueError(\n            f\"Expected a list of pipeline stages, got {type(data).__name__}\"\n        )\n\n    # 5) Finally, validate into our model\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Stages.steps","title":"steps","text":"<pre><code>steps() -&gt; Iterator[Stage]\n</code></pre> <p>Yield each stage in execution order.</p> <p>Returns:</p> Type Description <code>Iterator[Stage]</code> <p>Iterator[Datablock]: An iterator over the stages,</p> <code>Iterator[Stage]</code> <p>from first to last.</p> Source code in <code>ankaflow/models/core.py</code> <pre><code>def steps(self) -&gt; t.Iterator[Stage]:\n    \"\"\"Yield each stage in execution order.\n\n    Returns:\n        Iterator[Datablock]: An iterator over the stages,\n        from first to last.\n    \"\"\"\n    return iter(self.root)\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.core.Variables","title":"Variables","text":"<p>               Bases: <code>dict</code></p> <p>Variables is a <code>dict</code>-based collection storing arbitrary data under keys. Variable can be populated via:</p> <ul> <li>Dictionary passed to pipeline:   <code>Flow(defs, context, variables={'foo': 'bar'})</code></li> <li>Sink operation:<pre><code>- kind: sink\n\n    name: my_sink\n\n    connection:\n\n        kind: Variable\n\n        locator: variable_name\n</code></pre> </li> </ul> <p>This step will place a list of records from preceding step in   variables as <code>{'variable_name': t.List[dict]}</code></p> <p>In most cases sainking to variable is not needed as all preceding stages in a pipeline can be referenced via <code>name</code>. Sinking to variable is useful when you need to share data with subpipeline.</p> <p>Special variable is <code>loop_control</code> which is populated dynamically from previous step and current stage type is pipeline. In case previous step generates multiple records then the new pipeline is called for each record, and control variable holds a <code>dict</code> representing the current record.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.enums","title":"enums","text":""},{"location":"api/ankaflow.models/#ankaflow.models.enums.AuthType","title":"AuthType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.enums.ContentType","title":"ContentType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.enums.DataType","title":"DataType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.enums.LogLevel","title":"LogLevel","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.enums.ModelType","title":"ModelType","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.enums.ParameterDisposition","title":"ParameterDisposition","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.enums.RequestMethod","title":"RequestMethod","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.llm","title":"llm","text":""},{"location":"api/ankaflow.models/#ankaflow.models.llm.LLMConfig","title":"LLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Confuration for Language Model</p>"},{"location":"api/ankaflow.models/#ankaflow.models.llm.LLMConfig.kind","title":"kind","text":"<pre><code>kind: LLMKind = MOCK\n</code></pre> <p>Language model provider: <code>mock</code>(default)|<code>openai</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.llm.LLMConfig.model","title":"model","text":"<pre><code>model: str | None = None\n</code></pre> <p>Language model, if not set then uses default</p>"},{"location":"api/ankaflow.models/#ankaflow.models.llm.LLMConfig.proxy","title":"proxy","text":"<pre><code>proxy: LLMProxy | None = None\n</code></pre> <p>Reverse proxy used to connect to provider (optional)</p>"},{"location":"api/ankaflow.models/#ankaflow.models.llm.LLMKind","title":"LLMKind","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported LLM protocol backends.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.llm.LLMKind.MOCK","title":"MOCK","text":"<pre><code>MOCK = 'mock'\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.llm.LLMKind.OPENAI","title":"OPENAI","text":"<pre><code>OPENAI = 'openai'\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.llm.LLMProxy","title":"LLMProxy","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for proxy-based LLM usage.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.llm.LLMProxy.client","title":"client","text":"<pre><code>client: RestClientConfig\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.llm.LLMProxy.request","title":"request","text":"<pre><code>request: Request\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.rest","title":"rest","text":""},{"location":"api/ankaflow.models/#ankaflow.models.rest.BasicHandler","title":"BasicHandler","text":"<p>               Bases: <code>BaseModel</code></p> <p>A no-op response handler used when no special processing (like pagination or transformation) is required.</p> <p>Typically used for single-response REST endpoints where the entire payload is returned in one request.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal</code> <p>Specifies the handler type as BASIC.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.BasicHandler.kind","title":"kind","text":"<pre><code>kind: Literal[BASIC]\n</code></pre> <p>Specifies the handler type as Basic.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Paginator","title":"Paginator","text":"<p>               Bases: <code>BaseModel</code></p> <p>A response handler for paginated REST APIs.</p> <p>This handler generates repeated requests by incrementing a page-related parameter until no more data is available. The stopping condition is usually inferred from the number of records in the response being less than <code>page_size</code>, or from a total record count field.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Paginator.increment","title":"increment","text":"<pre><code>increment: int\n</code></pre> <p>Page parameter increment. Original request configuration should include initial value e.g. <code>page_no=1</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Paginator.kind","title":"kind","text":"<pre><code>kind: Literal[PAGINATOR]\n</code></pre> <p>Specifies the handler type as Paginator.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Paginator.page_param","title":"page_param","text":"<pre><code>page_param: str\n</code></pre> <p>Page parameter in the request (query or body) This will be incremented from request to request</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Paginator.page_size","title":"page_size","text":"<pre><code>page_size: int\n</code></pre> <p>Page size should be explicitly defined. If response contains less records it is considered to be last page</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Paginator.param_locator","title":"param_locator","text":"<pre><code>param_locator: ParameterDisposition\n</code></pre> <p>Define where the parameter is located: body or query</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Paginator.throttle","title":"throttle","text":"<pre><code>throttle: Optional[Union[int, float]] = None\n</code></pre> <p>If set to positive value then each page request is throttled given number of seconds.</p> <p>Useful when dealing with rate limits or otherwise spreading the load over time.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Paginator.total_records","title":"total_records","text":"<pre><code>total_records: Optional[str] = None\n</code></pre> <p>JMESPath to total records count in the response.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Request","title":"Request","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Request.body","title":"body","text":"<pre><code>body: Optional[Union[str, Dict]] = None\n</code></pre> <p>Request body parameters.</p> <p>This field accepts either: - A Python <code>dict</code> representing a direct key-value mapping, or - A Jinja-templated JSON string with magic <code>@json</code> prefix, e.g.: <code>@json{\"parameter\": \"value\"}</code></p> <p>The template will be rendered using the following custom delimiters: - <code>&lt;&lt; ... &gt;&gt;</code> for variable interpolation - <code>&lt;% ... %&gt;</code> for logic/control flow (e.g., for-loops) - <code>&lt;# ... #&gt;</code> for inline comments</p> <p>The template will be rendered before being parsed into a valid JSON object. This allows the use of dynamic expressions, filters, and control flow such as loops.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Request.body--example-with-looping","title":"Example with looping","text":"<p>Given:</p> <pre><code>variables = {\n    \"MyList\": [\n        {\"id\": 1, \"value\": 10},\n        {\"id\": 2, \"value\": 20}\n    ]\n}\n</code></pre> <p>You can generate a dynamic body with:</p> <pre><code>body: &gt;\n@json[\n    &lt;% for row in API.look(\"MyTable\", variables) %&gt;\n        { \"id\": &lt;&lt; row.id &gt;&gt;, \"value\": &lt;&lt; row.value &gt;&gt; }&lt;% if not loop.last %&gt;,&lt;% endif %&gt;\n    &lt;% endfor %&gt;\n]\n</code></pre> <p>This will render to a proper JSON list:</p> <pre><code>[\n    { \"id\": 1, \"value\": 10 },\n    { \"id\": 2, \"value\": 20 }\n]\n</code></pre> <p>Notes: - When using @json, the entire string is rendered as a Jinja template     and then parsed with json.loads(). - Nested @json blocks are not supported. - Newlines and whitespace are automatically collapsed during rendering.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Request.content_type","title":"content_type","text":"<pre><code>content_type: ContentType = JSON\n</code></pre> <p>Request content type</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Request.endpoint","title":"endpoint","text":"<pre><code>endpoint: str\n</code></pre> <p>Request endpoint e.g. <code>get/data</code> under base url:</p> <p>Example <code>https://api.example.com/v1</code> + <code>get/data</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Request.errorhandler","title":"errorhandler","text":"<pre><code>errorhandler: RestErrorHandler = Field(\n    default_factory=RestErrorHandler\n)\n</code></pre> <p>Custom error handler e.g. for searching conditions in response or custom status codes</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Request.initial_backoff","title":"initial_backoff","text":"<pre><code>initial_backoff: float = 0.5\n</code></pre> <p>Initial backoff time in seconds. Will be multiplied exponentially for subsequent retries (2^n).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Request.max_retries","title":"max_retries","text":"<pre><code>max_retries: int = 0\n</code></pre> <p>Maximum number of retries on transport errors. Default is 0 (no retry).</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Request.method","title":"method","text":"<pre><code>method: RequestMethod\n</code></pre> <p>Request method e.g. <code>post,get,put</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Request.query","title":"query","text":"<pre><code>query: Dict = {}\n</code></pre> <p>Query parameters. Parameters may contain template variables.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.Request.response","title":"response","text":"<pre><code>response: RestResponse\n</code></pre> <p>Response handling configuration</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.ResponseHandlerTypes","title":"ResponseHandlerTypes","text":""},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestAuth","title":"RestAuth","text":"<p>               Bases: <code>BaseModel</code></p> <p>Authenctication configuration for Rest connection.</p> <p>NOTE: Not all authentication methods may not work in browser due to limitations in the network API.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestAuth.method","title":"method","text":"<pre><code>method: AuthType\n</code></pre> <p>Specifies authentiation type.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestAuth.values","title":"values","text":"<pre><code>values: StringDict\n</code></pre> <p>Mapping of parameter names and values.</p> <p>{     'X-Auth-Token': '' }"},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestAuth.coerce_to_stringdict","title":"coerce_to_stringdict","text":"<pre><code>coerce_to_stringdict(v)\n</code></pre> <p>Rest header values must be strings. This convenience validator  automatically converts regiular dictionary to StringDict.</p> Source code in <code>ankaflow/models/rest.py</code> <pre><code>@field_validator(\"values\", mode=\"before\")\n@classmethod\ndef coerce_to_stringdict(cls, v):\n    \"\"\"Rest header values must be strings.\n    This convenience validator  automatically\n    converts regiular dictionary to StringDict.\n    \"\"\"\n    if isinstance(v, StringDict):\n        return v\n    if isinstance(v, dict):\n        return StringDict(v)\n    raise TypeError(\"Expected a StringDict or a dict for `values`\")\n</code></pre>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestClientConfig","title":"RestClientConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Rest client for given base URL. Includes transport and authentication configuration</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestClientConfig.base_url","title":"base_url","text":"<pre><code>base_url: str\n</code></pre> <p>Base URL, typically server or API root. All endpoints with the same base URL share the same authentication.</p> <p>Example: <code>https://api.example.com/v1</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestClientConfig.timeout","title":"timeout","text":"<pre><code>timeout: Optional[float] = None\n</code></pre> <p>Request timeout in seconds. Default is 5. Set 0 to disable timout.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestErrorHandler","title":"RestErrorHandler","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestErrorHandler.condition","title":"condition","text":"<pre><code>condition: Optional[str] = None\n</code></pre> <p>JMESPath expression to look for in the response body. Error will be generated if expression evaluates to True</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestErrorHandler.error_status_codes","title":"error_status_codes","text":"<pre><code>error_status_codes: List[int] = []\n</code></pre> <p>List of HTTP status codes to be treated as errors.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestErrorHandler.message","title":"message","text":"<pre><code>message: Optional[str] = None\n</code></pre> <p>JMESPath expression to extract error message from respose. If omitted entire response will be included in error.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestResponse","title":"RestResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response configuration. Response can be paged, polled URL or in body.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestResponse.content_type","title":"content_type","text":"<pre><code>content_type: DataType\n</code></pre> <p>Returned data type</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.RestResponse.locator","title":"locator","text":"<pre><code>locator: Optional[str] = None\n</code></pre> <p>JMESPath to read data from JSON body. If not set then entire body is treated as data.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.StatePoller","title":"StatePoller","text":"<p>               Bases: <code>BaseModel</code></p> <p>A response handler for state-based polling APIs.</p> <p>This handler is designed for asynchronous workflows where the client repeatedly polls an endpoint until a certain state is reached (e.g., job completion, resource readiness). Once the condition is met, the pipeline continues by reading from the final data <code>locator</code>.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.StatePoller.kind","title":"kind","text":"<pre><code>kind: Literal[STATEPOLLING]\n</code></pre> <p>Specifies the handler type as StatePolling.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.StatePoller.ready_status","title":"ready_status","text":"<pre><code>ready_status: str\n</code></pre> <p>JMESPath to read status from response. If the value at path evaluates to True then no more requests are made, and the API tries to read data from URL specified by <code>locator</code>.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.URLPoller","title":"URLPoller","text":"<p>               Bases: <code>BaseModel</code></p> <p>URL Poller makes request(s) to remote API until an URL is returned</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.URLPoller.kind","title":"kind","text":"<pre><code>kind: Literal[URLPOLLING]\n</code></pre> <p>Specifies the handler type as URLPolling.</p>"},{"location":"api/ankaflow.models/#ankaflow.models.rest.URLPoller.ready_status","title":"ready_status","text":"<pre><code>ready_status: Optional[str] = None\n</code></pre> <p>JMESPath to read status from response. If the value at path evaluates to True then no more requests are made, and the API tries to read data from URL specified by <code>locator</code>.</p>"}]}